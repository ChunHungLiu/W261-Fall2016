{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASCI W261: Machine Learning at Scale\n",
    "## Assignment Week 1\n",
    "Miki Seltzer (miki.seltzer@berkeley.edu)<br>\n",
    "W261-2, Spring 2016<br>\n",
    "Due date 19-Jan-2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.0.0: Define big data. Provide an example of a big data problem in your domain of expertise. \n",
    "\n",
    "\"Big data\" is a broad term that has many interpretations. It doesn't seem fair to assign a concrete size barrier above which is considered \"big,\" and below which is not. A definition that resonates with me is that if a data set is too large to fit on or process with one computer in a reasonable amount of time, then it is considered big data. This definition allows for more flexibility as hard drives become larger and computers become more powerful.\n",
    "\n",
    "I currently work at a home security company, so we are constantly generating IoT-type data. Analysis on any of the data we generate from wireless sensors is usually a big data problem, unless we are looking at a tiny subset of data. One of the most important problems we are currently trying to solve is determining when a home is unoccupied using data from wireless sensors such as motion detectors and door sensors. It is relatively easy to determine when a home is occupied: we can be fairly certain that someone is home when motion is detected. Conversely, the absence of motion does not always indicate that the home is unoccupied. A simple example is at nighttime when people are usually sleeping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.0.1: In 500 words (English or pseudo code or a combination) describe how to estimate the bias, the variance, the irreducible error for a test dataset T when using polynomial regression models of degree 1, 2, 3, 4, 5 are considered. How would you select a model?\n",
    "\n",
    "In order to estimate the bias, variance and irreducible error (noise) for a single data that is generated from the unknown true function $f(x)$, we first need to generate multiple sets of data from our original data set T. We can do this by sampling the original data set with replacement (bootstrapping). If we repeat the bootstrap resample 50 times, we will have 50 data sets to work with.\n",
    "\n",
    "With each of the 50 new data sets, we split the data set into training set and a testing set. We fit polynomials of degree 1, 2, 3, 4 and 5 to the training set. This yields 50 models per polynomial degree. For each polynomial degree, we can then estimate the average variance and bias using the testing set.\n",
    "\n",
    "#### Variance estimation\n",
    "For each observation, $x$, in the testing set, we now have 50 predictions per polynomial degree, which we denote as $y_1, y_2, ..., y_{50}$. We find the average of these predictions, denoted as $\\bar{y}$. We can find the variance of the predictions using the formula: $E((y_i-\\bar{y})^2)$. We then average the variance for each data point in the testing set, and then repeat the process for each polynomial degree. Thus, we will have one average variance per polynomial degree.\n",
    "\n",
    "#### Bias estimation\n",
    "For each observation, $x$, we also have the actual value of $y$ in the testing set. The bias of the observation $x$ is the difference between the average prediction and the actual value: $\\bar{y}-y$. If we happened to have the true function $f(x)$ from which the data were generated, we would calculate the bias as: $\\bar{y}-f(x)$.\n",
    "\n",
    "We then average the bias for each data point in the testing set per polynomial degree, and repeat the process for each polynomial degree. This yields one average bias per polynomial degree. It is also useful to calculate the average squared bias, which is calculated by squaring the bias before averaging all of the observations in the testing set.\n",
    "\n",
    "#### Noise estimation\n",
    "If we do not know the true function $f(x)$, we are forced to make the assumption that the noise is zero. If we knew the true function $f(x)$ that the data set T was generated from, we would be able to calculate the irreducible error, which would be the square of the difference between the observed values and the true function: $(y-f(x))^2$.\n",
    "\n",
    "#### Model selection\n",
    "We know that there is a trade-off between bias and variance. As the model gets more complex, bias generally decreases, while variance generally increases. We can plot the sum of the squared bias and the variance, and choose the degree where the sum is minimized.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.1: Read through the provided control script (pNaiveBayes.sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print \"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using a Linux system, need to modify new line character to work correctly\n",
    "\n",
    "!perl -pi -e 's/\\r/\\n/g' enronemail_1h.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to use the pNaiveBayes.sh file multiple times during this homework assignment, so let's make sure that it is written to our working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pNaiveBayes.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile pNaiveBayes.sh\n",
    "## pNaiveBayes.sh\n",
    "## Author: Jake Ryland Williams\n",
    "## Usage: pNaiveBayes.sh m wordlist\n",
    "## Input:\n",
    "##       m = number of processes (maps), e.g., 4\n",
    "##       wordlist = a space-separated list of words in quotes, e.g., \"the and of\"\n",
    "##\n",
    "## Instructions: Read this script and its comments closely.\n",
    "##               Do your best to understand the purpose of each command,\n",
    "##               and focus on how arguments are supplied to mapper.py/reducer.py,\n",
    "##               as this will determine how the python scripts take input.\n",
    "##               When you are comfortable with the unix code below,\n",
    "##               answer the questions on the LMS for HW1 about the starter code.\n",
    "\n",
    "## collect user input\n",
    "m=$1 ## the number of parallel processes (maps) to run\n",
    "wordlist=$2 ## if set to \"*\", then all words are used\n",
    "\n",
    "## a test set data of 100 messages\n",
    "data=\"enronemail_1h.txt\" \n",
    "\n",
    "## the full set of data (33746 messages)\n",
    "# data=\"enronemail.txt\" \n",
    "\n",
    "## 'wc' determines the number of lines in the data\n",
    "## 'perl -pe' regex strips the piped wc output to a number\n",
    "linesindata=`wc -l $data | perl -pe 's/^.*?(\\d+).*?$/$1/'`\n",
    "\n",
    "## determine the lines per chunk for the desired number of processes\n",
    "linesinchunk=`echo \"$linesindata/$m+1\" | bc`\n",
    "\n",
    "## split the original file into chunks by line\n",
    "split -l $linesinchunk $data $data.chunk.\n",
    "\n",
    "## assign python mappers (mapper.py) to the chunks of data\n",
    "## and emit their output to temporary files\n",
    "for datachunk in $data.chunk.*; do\n",
    "    ## feed word list to the python mapper here and redirect STDOUT to a temporary file on disk\n",
    "    ####\n",
    "    ####\n",
    "    ./mapper.py $datachunk \"$wordlist\" > $datachunk.counts &\n",
    "    ####\n",
    "    ####\n",
    "done\n",
    "## wait for the mappers to finish their work\n",
    "wait\n",
    "\n",
    "## 'ls' makes a list of the temporary count files\n",
    "## 'perl -pe' regex replaces line breaks with spaces\n",
    "countfiles=`\\ls $data.chunk.*.counts | perl -pe 's/\\n/ /'`\n",
    "\n",
    "## feed the list of countfiles to the python reducer and redirect STDOUT to disk\n",
    "####\n",
    "####\n",
    "./reducer.py $countfiles > $data.output\n",
    "####\n",
    "####\n",
    "\n",
    "## clean up the data chunks and temporary count files\n",
    "\\rm $data.chunk.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x pNaiveBayes.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.2: Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will determine the number of occurrences of a single, user-specified word. \n",
    "\n",
    "Examine the word “assistance” and report your results.\n",
    "\n",
    "To do so, make sure that\n",
    "\n",
    "- mapper.py counts all occurrences of a single word, and\n",
    "- reducer.py collates the counts of the single word.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: mapper code for HW1.2\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Collect input\n",
    "filename = sys.argv[1]\n",
    "findwords = re.split(\" \",sys.argv[2].lower())\n",
    "\n",
    "# Initialize dictionary to empty\n",
    "wordcount = {}\n",
    "\n",
    "with open(filename, \"rU\") as myfile:\n",
    "    for line in myfile:\n",
    "        # Format each line, fields separated by \\t according to enronemail_README.txt\n",
    "        # Remove \\n text at end of each line\n",
    "        fields = line.split(\"\\t\")\n",
    "        fields[3] = fields[3].replace(\"\\n\", \"\")\n",
    "        subj = fields[2].translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "        body = fields[3].translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "        \n",
    "        # For each word in list provided by user, count occurrences in subj and body\n",
    "        for word in findwords:\n",
    "            if word not in wordcount:\n",
    "                wordcount[word] = 0 \n",
    "            wordcount[word] += subj.count(word) + body.count(word)\n",
    "\n",
    "for word in wordcount:\n",
    "    print [word, wordcount[word]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: reducer code for HW1.2\n",
    "\n",
    "import sys\n",
    "\n",
    "filenames = sys.argv[1:]\n",
    "\n",
    "wordcount = {}\n",
    "\n",
    "# Each mapper outputs a [word, count] pair\n",
    "# For each file and each word, sum the counts\n",
    "for file in filenames:\n",
    "    with open(file, \"rU\") as myfile:\n",
    "        for line in myfile:\n",
    "            pair = eval(line)\n",
    "            word = pair[0]\n",
    "            count = pair[1]\n",
    "            if word not in wordcount:\n",
    "                wordcount[word] = 0\n",
    "            wordcount[word] += count\n",
    "            \n",
    "for word in wordcount:\n",
    "    print word + \"\\t\" + str(wordcount[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change file permissions\n",
    "!chmod a+x mapper.py\n",
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!./pNaiveBayes.sh 4 \"assistance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_counts():\n",
    "    with open(\"enronemail_1h.txt.output\", \"r\") as myfile:\n",
    "        print \"{:<15s}{:3s}\".format(\"word\", \"count\")\n",
    "        print \"----------------------\"\n",
    "        for line in myfile:\n",
    "            pair = line.split(\"\\t\")\n",
    "            word = pair[0]\n",
    "            count = int(pair[1])\n",
    "            print \"{:<15s}{:3d}\".format(word, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output of HW1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word           count\n",
      "----------------------\n",
      "assistance      10\n"
     ]
    }
   ],
   "source": [
    "print_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.3. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify the email messages by a single, user-specified word using the multinomial Naive Bayes Formulation. \n",
    "\n",
    "Examine the word “assistance” and report your results. To do so, make sure that mapper.py and reducer.py perform a single word Naive Bayes classification. For multinomial Naive Bayes, the Pr(X=“assistance”|Y=SPAM) is calculated as follows: \n",
    "\n",
    "$$\n",
    "\\frac{\\text{number of times \"assistance\" occurs in SPAM labeled documents}}{\\text{the number of words in documents labeled SPAM}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: mapper code for HW1.3\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Collect input\n",
    "filename = sys.argv[1]\n",
    "findwords = re.split(\" \",sys.argv[2].lower())\n",
    "\n",
    "# Initialize dictionary to empty\n",
    "word_count = {}\n",
    "\n",
    "with open(filename, \"rU\") as myfile:\n",
    "    for line in myfile:\n",
    "        # Format each line, fields separated by \\t according to enronemail_README.txt\n",
    "        fields = line.split(\"\\t\")\n",
    "        fields[3] = fields[3].replace(\"\\n\", \"\")\n",
    "        subj = fields[2].translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "        body = fields[3].translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "        \n",
    "        # For each word in list provided by user, count occurrences in subj and body\n",
    "        for word in findwords:\n",
    "            my_key = (fields[0], fields[1], word)\n",
    "            if my_key not in word_count:\n",
    "                word_count[my_key] = 0 \n",
    "            word_count[my_key] += subj.count(word) + body.count(word)\n",
    "\n",
    "for key in word_count:\n",
    "    print [key, word_count[key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: reducer code for HW1.3\n",
    "\n",
    "import sys\n",
    "import math\n",
    "\n",
    "filenames = sys.argv[1:]\n",
    "\n",
    "doc_ids = {}\n",
    "document_words = {}\n",
    "class_counts = {'1':0.0, '0':0.0}\n",
    "vocab = {}\n",
    "word_counts = {\n",
    "    '0': {},\n",
    "    '1': {}\n",
    "}\n",
    "\n",
    "for file in filenames:\n",
    "    # Train classifier with data from mapper.py\n",
    "    with open(file, \"r\") as myfile:\n",
    "        for line in myfile:\n",
    "            pair = eval(line)\n",
    "            doc_id = pair[0][0]\n",
    "            spam = pair[0][1]\n",
    "            word = pair[0][2]\n",
    "            count = int(pair[1])\n",
    "            \n",
    "            # We need to aggregate counts on specific levels. We need:\n",
    "            #    - number of total documents\n",
    "            #    - number of documents per class\n",
    "            #    - our entire vocabulary\n",
    "            #    - word counts per class\n",
    "            #    - word counts per document (for testing purposes)\n",
    "            if doc_id not in doc_ids: \n",
    "                doc_ids[doc_id] = spam\n",
    "                class_counts[spam] += 1\n",
    "                document_words[doc_id] = {}\n",
    "            if word not in vocab: vocab[word] = 0.0\n",
    "            vocab[word] += count\n",
    "            if word not in word_counts[spam]: word_counts[spam][word] = 0.0\n",
    "            word_counts[spam][word] += count\n",
    "            if word not in document_words[doc_id]: document_words[doc_id][word] = 0.0\n",
    "            document_words[doc_id][word] += count\n",
    "            \n",
    "\n",
    "prior_spam = class_counts['1'] / len(doc_ids)\n",
    "prior_ham = class_counts['0'] / len(doc_ids)\n",
    "\n",
    "p_spam_word = 0.0\n",
    "p_ham_word = 0.0\n",
    "\n",
    "for doc in document_words:\n",
    "    pred = 'none'\n",
    "    \n",
    "    # Test classifier using training data\n",
    "    for word in doc:        \n",
    "        # Make sure that the word was in the training data\n",
    "        if word not in vocab: continue\n",
    "            \n",
    "        # Calculate P(word)\n",
    "        p_word = vocab[word] / sum(vocab.values())\n",
    "        \n",
    "        # Calculate P(word|spam) and P(word|ham)\n",
    "        # If word does not occur in spam or ham documents, probability is 0\n",
    "        if word not in word_counts['1']: p_word_spam = 0.0\n",
    "        else: p_word_spam = word_counts['1'][word] / sum(word_counts['1'].values())\n",
    "        if word not in word_counts['0']: p_word_ham = 0.0\n",
    "        else: p_word_ham = word_counts['0'][word] / sum(word_counts['0'].values())\n",
    "        \n",
    "        # Calculate P(spam|word) and P(ham|word)\n",
    "        p_spam_word = prior_spam * p_word_spam ** document_words[doc][word]\n",
    "        p_ham_word = prior_ham * p_word_ham ** document_words[doc][word]\n",
    "        \n",
    "    if p_spam_word > p_ham_word: pred = '1'\n",
    "    else: pred = '0'\n",
    "    \n",
    "    print doc + \"\\t\" + doc_ids[doc] + \"\\t\" + pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change file permissions\n",
    "!chmod a+x mapper.py\n",
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!./pNaiveBayes.sh 4 \"assistance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_predictions():\n",
    "    \n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    \n",
    "    print \"{:<35s}{:5s}{:5s}\".format(\"document id\", \"true\", \"pred\")\n",
    "    print \"---------------------------------------------\"\n",
    "    \n",
    "    with open('enronemail_1h.txt.output', 'rU') as myfile:\n",
    "        for line in myfile:\n",
    "            fields = line.split(\"\\t\")\n",
    "            fields[-1] = fields[-1].replace(\"\\n\", \"\")\n",
    "            print \"{:<35s}{:5s}{:5s}\".format(fields[0], fields[1], fields[2])\n",
    "            total += 1\n",
    "            if fields[1] == fields[2]: correct += 1\n",
    "    \n",
    "    print \"\\nAccuracy: {:.2%}\".format(correct/total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output of HW1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document id                        true pred \n",
      "---------------------------------------------\n",
      "0010.2003-12-18.GP                 1    0    \n",
      "0010.2001-06-28.SA_and_HP          1    0    \n",
      "0001.2000-01-17.beck               0    0    \n",
      "0018.1999-12-14.kaminski           0    0    \n",
      "0005.1999-12-12.kaminski           0    0    \n",
      "0011.2001-06-29.SA_and_HP          1    0    \n",
      "0008.2004-08-01.BG                 1    0    \n",
      "0009.1999-12-14.farmer             0    0    \n",
      "0017.2003-12-18.GP                 1    0    \n",
      "0011.2001-06-28.SA_and_HP          1    0    \n",
      "0015.2001-07-05.SA_and_HP          1    0    \n",
      "0015.2001-02-12.kitchen            0    0    \n",
      "0009.2001-06-26.SA_and_HP          1    0    \n",
      "0018.2001-07-13.SA_and_HP          1    0    \n",
      "0012.2000-01-17.beck               0    0    \n",
      "0003.2000-01-17.beck               0    0    \n",
      "0004.2001-06-12.SA_and_HP          1    0    \n",
      "0008.2001-06-12.SA_and_HP          1    0    \n",
      "0007.2001-02-09.kitchen            0    0    \n",
      "0016.2004-08-01.BG                 1    0    \n",
      "0015.2000-06-09.lokay              0    0    \n",
      "0016.1999-12-15.farmer             0    0    \n",
      "0013.2004-08-01.BG                 1    0    \n",
      "0005.2003-12-18.GP                 1    0    \n",
      "0012.2001-02-09.kitchen            0    0    \n",
      "0011.1999-12-14.farmer             0    0    \n",
      "0013.1999-12-14.kaminski           0    0    \n",
      "0009.2001-02-09.kitchen            0    0    \n",
      "0006.2001-02-08.kitchen            0    0    \n",
      "0014.2003-12-19.GP                 1    0    \n",
      "0010.1999-12-14.farmer             0    0    \n",
      "0010.2004-08-01.BG                 1    0    \n",
      "0014.1999-12-14.kaminski           0    0    \n",
      "0006.1999-12-13.kaminski           0    0    \n",
      "0005.1999-12-14.farmer             0    0    \n",
      "0003.2001-02-08.kitchen            0    0    \n",
      "0001.2001-02-07.kitchen            0    0    \n",
      "0008.2001-02-09.kitchen            0    0    \n",
      "0007.2003-12-18.GP                 1    0    \n",
      "0017.2004-08-02.BG                 1    0    \n",
      "0014.2004-08-01.BG                 1    0    \n",
      "0006.2003-12-18.GP                 1    0    \n",
      "0016.2001-07-05.SA_and_HP          1    0    \n",
      "0008.2003-12-18.GP                 1    0    \n",
      "0014.2001-07-04.SA_and_HP          1    0    \n",
      "0001.2001-04-02.williams           0    0    \n",
      "0012.2000-06-08.lokay              0    0    \n",
      "0014.1999-12-15.farmer             0    0    \n",
      "0009.2000-06-07.lokay              0    0    \n",
      "0001.1999-12-10.farmer             0    0    \n",
      "0008.2001-06-25.SA_and_HP          1    0    \n",
      "0017.2001-04-03.williams           0    0    \n",
      "0014.2001-02-12.kitchen            0    0    \n",
      "0016.2001-07-06.SA_and_HP          1    0    \n",
      "0015.1999-12-15.farmer             0    0    \n",
      "0009.1999-12-13.kaminski           0    0    \n",
      "0001.2000-06-06.lokay              0    0    \n",
      "0011.2004-08-01.BG                 1    0    \n",
      "0004.2004-08-01.BG                 1    0    \n",
      "0018.2003-12-18.GP                 1    0    \n",
      "0002.1999-12-13.farmer             0    0    \n",
      "0016.2003-12-19.GP                 1    0    \n",
      "0004.1999-12-14.farmer             0    0    \n",
      "0015.2003-12-19.GP                 1    0    \n",
      "0006.2004-08-01.BG                 1    0    \n",
      "0009.2003-12-18.GP                 1    0    \n",
      "0007.1999-12-14.farmer             0    0    \n",
      "0005.2000-06-06.lokay              0    0    \n",
      "0010.1999-12-14.kaminski           0    0    \n",
      "0007.2000-01-17.beck               0    0    \n",
      "0003.1999-12-14.farmer             0    0    \n",
      "0003.2004-08-01.BG                 1    0    \n",
      "0017.2004-08-01.BG                 1    0    \n",
      "0013.2001-06-30.SA_and_HP          1    0    \n",
      "0003.1999-12-10.kaminski           0    0    \n",
      "0012.1999-12-14.farmer             0    0    \n",
      "0004.1999-12-10.kaminski           0    0    \n",
      "0017.1999-12-14.kaminski           0    0    \n",
      "0002.2001-02-07.kitchen            0    0    \n",
      "0007.2004-08-01.BG                 1    0    \n",
      "0012.1999-12-14.kaminski           0    0    \n",
      "0005.2001-06-23.SA_and_HP          1    0    \n",
      "0007.1999-12-13.kaminski           0    0    \n",
      "0017.2000-01-17.beck               0    0    \n",
      "0006.2001-06-25.SA_and_HP          1    0    \n",
      "0006.2001-04-03.williams           0    0    \n",
      "0005.2001-02-08.kitchen            0    0    \n",
      "0002.2003-12-18.GP                 1    0    \n",
      "0003.2003-12-18.GP                 1    0    \n",
      "0013.2001-04-03.williams           0    0    \n",
      "0004.2001-04-02.williams           0    0    \n",
      "0010.2001-02-09.kitchen            0    0    \n",
      "0001.1999-12-10.kaminski           0    0    \n",
      "0013.1999-12-14.farmer             0    0    \n",
      "0015.1999-12-14.kaminski           0    0    \n",
      "0012.2003-12-19.GP                 1    0    \n",
      "0016.2001-02-12.kitchen            0    0    \n",
      "0002.2004-08-01.BG                 1    0    \n",
      "0002.2001-05-25.SA_and_HP          1    0    \n",
      "0011.2003-12-18.GP                 1    0    \n",
      "\n",
      "Accuracy: 56.00%\n"
     ]
    }
   ],
   "source": [
    "print_predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.4: Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify the email messages by a list of one or more user-specified words. \n",
    "\n",
    "Examine the words “assistance”, “valium”, and “enlargementWithATypo” and report your results. To do so, make sure that mapper.py counts all occurrences of a list of words, and reducer.py performs the multiple-word multinomial Naive Bayes classification via the chosen list.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: mapper code for HW1.4\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Collect input\n",
    "filename = sys.argv[1]\n",
    "findwords = re.split(\" \",sys.argv[2].lower())\n",
    "\n",
    "# Initialize dictionary to empty\n",
    "word_count = {}\n",
    "\n",
    "with open(filename, \"rU\") as myfile:\n",
    "    for line in myfile:\n",
    "        # Format each line, fields separated by \\t according to enronemail_README.txt\n",
    "        fields = line.split(\"\\t\")\n",
    "        fields[3] = fields[3].replace(\"\\n\", \"\")\n",
    "        subj = fields[2].translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "        body = fields[3].translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "        \n",
    "        # For each word in list provided by user, count occurrences in subj and body\n",
    "        for word in findwords:\n",
    "            my_key = (fields[0], fields[1], word)\n",
    "            if my_key not in word_count:\n",
    "                word_count[my_key] = 0 \n",
    "            word_count[my_key] += subj.count(word) + body.count(word)\n",
    "\n",
    "for key in word_count:\n",
    "    print [key, word_count[key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: reducer code for HW1.4\n",
    "\n",
    "import sys\n",
    "import math\n",
    "\n",
    "filenames = sys.argv[1:]\n",
    "\n",
    "doc_ids = {}\n",
    "document_words = {}\n",
    "class_counts = {'1':0.0, '0':0.0}\n",
    "vocab = {}\n",
    "word_counts = {\n",
    "    '0': {},\n",
    "    '1': {}\n",
    "}\n",
    "\n",
    "for file in filenames:\n",
    "    # Train classifier with data from mapper.py\n",
    "    with open(file, \"r\") as myfile:\n",
    "        for line in myfile:\n",
    "            pair = eval(line)\n",
    "            doc_id = pair[0][0]\n",
    "            spam = pair[0][1]\n",
    "            word = pair[0][2]\n",
    "            count = int(pair[1])\n",
    "            \n",
    "            # We need to aggregate counts on specific levels. We need:\n",
    "            #    - number of total documents\n",
    "            #    - number of documents per class\n",
    "            #    - our entire vocabulary\n",
    "            #    - word counts per class\n",
    "            #    - word counts per document (for testing purposes)\n",
    "            if doc_id not in doc_ids: \n",
    "                doc_ids[doc_id] = spam\n",
    "                class_counts[spam] += 1\n",
    "                document_words[doc_id] = {}\n",
    "            if word not in vocab: vocab[word] = 0.0\n",
    "            vocab[word] += count\n",
    "            if word not in word_counts[spam]: word_counts[spam][word] = 0.0\n",
    "            word_counts[spam][word] += count\n",
    "            if word not in document_words[doc_id]: document_words[doc_id][word] = 0.0\n",
    "            document_words[doc_id][word] += count\n",
    "            \n",
    "\n",
    "prior_spam = class_counts['1'] / len(doc_ids)\n",
    "prior_ham = class_counts['0'] / len(doc_ids)\n",
    "\n",
    "for doc in document_words:\n",
    "    pred = 'none'\n",
    "    \n",
    "    # Test classifier using training data\n",
    "    for word in document_words[doc]:\n",
    "        prob_spam_word = prior_spam\n",
    "        prob_ham_word = prior_ham\n",
    "        \n",
    "        # Make sure that the word was in the training data\n",
    "        if word not in vocab: continue\n",
    "            \n",
    "        # Calculate P(word)\n",
    "        p_word = vocab[word] / sum(vocab.values())\n",
    "        \n",
    "        # Calculate P(word|spam) and P(word|ham)\n",
    "        # If word does not occur in spam or ham documents, probability is 0\n",
    "        if word not in word_counts['1']: p_word_spam = 0.0\n",
    "        else: p_word_spam = word_counts['1'][word] / sum(word_counts['1'].values())\n",
    "        if word not in word_counts['0']: p_word_ham = 0.0\n",
    "        else: p_word_ham = word_counts['0'][word] / sum(word_counts['0'].values())\n",
    "        \n",
    "        # Update probabilities\n",
    "        prob_spam_word *= p_word_spam ** document_words[doc][word]\n",
    "        prob_ham_word *= p_word_ham ** document_words[doc][word]\n",
    "    \n",
    "    if prob_spam_word > prob_ham_word: pred = '1'\n",
    "    else: pred = '0'\n",
    "    \n",
    "    print doc + \"\\t\" + doc_ids[doc] + \"\\t\" + pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change file permissions\n",
    "!chmod a+x mapper.py\n",
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!./pNaiveBayes.sh 4 \"assistance valium enlargementWithATypo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output of HW1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document id                        true pred \n",
      "---------------------------------------------\n",
      "0010.2003-12-18.GP                 1    0    \n",
      "0010.2001-06-28.SA_and_HP          1    0    \n",
      "0001.2000-01-17.beck               0    0    \n",
      "0018.1999-12-14.kaminski           0    0    \n",
      "0005.1999-12-12.kaminski           0    0    \n",
      "0011.2001-06-29.SA_and_HP          1    0    \n",
      "0008.2004-08-01.BG                 1    0    \n",
      "0009.1999-12-14.farmer             0    0    \n",
      "0017.2003-12-18.GP                 1    0    \n",
      "0011.2001-06-28.SA_and_HP          1    0    \n",
      "0015.2001-07-05.SA_and_HP          1    0    \n",
      "0015.2001-02-12.kitchen            0    0    \n",
      "0009.2001-06-26.SA_and_HP          1    0    \n",
      "0017.1999-12-14.kaminski           0    0    \n",
      "0012.2000-01-17.beck               0    0    \n",
      "0003.2000-01-17.beck               0    0    \n",
      "0004.2001-06-12.SA_and_HP          1    0    \n",
      "0008.2001-06-12.SA_and_HP          1    0    \n",
      "0007.2001-02-09.kitchen            0    0    \n",
      "0016.2004-08-01.BG                 1    0    \n",
      "0015.2000-06-09.lokay              0    0    \n",
      "0016.1999-12-15.farmer             0    0    \n",
      "0013.2004-08-01.BG                 1    0    \n",
      "0005.2003-12-18.GP                 1    0    \n",
      "0012.2001-02-09.kitchen            0    0    \n",
      "0011.1999-12-14.farmer             0    0    \n",
      "0013.1999-12-14.kaminski           0    0    \n",
      "0009.2001-02-09.kitchen            0    0    \n",
      "0006.2001-02-08.kitchen            0    0    \n",
      "0014.2003-12-19.GP                 1    0    \n",
      "0010.1999-12-14.farmer             0    0    \n",
      "0010.2004-08-01.BG                 1    0    \n",
      "0014.1999-12-14.kaminski           0    0    \n",
      "0006.1999-12-13.kaminski           0    0    \n",
      "0005.1999-12-14.farmer             0    0    \n",
      "0003.2001-02-08.kitchen            0    0    \n",
      "0001.2001-02-07.kitchen            0    0    \n",
      "0008.2001-02-09.kitchen            0    0    \n",
      "0007.2003-12-18.GP                 1    0    \n",
      "0017.2004-08-02.BG                 1    0    \n",
      "0014.2004-08-01.BG                 1    0    \n",
      "0006.2003-12-18.GP                 1    0    \n",
      "0016.2001-07-05.SA_and_HP          1    0    \n",
      "0008.2003-12-18.GP                 1    0    \n",
      "0014.2001-07-04.SA_and_HP          1    0    \n",
      "0001.2001-04-02.williams           0    0    \n",
      "0012.2000-06-08.lokay              0    0    \n",
      "0014.1999-12-15.farmer             0    0    \n",
      "0009.2000-06-07.lokay              0    0    \n",
      "0001.1999-12-10.farmer             0    0    \n",
      "0008.2001-06-25.SA_and_HP          1    0    \n",
      "0017.2001-04-03.williams           0    0    \n",
      "0014.2001-02-12.kitchen            0    0    \n",
      "0016.2001-07-06.SA_and_HP          1    0    \n",
      "0015.1999-12-15.farmer             0    0    \n",
      "0009.1999-12-13.kaminski           0    0    \n",
      "0001.2000-06-06.lokay              0    0    \n",
      "0011.2004-08-01.BG                 1    0    \n",
      "0004.2004-08-01.BG                 1    0    \n",
      "0018.2003-12-18.GP                 1    0    \n",
      "0002.1999-12-13.farmer             0    0    \n",
      "0016.2003-12-19.GP                 1    1    \n",
      "0004.1999-12-14.farmer             0    0    \n",
      "0015.2003-12-19.GP                 1    0    \n",
      "0006.2004-08-01.BG                 1    0    \n",
      "0009.2003-12-18.GP                 1    1    \n",
      "0007.1999-12-14.farmer             0    0    \n",
      "0005.2000-06-06.lokay              0    0    \n",
      "0010.1999-12-14.kaminski           0    0    \n",
      "0007.2000-01-17.beck               0    0    \n",
      "0003.1999-12-14.farmer             0    0    \n",
      "0003.2004-08-01.BG                 1    0    \n",
      "0017.2004-08-01.BG                 1    1    \n",
      "0013.2001-06-30.SA_and_HP          1    0    \n",
      "0003.1999-12-10.kaminski           0    0    \n",
      "0012.1999-12-14.farmer             0    0    \n",
      "0004.1999-12-10.kaminski           0    0    \n",
      "0018.2001-07-13.SA_and_HP          1    0    \n",
      "0002.2001-02-07.kitchen            0    0    \n",
      "0007.2004-08-01.BG                 1    0    \n",
      "0012.1999-12-14.kaminski           0    0    \n",
      "0005.2001-06-23.SA_and_HP          1    0    \n",
      "0007.1999-12-13.kaminski           0    0    \n",
      "0017.2000-01-17.beck               0    0    \n",
      "0006.2001-06-25.SA_and_HP          1    0    \n",
      "0006.2001-04-03.williams           0    0    \n",
      "0005.2001-02-08.kitchen            0    0    \n",
      "0002.2003-12-18.GP                 1    0    \n",
      "0003.2003-12-18.GP                 1    0    \n",
      "0013.2001-04-03.williams           0    0    \n",
      "0004.2001-04-02.williams           0    0    \n",
      "0010.2001-02-09.kitchen            0    0    \n",
      "0001.1999-12-10.kaminski           0    0    \n",
      "0013.1999-12-14.farmer             0    0    \n",
      "0015.1999-12-14.kaminski           0    0    \n",
      "0012.2003-12-19.GP                 1    0    \n",
      "0016.2001-02-12.kitchen            0    0    \n",
      "0002.2004-08-01.BG                 1    0    \n",
      "0002.2001-05-25.SA_and_HP          1    0    \n",
      "0011.2003-12-18.GP                 1    0    \n",
      "\n",
      "Accuracy: 59.00%\n"
     ]
    }
   ],
   "source": [
    "print_predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.5. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify the email messages by all words present.\n",
    "\n",
    "To do so, make sure that mapper.py counts all occurrences of all words, and reducer.py performs a word-distribution-wide Naive Bayes classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: mapper code for HW1.5\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Collect input\n",
    "filename = sys.argv[1]\n",
    "findwords = re.split(\" \",sys.argv[2].lower())\n",
    "\n",
    "# Initialize dictionary to empty\n",
    "word_count = {}\n",
    "\n",
    "with open(filename, \"rU\") as myfile:\n",
    "    for line in myfile:\n",
    "        # Format each line, fields separated by \\t according to enronemail_README.txt\n",
    "        fields = re.split(\"\\t\", line)\n",
    "        fields[3] = fields[3].replace(\"\\n\", \"\")\n",
    "        subj = fields[2].translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "        body = fields[3].translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "        \n",
    "        # For each word, count occurrences in subj and body\n",
    "        # Key is (document id, spam, word)\n",
    "        if findwords[0] == \"*\":\n",
    "            full_text = subj + \" \" + body\n",
    "            for word in full_text.split():\n",
    "                my_key = (fields[0], fields[1], word)\n",
    "                if my_key not in word_count:\n",
    "                    word_count[my_key] = 0.0\n",
    "                word_count[my_key] += 1\n",
    "        else:\n",
    "            for word in findwords:\n",
    "                my_key = (fields[0], fields[1], word)\n",
    "                if my_key not in word_count:\n",
    "                    word_count[my_key] = 0.0\n",
    "                word_count[my_key] += subj.count(word) + body.count(word)\n",
    "\n",
    "for key in word_count:\n",
    "    print [key, word_count[key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: reducer code for HW1.3\n",
    "\n",
    "import sys\n",
    "import math\n",
    "\n",
    "filenames = sys.argv[1:]\n",
    "\n",
    "doc_ids = {}\n",
    "document_words = {}\n",
    "class_counts = {'1':0.0, '0':0.0}\n",
    "vocab = {}\n",
    "word_counts = {\n",
    "    '0': {},\n",
    "    '1': {}\n",
    "}\n",
    "\n",
    "for file in filenames:\n",
    "    # Train classifier with data from mapper.py\n",
    "    with open(file, \"r\") as myfile:\n",
    "        for line in myfile:\n",
    "            pair = eval(line)\n",
    "            doc_id = pair[0][0]\n",
    "            spam = pair[0][1]\n",
    "            word = pair[0][2]\n",
    "            count = int(pair[1])\n",
    "            \n",
    "            # We need to aggregate counts on specific levels. We need:\n",
    "            #    - number of total documents\n",
    "            #    - number of documents per class\n",
    "            #    - our entire vocabulary\n",
    "            #    - word counts per class\n",
    "            #    - word counts per document (for testing purposes)\n",
    "            if doc_id not in doc_ids: \n",
    "                doc_ids[doc_id] = spam\n",
    "                class_counts[spam] += 1\n",
    "                document_words[doc_id] = {}\n",
    "            if word not in vocab: vocab[word] = 0.0\n",
    "            vocab[word] += count\n",
    "            if word not in word_counts[spam]: word_counts[spam][word] = 0.0\n",
    "            word_counts[spam][word] += count\n",
    "            if word not in document_words[doc_id]: document_words[doc_id][word] = 0.0\n",
    "            document_words[doc_id][word] += count\n",
    "            \n",
    "\n",
    "prior_spam = class_counts['1'] / len(doc_ids)\n",
    "prior_ham = class_counts['0'] / len(doc_ids)\n",
    "\n",
    "for doc in document_words:\n",
    "    pred = 'none'\n",
    "\n",
    "    log_prob_spam_word = math.log(prior_spam)\n",
    "    log_prob_ham_word = math.log(prior_ham)\n",
    "    \n",
    "    # Test classifier using training data\n",
    "    for word in document_words[doc]:\n",
    "\n",
    "\n",
    "        # Make sure that the word was in the training data\n",
    "        if word not in vocab: continue\n",
    "\n",
    "        # Calculate P(word)\n",
    "        p_word = vocab[word] / sum(vocab.values())\n",
    "\n",
    "        # Calculate P(word|spam) and P(word|ham)\n",
    "        # Use add-1 smoothing\n",
    "        if word not in word_counts['1']: num_word_spam = 0\n",
    "        else: num_word_spam = word_counts['1'][word]\n",
    "        p_word_spam = (num_word_spam + 1.0) / (sum(word_counts['1'].values()) + len(vocab))\n",
    "\n",
    "        if word not in word_counts['0']: num_word_ham = 0\n",
    "        else: num_word_ham = word_counts['0'][word]\n",
    "        p_word_ham = (num_word_ham + 1.0) / (sum(word_counts['0'].values()) + len(vocab))\n",
    "\n",
    "        # Update probabilities\n",
    "        log_prob_spam_word += math.log(p_word_spam) * document_words[doc][word]\n",
    "        log_prob_ham_word += math.log(p_word_ham) * document_words[doc][word]\n",
    "\n",
    "    if log_prob_spam_word > log_prob_ham_word: pred = '1'\n",
    "    else: pred = '0'\n",
    "\n",
    "    print doc + \"\\t\" + doc_ids[doc] + \"\\t\" + pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change file permissions\n",
    "!chmod a+x mapper.py\n",
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!./pNaiveBayes.sh 4 \"*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output of HW1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document id                        true pred \n",
      "---------------------------------------------\n",
      "0010.2003-12-18.GP                 1    1    \n",
      "0010.2001-06-28.SA_and_HP          1    1    \n",
      "0001.2000-01-17.beck               0    0    \n",
      "0018.1999-12-14.kaminski           0    0    \n",
      "0005.1999-12-12.kaminski           0    0    \n",
      "0011.2001-06-29.SA_and_HP          1    1    \n",
      "0008.2004-08-01.BG                 1    1    \n",
      "0009.1999-12-14.farmer             0    0    \n",
      "0017.2003-12-18.GP                 1    1    \n",
      "0011.2001-06-28.SA_and_HP          1    1    \n",
      "0015.2001-07-05.SA_and_HP          1    1    \n",
      "0015.2001-02-12.kitchen            0    0    \n",
      "0009.2001-06-26.SA_and_HP          1    1    \n",
      "0018.2001-07-13.SA_and_HP          1    1    \n",
      "0012.2000-01-17.beck               0    0    \n",
      "0003.2000-01-17.beck               0    0    \n",
      "0004.2001-06-12.SA_and_HP          1    1    \n",
      "0008.2001-06-12.SA_and_HP          1    1    \n",
      "0007.2001-02-09.kitchen            0    0    \n",
      "0016.2004-08-01.BG                 1    1    \n",
      "0015.2000-06-09.lokay              0    0    \n",
      "0016.1999-12-15.farmer             0    0    \n",
      "0013.2004-08-01.BG                 1    1    \n",
      "0005.2003-12-18.GP                 1    1    \n",
      "0012.2001-02-09.kitchen            0    0    \n",
      "0011.1999-12-14.farmer             0    0    \n",
      "0009.2001-02-09.kitchen            0    0    \n",
      "0006.2001-02-08.kitchen            0    0    \n",
      "0014.2003-12-19.GP                 1    1    \n",
      "0010.1999-12-14.farmer             0    0    \n",
      "0010.2004-08-01.BG                 1    1    \n",
      "0014.1999-12-14.kaminski           0    0    \n",
      "0006.1999-12-13.kaminski           0    0    \n",
      "0005.1999-12-14.farmer             0    0    \n",
      "0003.2001-02-08.kitchen            0    0    \n",
      "0001.2001-02-07.kitchen            0    0    \n",
      "0008.2001-02-09.kitchen            0    0    \n",
      "0007.2003-12-18.GP                 1    1    \n",
      "0017.2004-08-02.BG                 1    1    \n",
      "0014.2004-08-01.BG                 1    1    \n",
      "0006.2003-12-18.GP                 1    1    \n",
      "0016.2001-07-05.SA_and_HP          1    1    \n",
      "0008.2003-12-18.GP                 1    1    \n",
      "0014.2001-07-04.SA_and_HP          1    1    \n",
      "0001.2001-04-02.williams           0    0    \n",
      "0012.2000-06-08.lokay              0    0    \n",
      "0014.1999-12-15.farmer             0    0    \n",
      "0009.2000-06-07.lokay              0    0    \n",
      "0001.1999-12-10.farmer             0    0    \n",
      "0008.2001-06-25.SA_and_HP          1    1    \n",
      "0017.2001-04-03.williams           0    0    \n",
      "0014.2001-02-12.kitchen            0    0    \n",
      "0016.2001-07-06.SA_and_HP          1    1    \n",
      "0015.1999-12-15.farmer             0    0    \n",
      "0009.1999-12-13.kaminski           0    0    \n",
      "0001.2000-06-06.lokay              0    0    \n",
      "0011.2004-08-01.BG                 1    1    \n",
      "0004.2004-08-01.BG                 1    1    \n",
      "0018.2003-12-18.GP                 1    1    \n",
      "0002.1999-12-13.farmer             0    0    \n",
      "0016.2003-12-19.GP                 1    1    \n",
      "0004.1999-12-14.farmer             0    0    \n",
      "0015.2003-12-19.GP                 1    1    \n",
      "0006.2004-08-01.BG                 1    1    \n",
      "0009.2003-12-18.GP                 1    1    \n",
      "0007.1999-12-14.farmer             0    0    \n",
      "0002.2004-08-01.BG                 1    1    \n",
      "0010.1999-12-14.kaminski           0    0    \n",
      "0007.2000-01-17.beck               0    0    \n",
      "0003.1999-12-14.farmer             0    0    \n",
      "0003.2004-08-01.BG                 1    1    \n",
      "0017.2004-08-01.BG                 1    1    \n",
      "0013.2001-06-30.SA_and_HP          1    1    \n",
      "0003.1999-12-10.kaminski           0    0    \n",
      "0012.1999-12-14.farmer             0    0    \n",
      "0004.1999-12-10.kaminski           0    0    \n",
      "0017.1999-12-14.kaminski           0    0    \n",
      "0002.2001-02-07.kitchen            0    0    \n",
      "0007.2004-08-01.BG                 1    1    \n",
      "0012.1999-12-14.kaminski           0    0    \n",
      "0005.2001-06-23.SA_and_HP          1    1    \n",
      "0005.2000-06-06.lokay              0    0    \n",
      "0013.1999-12-14.kaminski           0    0    \n",
      "0007.1999-12-13.kaminski           0    0    \n",
      "0017.2000-01-17.beck               0    0    \n",
      "0006.2001-06-25.SA_and_HP          1    1    \n",
      "0006.2001-04-03.williams           0    0    \n",
      "0005.2001-02-08.kitchen            0    0    \n",
      "0002.2003-12-18.GP                 1    1    \n",
      "0003.2003-12-18.GP                 1    1    \n",
      "0013.2001-04-03.williams           0    0    \n",
      "0004.2001-04-02.williams           0    0    \n",
      "0010.2001-02-09.kitchen            0    0    \n",
      "0001.1999-12-10.kaminski           0    0    \n",
      "0013.1999-12-14.farmer             0    0    \n",
      "0015.1999-12-14.kaminski           0    0    \n",
      "0012.2003-12-19.GP                 1    1    \n",
      "0016.2001-02-12.kitchen            0    0    \n",
      "0002.2001-05-25.SA_and_HP          1    1    \n",
      "0011.2003-12-18.GP                 1    1    \n",
      "\n",
      "Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "print_predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.6 Benchmark your code with the Python SciKit-Learn implementation of multinomial Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up SK-learn libraries and data ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# General libraries\n",
    "import numpy as np\n",
    "\n",
    "# SK-learn libraries for learning\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,) (100,)\n"
     ]
    }
   ],
   "source": [
    "# Read data in and create data and label arrays\n",
    "ids, X, Y = [], [], []\n",
    "\n",
    "with open('enronemail_1h.txt', 'rU') as myfile:\n",
    "    for line in myfile:\n",
    "        fields = line.split(\"\\t\")\n",
    "        text = fields[2] + \" \" + fields[3]\n",
    "        text = text.replace(\"\\n\", \"\")\n",
    "        X.append(text)\n",
    "        Y.append(fields[1])\n",
    "        ids.append(fields[0])\n",
    "\n",
    "# Convert these to numpy arrays\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "# Check that the shapes look correct\n",
    "print X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run the Multinomial Naive Bayes algorithm (using default settings) from SciKit-Learn over the same training data used in HW1.5 and report the training error\n",
    "- Run the Bernoulli Naive Bayes algorithm from SciKit-Learn (using default settings) over the same training data used in HW1.5 and report the training error \n",
    "- Run the Multinomial Naive Bayes algorithm you developed for HW1.5 over the same data used HW1.5 and report the training error \n",
    "- Please prepare a table to present your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hw1_6():\n",
    "    train_errors = {}\n",
    "    \n",
    "    ##### MULTINOMIAL NB\n",
    "    # Create Pipeline to get feature vectors and train\n",
    "    # Use CountVectorizer to get feature arrays\n",
    "    # Classify using Multinomial NB\n",
    "    pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                         ('clf', MultinomialNB()),\n",
    "                        ])\n",
    "\n",
    "    # Fit training data and labels\n",
    "    pipe.fit(X, Y)\n",
    "\n",
    "    # Print training error\n",
    "    predictions = pipe.predict(X)\n",
    "    train_errors[\"Multinomial\"] = sum(predictions != Y) / Y.size\n",
    "    \n",
    "    ##### BERNOULLI NB\n",
    "    # Create Pipeline to get feature vectors and train\n",
    "    # Use CountVectorizer to get feature arrays\n",
    "    # Classify using Bernoulli NB\n",
    "    pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('clf', BernoulliNB()),\n",
    "                    ])\n",
    "\n",
    "    # Fit training data and labels\n",
    "    pipe.fit(X, Y)\n",
    "\n",
    "    # Print training error\n",
    "    predictions = pipe.predict(X)\n",
    "    train_errors[\"Bernoulli\"] = sum(predictions != Y) / Y.size\n",
    "    \n",
    "    ##### CLASSIFIER in HW1.5\n",
    "    # Read output from enronemail_1h.txt.output\n",
    "    \n",
    "    incorrect = 0.0\n",
    "    total = 0.0\n",
    "\n",
    "    with open('enronemail_1h.txt.output', 'rU') as myfile:\n",
    "        for line in myfile:\n",
    "            fields = line.split(\"\\t\")\n",
    "            fields[-1] = fields[-1].replace(\"\\n\", \"\")\n",
    "            total += 1\n",
    "            if fields[1] != fields[2]: incorrect += 1\n",
    "    \n",
    "    train_errors[\"HW1.5\"] = incorrect/total\n",
    "    \n",
    "    ##### TABLE OF TRAINING ERRORS\n",
    "    print \"{:<14s}{:6s}\".format(\"Method\", \"Error\")\n",
    "    print \"--------------------\"\n",
    "    for method in train_errors:\n",
    "        print \"{:<14s}{:4.2%}\".format(method, train_errors[method])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method        Error \n",
      "--------------------\n",
      "Multinomial   0.00%\n",
      "Bernoulli     0.00%\n",
      "HW1.5         0.00%\n"
     ]
    }
   ],
   "source": [
    "hw1_6()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
