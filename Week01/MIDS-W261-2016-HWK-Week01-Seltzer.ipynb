{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASCI W261: Machine Learning at Scale\n",
    "## Assignment Week 1\n",
    "Miki Seltzer (miki.seltzer@berkeley.edu)<br>\n",
    "W261-2, Spring 2016<br>\n",
    "Due date 19-Jan-2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.0.0: Define big data. Provide an example of a big data problem in your domain of expertise. \n",
    "\n",
    "\"Big data\" is a broad term that has many interpretations. It doesn't seem fair to assign a concrete size barrier above which is considered \"big,\" and below which is not. A definition that resonates with me is that if a data set is too large to fit on or process with one computer in a reasonable amount of time, then it is considered big data. This definition allows for more flexibility as hard drives become larger and computers become more powerful.\n",
    "\n",
    "I currently work at a home security company, so we are constantly generating IoT-type data. Analysis on any of the data we generate from wireless sensors is usually a big data problem, unless we are looking at a tiny subset of data. One of the most important problems we are currently trying to solve is determining when a home is unoccupied using data from wireless sensors such as motion detectors and door sensors. It is relatively easy to determine when a home is occupied: we can be fairly certain that someone is home when motion is detected. Conversely, the absence of motion does not always indicate that the home is unoccupied. A simple example is at nighttime when people are usually sleeping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.0.1: In 500 words (English or pseudo code or a combination) describe how to estimate the bias, the variance, the irreducible error for a test dataset T when using polynomial regression models of degree 1, 2, 3, 4, 5 are considered. How would you select a model?\n",
    "\n",
    "In order to estimate the bias, variance and irreducible error (noise) for a single data that is generated from the unknown true function $f(x)$, we first need to generate multiple sets of data from our original data set T. We can do this by sampling the original data set with replacement (bootstrapping). If we repeat the bootstrap resample 50 times, we will have 50 data sets to work with.\n",
    "\n",
    "With each of the 50 new data sets, we split the data set into training set and a testing set. We fit polynomials of degree 1, 2, 3, 4 and 5 to the training set. This yields 50 models per polynomial degree. For each polynomial degree, we can then estimate the average variance and bias using the testing set.\n",
    "\n",
    "#### Variance estimation\n",
    "For each observation, $x$, in the testing set, we now have 50 predictions per polynomial degree, which we denote as $y_1, y_2, ..., y_{50}$. We find the average of these predictions, denoted as $\\bar{y}$. We can find the variance of the predictions using the formula: $E((y_i-\\bar{y})^2)$. We then average the variance for each data point in the testing set, and then repeat the process for each polynomial degree. Thus, we will have one average variance per polynomial degree.\n",
    "\n",
    "#### Bias estimation\n",
    "For each observation, $x$, we also have the actual value of $y$ in the testing set. The bias of the observation $x$ is the difference between the average prediction and the actual value: $\\bar{y}-y$. If we happened to have the true function $f(x)$ from which the data were generated, we would calculate the bias as: $\\bar{y}-f(x)$.\n",
    "\n",
    "We then average the bias for each data point in the testing set per polynomial degree, and repeat the process for each polynomial degree. This yields one average bias per polynomial degree. It is also useful to calculate the average squared bias, which is calculated by squaring the bias before averaging all of the observations in the testing set.\n",
    "\n",
    "#### Noise estimation\n",
    "If we do not know the true function $f(x)$, we are forced to make the assumption that the noise is zero. If we knew the true function $f(x)$ that the data set T was generated from, we would be able to calculate the irreducible error, which would be the square of the difference between the observed values and the true function: $(y-f(x))^2$.\n",
    "\n",
    "#### Model selection\n",
    "We know that there is a trade-off between bias and variance. As the model gets more complex, bias generally decreases, while variance generally increases. We can plot the sum of the squared bias and the variance, and choose the degree where the sum is minimized.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.1: Read through the provided control script (pNaiveBayes.sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print \"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't remove enronemail_1h.txt: Text file busy, skipping file.\r\n"
     ]
    }
   ],
   "source": [
    "!perl -pi -e 's/\\r/\\n/g' enronemail_1h.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to use the pNaiveBayes.sh file multiple times during this homework assignment, so let's make sure that it is written to our working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pNaiveBayes.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile pNaiveBayes.sh\n",
    "## pNaiveBayes.sh\n",
    "## Author: Jake Ryland Williams\n",
    "## Usage: pNaiveBayes.sh m wordlist\n",
    "## Input:\n",
    "##       m = number of processes (maps), e.g., 4\n",
    "##       wordlist = a space-separated list of words in quotes, e.g., \"the and of\"\n",
    "##\n",
    "## Instructions: Read this script and its comments closely.\n",
    "##               Do your best to understand the purpose of each command,\n",
    "##               and focus on how arguments are supplied to mapper.py/reducer.py,\n",
    "##               as this will determine how the python scripts take input.\n",
    "##               When you are comfortable with the unix code below,\n",
    "##               answer the questions on the LMS for HW1 about the starter code.\n",
    "\n",
    "## collect user input\n",
    "m=$1 ## the number of parallel processes (maps) to run\n",
    "wordlist=$2 ## if set to \"*\", then all words are used\n",
    "\n",
    "## a test set data of 100 messages\n",
    "data=\"enronemail_1h.txt\" \n",
    "\n",
    "## the full set of data (33746 messages)\n",
    "# data=\"enronemail.txt\" \n",
    "\n",
    "## 'wc' determines the number of lines in the data\n",
    "## 'perl -pe' regex strips the piped wc output to a number\n",
    "linesindata=`wc -l $data | perl -pe 's/^.*?(\\d+).*?$/$1/'`\n",
    "\n",
    "## determine the lines per chunk for the desired number of processes\n",
    "linesinchunk=`echo \"$linesindata/$m+1\" | bc`\n",
    "\n",
    "## split the original file into chunks by line\n",
    "split -l $linesinchunk $data $data.chunk.\n",
    "\n",
    "## assign python mappers (mapper.py) to the chunks of data\n",
    "## and emit their output to temporary files\n",
    "for datachunk in $data.chunk.*; do\n",
    "    ## feed word list to the python mapper here and redirect STDOUT to a temporary file on disk\n",
    "    ####\n",
    "    ####\n",
    "    ./mapper.py $datachunk \"$wordlist\" > $datachunk.counts &\n",
    "    ####\n",
    "    ####\n",
    "done\n",
    "## wait for the mappers to finish their work\n",
    "wait\n",
    "\n",
    "## 'ls' makes a list of the temporary count files\n",
    "## 'perl -pe' regex replaces line breaks with spaces\n",
    "countfiles=`\\ls $data.chunk.*.counts | perl -pe 's/\\n/ /'`\n",
    "\n",
    "## feed the list of countfiles to the python reducer and redirect STDOUT to disk\n",
    "####\n",
    "####\n",
    "./reducer.py $countfiles > $data.output\n",
    "####\n",
    "####\n",
    "\n",
    "## clean up the data chunks and temporary count files\n",
    "# \\rm $data.chunk.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x pNaiveBayes.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.2: Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will determine the number of occurrences of a single, user-specified word. \n",
    "\n",
    "Examine the word “assistance” and report your results.\n",
    "\n",
    "To do so, make sure that\n",
    "\n",
    "- mapper.py counts all occurrences of a single word, and\n",
    "- reducer.py collates the counts of the single word.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: mapper code for HW1.2\n",
    "\n",
    "import sys\n",
    "import re\n",
    "count = 0\n",
    "\n",
    "# Collect input\n",
    "filename = sys.argv[1]\n",
    "findwords = re.split(\" \",sys.argv[2].lower())\n",
    "\n",
    "# Initialize dictionary to empty\n",
    "wordcount = {}\n",
    "\n",
    "with open(filename, \"rU\") as myfile:\n",
    "    for line in myfile:\n",
    "        # Format each line, fields separated by \\t according to enronemail_README.txt\n",
    "        fields = re.split(\"\\t\", line)\n",
    "        \n",
    "        # For each word in list provided by user, count occurrences in subj and body\n",
    "        for word in findwords:\n",
    "            if word not in wordcount:\n",
    "                wordcount[word] = 0 \n",
    "            wordcount[word] += fields[2].count(word) + fields[3].count(word)\n",
    "\n",
    "for word in wordcount:\n",
    "    print [word, wordcount[word]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: reducer code for HW1.2\n",
    "\n",
    "import sys\n",
    "sum = 0\n",
    "\n",
    "filenames = sys.argv[1:]\n",
    "\n",
    "wordcount = {}\n",
    "\n",
    "# Each mapper outputs a [word, count] pair\n",
    "# For each file and each word, sum the counts\n",
    "for file in filenames:\n",
    "    with open(file, \"rU\") as myfile:\n",
    "        for line in myfile:\n",
    "            pair = eval(line)\n",
    "            word = pair[0]\n",
    "            count = pair[1]\n",
    "            if word not in wordcount:\n",
    "                wordcount[word] = 0\n",
    "            wordcount[word] += count\n",
    "            \n",
    "for word in wordcount:\n",
    "    print word, \"\\t\", wordcount[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change file permissions\n",
    "!chmod a+x mapper.py\n",
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!./pNaiveBayes.sh 4 \"assistance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_counts():\n",
    "    with open(\"enronemail_1h.txt.output\", \"r\") as myfile:\n",
    "        print \"{:<15s}{:3s}\".format(\"word\", \"count\")\n",
    "        print \"----------------------\"\n",
    "        for line in myfile:\n",
    "            pair = line.split(\"\\t\")\n",
    "            word = pair[0]\n",
    "            count = int(pair[1])\n",
    "            print \"{:<15s}{:3d}\".format(word, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output of HW1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word           count\n",
      "----------------------\n",
      "assistance      10\n"
     ]
    }
   ],
   "source": [
    "print_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.3. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify the email messages by a single, user-specified word using the multinomial Naive Bayes Formulation. \n",
    "\n",
    "Examine the word “assistance” and report your results. To do so, make sure that mapper.py and reducer.py perform a single word Naive Bayes classification. For multinomial Naive Bayes, the Pr(X=“assistance”|Y=SPAM) is calculated as follows: \n",
    "\n",
    "$$\n",
    "\\frac{\\text{number of times \"assistance\" occurs in SPAM labeled documents}}{\\text{the number of words in documents labeled SPAM}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: mapper code for HW1.3\n",
    "\n",
    "import sys\n",
    "import re\n",
    "count = 0\n",
    "\n",
    "# Collect input\n",
    "filename = sys.argv[1]\n",
    "findwords = re.split(\" \",sys.argv[2].lower())\n",
    "\n",
    "# Initialize dictionary to empty\n",
    "word_count = {}\n",
    "\n",
    "with open(filename, \"rU\") as myfile:\n",
    "    for line in myfile:\n",
    "        # Format each line, fields separated by \\t according to enronemail_README.txt\n",
    "        fields = re.split(\"\\t\", line)\n",
    "        \n",
    "        # For each word in list provided by user, count occurrences in subj and body\n",
    "        for word in findwords:\n",
    "            my_key = (fields[0], fields[1], word)\n",
    "            if my_key not in word_count:\n",
    "                word_count[my_key] = 0 \n",
    "            word_count[my_key] += fields[2].count(word) + fields[3].count(word)\n",
    "\n",
    "for key in word_count:\n",
    "    print [key, word_count[key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: reducer code for HW1.3\n",
    "\n",
    "import sys\n",
    "sum = 0\n",
    "\n",
    "filenames = sys.argv[1:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change file permissions\n",
    "!chmod a+x mapper.py\n",
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!./pNaiveBayes.sh 4 \"assistance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.56\n"
     ]
    }
   ],
   "source": [
    "filenames = ['enronemail_1h_edit.txt.chunk.aa.counts',\n",
    "             'enronemail_1h_edit.txt.chunk.ab.counts',\n",
    "             'enronemail_1h_edit.txt.chunk.ac.counts',\n",
    "             'enronemail_1h_edit.txt.chunk.ad.counts',]\n",
    "\n",
    "doc_ids = []\n",
    "class_counts = {}\n",
    "word_counts = {}\n",
    "\n",
    "for file in filenames:\n",
    "    with open(file, \"r\") as myfile:\n",
    "        for line in myfile:\n",
    "            pair = eval(line)\n",
    "            doc_id = pair[0][0]\n",
    "            spam = pair[0][1]\n",
    "            word = pair[0][2]\n",
    "            count = int(pair[1])\n",
    "            if doc_id not in doc_ids: doc_ids.append(doc_id)\n",
    "            if spam not in class_counts: class_counts[spam] = 0.0\n",
    "            class_counts[spam] += 1\n",
    "            if word not in word_counts: word_counts[word] = 0.0\n",
    "            word_counts[word] += count\n",
    "\n",
    "prior_spam = class_counts['0'] / len(doc_ids)\n",
    "prior_ham = class_counts['1'] / len(doc_ids)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('0012.1999-12-14.farmer', '0', 'assistance'), 0]\n",
      "[('0007.2000-01-17.beck', '0', 'assistance'), 0]\n",
      "[('0003.1999-12-14.farmer', '0', 'assistance'), 0]\n",
      "[('0001.2000-01-17.beck', '0', 'assistance'), 0]\n",
      "[('0016.2003-12-19.GP', '1', 'assistance'), 0]\n",
      "[('0013.2001-04-03.williams', '0', 'assistance'), 0]\n",
      "[('0013.2001-06-30.SA_and_HP', '1', 'assistance'), 0]\n",
      "[('0001.2001-02-07.kitchen', '0', 'assistance'), 0]\n",
      "[('0009.2001-02-09.kitchen', '0', 'assistance'), 0]\n",
      "[('0006.1999-12-13.kaminski', '0', 'assistance'), 0]\n",
      "[('0018.2003-12-18.GP', '1', 'assistance'), 1]\n",
      "[('0005.2001-02-08.kitchen', '0', 'assistance'), 0]\n",
      "[('0016.2001-02-12.kitchen', '0', 'assistance'), 0]\n",
      "[('0015.1999-12-15.farmer', '0', 'assistance'), 0]\n",
      "[('0017.2004-08-01.BG', '1', 'assistance'), 0]\n",
      "[('0010.2001-06-28.SA_and_HP', '1', 'assistance'), 1]\n",
      "[('0010.1999-12-14.kaminski', '0', 'assistance'), 0]\n",
      "[('0017.1999-12-14.kaminski', '0', 'assistance'), 0]\n",
      "[('0013.2004-08-01.BG', '1', 'assistance'), 1]\n",
      "[('0018.1999-12-14.kaminski', '0', 'assistance'), 0]\n",
      "[('0002.2001-02-07.kitchen', '0', 'assistance'), 0]\n",
      "[('0012.1999-12-14.kaminski', '0', 'assistance'), 0]\n",
      "[('0016.2001-07-06.SA_and_HP', '1', 'assistance'), 0]\n",
      "[('0015.2000-06-09.lokay', '0', 'assistance'), 0]\n",
      "[('0009.1999-12-13.kaminski', '0', 'assistance'), 0]\n",
      "[('0010.2001-02-09.kitchen', '0', 'assistance'), 0]\n",
      "[('0016.2004-08-01.BG', '1', 'assistance'), 0]\n",
      "[('0003.2000-01-17.beck', '0', 'assistance'), 0]\n",
      "[('0006.2001-04-03.williams', '0', 'assistance'), 0]\n",
      "[('0004.1999-12-14.farmer', '0', 'assistance'), 0]\n",
      "[('0003.2004-08-01.BG', '1', 'assistance'), 0]\n",
      "[('0002.2003-12-18.GP', '1', 'assistance'), 0]\n",
      "[('0006.2004-08-01.BG', '1', 'assistance'), 0]\n",
      "[('0007.1999-12-13.kaminski', '0', 'assistance'), 0]\n",
      "[('0010.1999-12-14.farmer', '0', 'assistance'), 0]\n",
      "[('0017.2003-12-18.GP', '1', 'assistance'), 0]\n",
      "[('0005.1999-12-12.kaminski', '0', 'assistance'), 1]\n",
      "[('0017.2004-08-02.BG', '1', 'assistance'), 0]\n",
      "[('0011.1999-12-14.farmer', '0', 'assistance'), 0]\n",
      "[('0005.2000-06-06.lokay', '0', 'assistance'), 0]\n",
      "[('0002.1999-12-13.farmer', '0', 'assistance'), 0]\n",
      "[('0012.2001-02-09.kitchen', '0', 'assistance'), 0]\n",
      "[('0006.2001-06-25.SA_and_HP', '1', 'assistance'), 0]\n",
      "[('0011.2001-06-29.SA_and_HP', '1', 'assistance'), 0]\n",
      "[('0010.2003-12-18.GP', '1', 'assistance'), 0]\n",
      "[('0015.2001-07-05.SA_and_HP', '1', 'assistance'), 0]\n",
      "[('0016.1999-12-15.farmer', '0', 'assistance'), 0]\n",
      "[('0015.2001-02-12.kitchen', '0', 'assistance'), 0]\n",
      "[('0012.2000-06-08.lokay', '0', 'assistance'), 0]\n",
      "[('0002.2004-08-01.BG', '1', 'assistance'), 1]\n",
      "[('0007.2001-02-09.kitchen', '0', 'assistance'), 0]\n",
      "[('0006.2001-02-08.kitchen', '0', 'assistance'), 0]\n",
      "[('0007.1999-12-14.farmer', '0', 'assistance'), 0]\n",
      "[('0016.2001-07-05.SA_and_HP', '1', 'assistance'), 0]\n",
      "[('0003.2003-12-18.GP', '1', 'assistance'), 0]\n",
      "[('0008.2001-06-25.SA_and_HP', '1', 'assistance'), 0]\n",
      "[('0009.2003-12-18.GP', '1', 'assistance'), 0]\n",
      "[('0004.1999-12-10.kaminski', '0', 'assistance'), 1]\n",
      "[('0006.2003-12-18.GP', '1', 'assistance'), 0]\n",
      "[('0013.1999-12-14.farmer', '0', 'assistance'), 0]\n",
      "[('0014.2003-12-19.GP', '1', 'assistance'), 0]\n",
      "[('0011.2001-06-28.SA_and_HP', '1', 'assistance'), 1]\n",
      "[('0009.2001-06-26.SA_and_HP', '1', 'assistance'), 0]\n",
      "[('0009.2000-06-07.lokay', '0', 'assistance'), 0]\n",
      "[('0001.2001-04-02.williams', '0', 'assistance'), 0]\n",
      "[('0014.2004-08-01.BG', '1', 'assistance'), 0]\n",
      "[('0003.2001-02-08.kitchen', '0', 'assistance'), 0]\n",
      "[('0010.2004-08-01.BG', '1', 'assistance'), 0]\n",
      "[('0011.2003-12-18.GP', '1', 'assistance'), 0]\n",
      "[('0014.2001-07-04.SA_and_HP', '1', 'assistance'), 0]\n",
      "[('0015.1999-12-14.kaminski', '0', 'assistance'), 0]\n",
      "[('0007.2004-08-01.BG', '1', 'assistance'), 0]\n",
      "[('0002.2001-05-25.SA_and_HP', '1', 'assistance'), 0]\n",
      "[('0004.2004-08-01.BG', '1', 'assistance'), 0]\n",
      "[('0001.1999-12-10.kaminski', '0', 'assistance'), 0]\n",
      "[('0017.2000-01-17.beck', '0', 'assistance'), 0]\n",
      "[('0001.2000-06-06.lokay', '0', 'assistance'), 0]\n",
      "[('0018.2001-07-13.SA_and_HP', '1', 'assistance'), 3]\n",
      "[('0005.2003-12-18.GP', '1', 'assistance'), 0]\n",
      "[('0014.2001-02-12.kitchen', '0', 'assistance'), 0]\n",
      "[('0008.2001-06-12.SA_and_HP', '1', 'assistance'), 0]\n",
      "[('0005.1999-12-14.farmer', '0', 'assistance'), 0]\n",
      "[('0014.1999-12-15.farmer', '0', 'assistance'), 0]\n",
      "[('0008.2004-08-01.BG', '1', 'assistance'), 0]\n",
      "[('0013.1999-12-14.kaminski', '0', 'assistance'), 0]\n",
      "[('0004.2001-06-12.SA_and_HP', '1', 'assistance'), 0]\n",
      "[('0009.1999-12-14.farmer', '0', 'assistance'), 0]\n",
      "[('0012.2000-01-17.beck', '0', 'assistance'), 0]\n",
      "[('0008.2001-02-09.kitchen', '0', 'assistance'), 0]\n",
      "[('0004.2001-04-02.williams', '0', 'assistance'), 0]\n",
      "[('0011.2004-08-01.BG', '1', 'assistance'), 0]\n",
      "[('0015.2003-12-19.GP', '1', 'assistance'), 0]\n",
      "[('0008.2003-12-18.GP', '1', 'assistance'), 0]\n",
      "[('0017.2001-04-03.williams', '0', 'assistance'), 0]\n",
      "[('0001.1999-12-10.farmer', '0', 'assistance'), 0]\n",
      "[('0003.1999-12-10.kaminski', '0', 'assistance'), 0]\n",
      "[('0005.2001-06-23.SA_and_HP', '1', 'assistance'), 0]\n",
      "[('0007.2003-12-18.GP', '1', 'assistance'), 0]\n",
      "[('0014.1999-12-14.kaminski', '0', 'assistance'), 0]\n",
      "[('0012.2003-12-19.GP', '1', 'assistance'), 0]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "findwords = ['assistance']\n",
    "word_count = {}\n",
    "\n",
    "with open(\"enronemail_1h.txt\", \"rU\") as myfile:\n",
    "    for line in myfile:\n",
    "        # Format each line, fields separated by \\t according to enronemail_README.txt\n",
    "        fields = re.split(\"\\t\", line)\n",
    "        \n",
    "        # For each word in list provided by user, count occurrences in subj and body\n",
    "        for word in findwords:\n",
    "            my_key = (fields[0], fields[1], word)\n",
    "            if my_key not in word_count:\n",
    "                word_count[my_key] = 0 \n",
    "            word_count[my_key] += fields[2].count(word) + fields[3].count(word)\n",
    "\n",
    "for key in word_count:\n",
    "    print [key, word_count[key]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
