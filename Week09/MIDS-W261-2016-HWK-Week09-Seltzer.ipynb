{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASCI W261: Machine Learning at Scale\n",
    "## Assignment Week 9\n",
    "Miki Seltzer (miki.seltzer@berkeley.edu)<br>\n",
    "W261-2, Spring 2016<br>\n",
    "Submission: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We will need these so we can reload modules as we modify them\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using configs in /etc/mrjob.conf\n",
      "using existing scratch bucket mrjob-ac40f1afcc0b86ce\n",
      "using s3://mrjob-ac40f1afcc0b86ce/tmp/ as our scratch dir on S3\n",
      "Creating persistent job flow to run several jobs in...\n",
      "creating tmp directory /tmp/no_script.cloudera.20160313.010339.237936\n",
      "writing master bootstrap script to /tmp/no_script.cloudera.20160313.010339.237936/b.py\n",
      "Copying non-input files into s3://mrjob-ac40f1afcc0b86ce/tmp/no_script.cloudera.20160313.010339.237936/files/\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Creating Elastic MapReduce job flow\n",
      "Job flow created with ID: j-3D12NC1JXN44F\n",
      "j-3D12NC1JXN44F\n"
     ]
    }
   ],
   "source": [
    "# Just in case we need a cluster\n",
    "# Create job flow so that we don't need to keep spinning up clusters\n",
    "!python -m mrjob.tools.emr.create_job_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clusterId = 'j-3D12NC1JXN44F'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW9.0\n",
    "\n",
    "### What is PageRank and what is it used for in the context of web search?\n",
    "PageRank is a ranking algorithm used by Google. At a high level, it provides a measure of \"popularity\" of pages, due to the underlying assumption that important websites are likely to have more incoming links than unimportant websites.\n",
    "\n",
    "### What modifications have to be made to the webgraph in order to leverage the machinery of Markov Chains to compute the steady stade distibution?\n",
    "We must introduce a \"teleportation\" factor by scaling the webgraph by $\\alpha$ and adding a factor of $(1-\\alpha) * (\\text{matrix with entries }1/n)$. This results in a Markov matrix since the entries will be strictly positive (Perron-Frobenius theorem)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW9.1: MRJob implementation of basic PageRank\n",
    "\n",
    "Write a basic MRJob implementation of the iterative PageRank algorithm that takes sparse adjacency lists as input (as explored in HW 7). Make sure that you implementation utilizes teleportation (1-damping/the number of nodes in the network), and further, distributes the mass of dangling nodes with each iteration so that the output of each iteration is correctly normalized (sums to 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utilize last week's HW to count the number of nodes in the graph\n",
    "from MRJob_Explore import explore\n",
    "\n",
    "def countNodes(filename):\n",
    "\n",
    "    mr_job = explore(args=[filename, '--no-strict-protocols', '--exploreType', 'nodes'])\n",
    "#                            '-r', 'emr', '--emr-job-flow-id', clusterId,])\n",
    "    output = []\n",
    "    \n",
    "    with mr_job.make_runner() as runner:\n",
    "        runner.run()\n",
    "        \n",
    "        for line in runner.stream_output():\n",
    "            out = mr_job.parse_output_line(line)\n",
    "            print 'Number of nodes =', '{:,d}'.format(out[1])\n",
    "    \n",
    "    return out[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MRJob to initialize the starting PageRank vector with a uniform distribution\n",
    "\n",
    "This job requires us to do several things:\n",
    "- Find all nodes in the graph (dangling nodes will not have an explicit neighbors list)\n",
    "- For each node, maintain graph structure and initialize PageRank to 1/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting PageRank_Initialize.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile PageRank_Initialize.py\n",
    "from __future__ import division\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class initialize(MRJob):\n",
    "    \n",
    "    \"\"\"\n",
    "    Get all nodes\n",
    "    \"\"\"\n",
    "    \n",
    "    #------------------\n",
    "    # Mapper:\n",
    "    # - We need to make sure we emit a line for each node in the graph\n",
    "    # - Right now there are no lines for nodes with no neighbors\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        # Split fields\n",
    "        \n",
    "        fields = line.split('\\t')\n",
    "        key = fields[0]\n",
    "        stripe = eval(fields[1])\n",
    "        \n",
    "        # Emit the key and stripe\n",
    "        \n",
    "        yield key, stripe\n",
    "        \n",
    "        # For each neighbor, emit a 0\n",
    "        # We just do this so we catch all nodes\n",
    "        \n",
    "        for neighbor in stripe:\n",
    "            yield neighbor, 0\n",
    "            \n",
    "    #------------------\n",
    "    # Reducer:\n",
    "    # - We need to deduplicate each of our nodes\n",
    "    # - If we encounter a value that is a dictionary, these are the neighbors\n",
    "    # - If we do not encounter any dictionaries, then the node is dangling, we emit an empty neighbor list\n",
    "    \n",
    "    def reducer(self, key, values):       \n",
    "        stripe = {}\n",
    "        \n",
    "        # Loop through values for a key to see if it has neighbors\n",
    "        # If it does, we need to keep the neighbors\n",
    "        \n",
    "        for val in values:\n",
    "            if type(val) == type(stripe):\n",
    "                stripe = val\n",
    "                \n",
    "        # For each key, emit only one thing, which is the neighbor list\n",
    "        # We should now have a line for each node, even if the neighbor list is empty\n",
    "        \n",
    "        yield key, stripe\n",
    "        \n",
    "    \"\"\"\n",
    "    Normalize length\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize total to 0\n",
    "    def mapper_norm_init(self):\n",
    "        self.total = 0.0\n",
    "    \n",
    "    # For each key we encounter, increment total\n",
    "    # We know that we will only encounter each node once\n",
    "    def mapper_norm(self, key, value):\n",
    "        yield key, value\n",
    "        self.total += 1\n",
    "        \n",
    "    # Emit the total number of nodes we saw\n",
    "    def mapper_norm_final(self):\n",
    "        yield '*', self.total\n",
    "    \n",
    "    # To combine the totals if we have multiple mappers\n",
    "    def combiner_norm(self, key, values):\n",
    "        if key == '*':\n",
    "            yield key, sum(values)\n",
    "        else:\n",
    "            for val in values:\n",
    "                yield key, val\n",
    "        \n",
    "    # Initialize the totalNodes to 0\n",
    "    def reducer_norm_init(self):\n",
    "        self.totalNodes = 0\n",
    "       \n",
    "    # If the key is '*', save the sum of the values\n",
    "    # Otherwise, yield the key, stripe, and 1/n\n",
    "    def reducer_norm(self, key, values):\n",
    "        if key == '*':\n",
    "            self.totalNodes = sum(values)\n",
    "        else:\n",
    "            for val in values:\n",
    "                yield key, (val, 1 / self.totalNodes)\n",
    "    \n",
    "    \"\"\"\n",
    "    Multi-step pipeline\n",
    "    \"\"\"\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper,\n",
    "                   reducer=self.reducer),\n",
    "            MRStep(mapper_init=self.mapper_norm_init,\n",
    "                   mapper=self.mapper_norm,\n",
    "                   mapper_final=self.mapper_norm_final,\n",
    "                   combiner=self.combiner_norm,\n",
    "                   reducer_init=self.reducer_norm_init,\n",
    "                   reducer=self.reducer_norm,\n",
    "                   jobconf={'mapred.reduce.tasks': 1})\n",
    "        ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    initialize.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from PageRank_Initialize import initialize\n",
    "\n",
    "def initializePR(filename, runner, outputDir):\n",
    "\n",
    "    if runner == 'local':\n",
    "        mr_job = initialize(args=[filename, '--no-strict-protocols'])\n",
    "        \n",
    "    elif runner == 'hadoop':\n",
    "        !hdfs dfs -rm -r {outputDir}\n",
    "        mr_job = initialize(args=[filename, '--no-strict-protocols', '-r', 'hadoop', '--hadoop-home', '/usr/',\n",
    "                                 '--output-dir', outputDir])\n",
    "        \n",
    "    elif runner == 'emr':\n",
    "        !aws s3 rm --quiet {outputDir}\n",
    "        mr_job = initialize(args=[filename, '--no-strict-protocols', '--no-output', \n",
    "                                  '-r', 'emr', '--emr-job-flow-id', clusterId, '--output-dir', outputDir])\n",
    "        \n",
    "\n",
    "    with mr_job.make_runner() as runner:\n",
    "        runner.run()\n",
    "\n",
    "        if runner != 'emr':\n",
    "            for line in runner.stream_output():\n",
    "                print mr_job.parse_output_line(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes = 11\n",
      "Deleted /user/miki/week09/initialize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.compat:Detected hadoop configuration property names that do not match hadoop version 2.6.0:\n",
      "The have been translated as follows\n",
      " mapred.reduce.tasks: mapreduce.job.reduces\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('A', [{}, 0.09090909090909091])\n",
      "('B', [{'C': 1}, 0.09090909090909091])\n",
      "('C', [{'B': 1}, 0.09090909090909091])\n",
      "('D', [{'A': 1, 'B': 1}, 0.09090909090909091])\n",
      "('E', [{'B': 1, 'D': 1, 'F': 1}, 0.09090909090909091])\n",
      "('F', [{'B': 1, 'E': 1}, 0.09090909090909091])\n",
      "('G', [{'B': 1, 'E': 1}, 0.09090909090909091])\n",
      "('H', [{'B': 1, 'E': 1}, 0.09090909090909091])\n",
      "('I', [{'B': 1, 'E': 1}, 0.09090909090909091])\n",
      "('J', [{'E': 1}, 0.09090909090909091])\n",
      "('K', [{'E': 1}, 0.09090909090909091])\n"
     ]
    }
   ],
   "source": [
    "inputFile = 'PageRank-test.txt'\n",
    "outputDir = '/user/miki/week09/initialize'\n",
    "n = countNodes(inputFile)\n",
    "\n",
    "initializePR(inputFile, 'hadoop', outputDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MRJob to iterate over the PageRank algorithm\n",
    "\n",
    "In this job we need to do the following:\n",
    "- Distribute a node's PageRank to its neighbors\n",
    "- Distribute the mass of dangling nodes, and account for teleporting\n",
    "\n",
    "#### Distribute node's PageRank to neighbors\n",
    "\n",
    "For each node we encounter, we have the current PageRank (let's call this $\\text{PR}_0(A)$ for node A), and the list of neighbors. We need to divide the PageRank by the number of neighbors, and emit this to each neighbor. If there are no neighbors, we cannot emit the PageRank anywhere, so we accumulate it to a special key, \\*dangling. Once we accumulate the distributed PageRank mass in each node, we have a preliminary PageRank for each node, which we can denote as $\\text{PR}_1(A)$.\n",
    "\n",
    "#### Distribute the mass of dangling nodes and account for teleporting\n",
    "\n",
    "We have accumulated the total PageRank mass that has not yet been distributed, because there were no neighbors to distribute the mass to. We also need to account for teleporting. Given the dangling mass, $m$, and the damping factor, $\\alpha$, and $n$ nodes, we have the following:\n",
    "\n",
    "$$\n",
    "\\text{PR}_{final}(A) = \\alpha \\bigg(\\frac{1}{n}\\bigg) + (1-\\alpha)\\bigg(\\frac{m}{n} + \\text{PR}_1(A)\\bigg)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting PageRank_Iterate.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile PageRank_Iterate.py\n",
    "from __future__ import division\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.protocol import JSONProtocol\n",
    "\n",
    "class iterate(MRJob):\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Configurations\n",
    "    \"\"\"\n",
    "    \n",
    "    def configure_options(self):\n",
    "        super(iterate, self).configure_options()\n",
    "        self.add_passthrough_option('--numNodes', default=1, type='int')\n",
    "        self.add_passthrough_option('--alpha', default=0.15, type='float')\n",
    "    \n",
    "    \"\"\"\n",
    "    Mapper: Distribute PageRank mass to all neighbors\n",
    "    - Do not account for teleportation yet\n",
    "    \"\"\"\n",
    "    \n",
    "    INPUT_PROTOCOL = JSONProtocol\n",
    "    \n",
    "    #------------------\n",
    "    # Mapper:\n",
    "    # - Find the number of neighbors for the node\n",
    "    # - Distribute current PageRank among all neighbors\n",
    "    # - If there are no neighbors, keep track of dangling mass\n",
    "    \n",
    "    def mapper_dist(self, key, value):\n",
    "\n",
    "        # Divide the current PageRank by the number of neighbors\n",
    "        \n",
    "        numNeighbors = len(value[0])\n",
    "        PageRank = value[1]\n",
    "        \n",
    "        # If there are neighbors, distribute the PageRank to each neighbors\n",
    "        \n",
    "        if numNeighbors > 0:\n",
    "            for neighbor in value[0]:\n",
    "                yield neighbor, PageRank / numNeighbors\n",
    "                \n",
    "        # If there are no neighbors, we need to account for this dangling node\n",
    "        \n",
    "        else:\n",
    "            yield '*dangling', PageRank\n",
    "        \n",
    "        # Maintain the graph structure\n",
    "        \n",
    "        yield key, value[0]\n",
    "     \n",
    "    #------------------\n",
    "    # Reducer:\n",
    "    # - For each node, accumulate PageRank distributed from other nodes\n",
    "    # - Maintain graph structure\n",
    "    \n",
    "    def reducer_dist(self, key, values):\n",
    "        \n",
    "        new_PageRank = 0.0\n",
    "        neighbors = {}\n",
    "        \n",
    "        for val in values:\n",
    "            if type(val) == type(0.0):\n",
    "                new_PageRank += val\n",
    "            elif type(val) == type({}):\n",
    "                neighbors = val\n",
    "        \n",
    "        if key == '*dangling':\n",
    "            with open('danglingMass.txt', 'w') as myfile:\n",
    "                myfile.write(str(new_PageRank))\n",
    "        else:\n",
    "            yield key, (neighbors, new_PageRank)\n",
    "\n",
    "    #------------------\n",
    "    # Mapper: \n",
    "    # - Account for teleportation\n",
    "    # - Distribute dangling mass to all nodes\n",
    "    \n",
    "    def mapper_dangle_init(self):\n",
    "        with open('danglingMass.txt', 'r') as f:\n",
    "            for line in f:\n",
    "                self.m = float(line)\n",
    "    \n",
    "    def mapper_dangle(self, key, value):\n",
    "        a = self.options.alpha\n",
    "        n = self.options.numNodes\n",
    "        new_PageRank = a * (1 / n) + (1 - a) * (self.m / n + value[1])\n",
    "        yield key, (value[0], new_PageRank)\n",
    "    \n",
    "            \n",
    "    \"\"\"\n",
    "    Multi-step pipeline\n",
    "    \"\"\"\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_dist,\n",
    "                   reducer=self.reducer_dist),\n",
    "            MRStep(mapper_init=self.mapper_dangle_init,\n",
    "                   mapper=self.mapper_dangle)\n",
    "            ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    iterate.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from PageRank_Iterate import iterate\n",
    "\n",
    "def iteratePR(filename, n, a, runnerType, outputDir, iterations):\n",
    "    \n",
    "    thisInputDir = filename\n",
    "    thisOutputDir = outputDir + str(0)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        \n",
    "        output = []\n",
    "        print '\\n'\n",
    "        \n",
    "        if runnerType == 'local':\n",
    "            mr_job = iterate(args=[thisInputDir + '.txt', '--no-strict-protocols', '--numNodes=' + str(n), \n",
    "                                   '--file', 'danglingMass.txt', '--alpha=' + str(a)])\n",
    "\n",
    "        elif runnerType == 'hadoop':\n",
    "            !hdfs dfs -rm -r {thisOutputDir}\n",
    "            mr_job = iterate(args=[thisInputDir, '--no-strict-protocols', '-r', 'hadoop', '--hadoop-home', '/usr/',\n",
    "                                   '--output-dir', thisOutputDir, '--numNodes=' + str(n), '--file', 'danglingMass.txt',\n",
    "                                   '--alpha=' + str(a)])\n",
    "\n",
    "        elif runnerType == 'emr':\n",
    "            !aws s3 rm --quiet {thisOutputDir}\n",
    "            mr_job = iterate(args=[thisInputDir, '--no-strict-protocols', '--no-output', '--numNodes=' + str(n),\n",
    "                                   '-r', 'emr', '--emr-job-flow-id', clusterId, '--output-dir', thisOutputDir,\n",
    "                                   '--file', 'danglingMass.txt', '--alpha=' + str(a)])\n",
    "\n",
    "\n",
    "        with mr_job.make_runner() as runner:\n",
    "            runner.run()\n",
    "\n",
    "            if runnerType != 'emr':\n",
    "                for line in runner.stream_output():\n",
    "                    out = mr_job.parse_output_line(line)\n",
    "                    output.append(out)\n",
    "                    print out\n",
    "                    \n",
    "            if runnerType == 'local':\n",
    "                with open(thisOutputDir + '.txt', 'w') as f:\n",
    "                    for line in output:\n",
    "                        f.writelines('\"' + line[0] + '\"\\t' + str(line[1]) + '\\n' )\n",
    "                        \n",
    "                    \n",
    "        thisInputDir = outputDir + str(i)\n",
    "        thisOutputDir = outputDir + str(i + 1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "('A', [{}, 0.059297520661157725])\n",
      "('B', [{'C': 1}, 0.3168732782369153])\n",
      "('C', [{'B': 1}, 0.09793388429752137])\n",
      "('D', [{'A': 1, 'B': 1}, 0.04641873278236985])\n",
      "('E', [{'B': 1, 'D': 1, 'F': 1}, 0.3297520661157031])\n",
      "('F', [{'B': 1, 'E': 1}, 0.04641873278236985])\n",
      "('G', [{'B': 1, 'E': 1}, 0.02066115702479409])\n",
      "('H', [{'B': 1, 'E': 1}, 0.02066115702479409])\n",
      "('I', [{'B': 1, 'E': 1}, 0.02066115702479409])\n",
      "('J', [{'E': 1}, 0.02066115702479409])\n",
      "('K', [{'E': 1}, 0.02066115702479409])\n",
      "\n",
      "\n",
      "('A', [{}, 0.018218444778365452])\n"
     ]
    }
   ],
   "source": [
    "inputFile = 'PageRank-testInitialized'\n",
    "outputDir = 'iterations/iteration'\n",
    "\n",
    "k = 2\n",
    "\n",
    "iteratePR(inputFile, n, 0.15, 'local', outputDir, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Wikipedia data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputFile = 's3://ucb-mids-mls-networks/wikipedia/all-pages-indexed-out.txt'\n",
    "outputDir = 's3://ms-w261-hw09/wikipedia/initialize'\n",
    "\n",
    "initializePR(inputFile, 'emr', outputDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
