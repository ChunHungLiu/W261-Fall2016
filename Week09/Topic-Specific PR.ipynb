{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# We will need these so we can reload modules as we modify them\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utilize last week's HW to count the number of nodes in the graph\n",
    "from MRJob_Explore import explore\n",
    "\n",
    "def countNodes(filename):\n",
    "\n",
    "    mr_job = explore(args=[filename, '--no-strict-protocols', '--exploreType', 'nodes'])\n",
    "#                            '-r', 'emr', '--emr-job-flow-id', clusterId,])\n",
    "    output = []\n",
    "    \n",
    "    with mr_job.make_runner() as runner:\n",
    "        runner.run()\n",
    "        \n",
    "        for line in runner.stream_output():\n",
    "            out = mr_job.parse_output_line(line)\n",
    "            print 'Number of nodes =', '{:,d}'.format(out[1])\n",
    "    \n",
    "    return out[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PageRank_Initialize import initialize\n",
    "from PageRank_Iterate import iterate\n",
    "from PageRank_TopN import topN\n",
    "\n",
    "def initializePR(filename, runnerType, outputDir, printOutput):\n",
    "    \n",
    "    if runnerType == 'local':\n",
    "        mr_job = initialize(args=[filename, '--no-strict-protocols'])\n",
    "        \n",
    "    elif runnerType == 'hadoop':\n",
    "        !hdfs dfs -rm -r {outputDir}\n",
    "        mr_job = initialize(args=[filename, '--no-strict-protocols', '-r', 'hadoop', '--hadoop-home', '/usr/',\n",
    "                                 '--output-dir', outputDir])\n",
    "        \n",
    "    elif runnerType == 'emr':\n",
    "        !aws s3 rm --quiet {outputDir}\n",
    "        mr_job = initialize(args=[filename, '--no-strict-protocols', '--no-output', \n",
    "                                  '-r', 'emr', '--emr-job-flow-id', clusterId, '--output-dir', outputDir])\n",
    "        \n",
    "\n",
    "    with mr_job.make_runner() as runner:\n",
    "        runner.run()\n",
    "\n",
    "        if printOutput:\n",
    "            for line in runner.stream_output():\n",
    "                print mr_job.parse_output_line(line)\n",
    "\n",
    "def iteratePR(filename, n, a, runnerType, outputDir, iterations, printOutput):\n",
    "\n",
    "    output = []\n",
    "\n",
    "    if runnerType == 'local':\n",
    "        mr_job = iterate(args=[filename, '--no-strict-protocols', '--numNodes=' + str(n), \n",
    "                               '--alpha=' + str(a), '--iterations=' + str(iterations)])\n",
    "\n",
    "    elif runnerType == 'hadoop':\n",
    "        !hdfs dfs -rm -r {outputDir}\n",
    "        mr_job = iterate(args=[filename, '--no-strict-protocols', '-r', 'hadoop', '--hadoop-home', '/usr/',\n",
    "                               '--output-dir', outputDir, '--numNodes=' + str(n),\n",
    "                               '--alpha=' + str(a), '--iterations=' + str(iterations)])\n",
    "\n",
    "    elif runnerType == 'emr':\n",
    "        !aws s3 rm --quiet {outputDir}\n",
    "        mr_job = iterate(args=[filename, '--no-strict-protocols', '--no-output', '--numNodes=' + str(n),\n",
    "                               '-r', 'emr', '--emr-job-flow-id', clusterId, '--output-dir', outputDir,\n",
    "                               '--alpha=' + str(a), '--iterations=' + str(iterations)])\n",
    "\n",
    "\n",
    "    with mr_job.make_runner() as runner:\n",
    "        runner.run()\n",
    "\n",
    "        if runnerType != 'emr':\n",
    "            for line in runner.stream_output():\n",
    "                out = mr_job.parse_output_line(line)\n",
    "                output.append(out)\n",
    "                if printOutput:\n",
    "                    print out\n",
    "                    \n",
    "def topNPR(filename, n, runnerType, outputDir, printOutput):\n",
    "\n",
    "    output = []\n",
    "    \n",
    "    if runnerType == 'local':\n",
    "        mr_job = topN(args=[filename, '--no-strict-protocols', '--top=' + str(n)])\n",
    "\n",
    "    elif runnerType == 'hadoop':\n",
    "        !hdfs dfs -rm -r {outputDir}\n",
    "        mr_job = topN(args=[filename, '--no-strict-protocols', '-r', 'hadoop', '--hadoop-home', '/usr/',\n",
    "                            '--output-dir', outputDir, '--top=' + str(n)])\n",
    "\n",
    "    elif runnerType == 'emr':\n",
    "        !aws s3 rm --quiet {outputDir}\n",
    "        mr_job = topN(args=[filename, '--no-strict-protocols', '--no-output', '--top=' + str(n),\n",
    "                            '-r', 'emr', '--emr-job-flow-id', clusterId, '--output-dir', outputDir])\n",
    "\n",
    "\n",
    "    with mr_job.make_runner() as runner:\n",
    "        runner.run()\n",
    "\n",
    "        if runnerType != 'emr':\n",
    "            for line in runner.stream_output():\n",
    "                out = mr_job.parse_output_line(line)\n",
    "                output.append(out)\n",
    "                if printOutput:\n",
    "                    print out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes = 100\n",
      "Deleted /user/miki/week09/randNet/initialize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.compat:Detected hadoop configuration property names that do not match hadoop version 2.6.0:\n",
      "The have been translated as follows\n",
      " mapred.reduce.tasks: mapreduce.job.reduces\n"
     ]
    }
   ],
   "source": [
    "inputFile = 'randNet.txt'\n",
    "outputDir = '/user/miki/week09/randNet/initialize'\n",
    "n = countNodes(inputFile)\n",
    "\n",
    "initializePR(inputFile, 'hadoop', outputDir, False)\n",
    "\n",
    "# Note: we have to do this to preserve JSON protocol for reading in on next job\n",
    "localFilename = 'randNet_initialized.txt'\n",
    "!rm {localFilename}\n",
    "!hdfs dfs -copyToLocal {outputDir + '/part-00000'} {localFilename}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/miki/week09/randNet/result': No such file or directory\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.compat:Detected hadoop configuration property names that do not match hadoop version 2.6.0:\n",
      "The have been translated as follows\n",
      " mapred.reduce.tasks: mapreduce.job.reduces\n"
     ]
    }
   ],
   "source": [
    "localFilename = 'randNet_initialized.txt'\n",
    "outputDir = '/user/miki/week09/randNet/result'\n",
    "\n",
    "k = 1\n",
    "\n",
    "iteratePR(localFilename, n, 0.85, 'hadoop', outputDir, k, False)\n",
    "\n",
    "# Note: we have to do this to preserve JSON protocol for reading in on next job\n",
    "localFilename = 'randNet_result.txt'\n",
    "!rm {localFilename}\n",
    "!hdfs dfs -copyToLocal {outputDir + '/part-00000'} {localFilename}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/miki/week09/randNet/top': No such file or directory\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.compat:Detected hadoop configuration property names that do not match hadoop version 2.6.0:\n",
      "The have been translated as follows\n",
      " mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "mapred.reduce.tasks: mapreduce.job.reduces\n",
      "mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.016290768953268954, '100')\n",
      "(0.016000901875901877, '15')\n",
      "(0.015994462481962485, '63')\n",
      "(0.015369935619935621, '9')\n",
      "(0.014743380230880232, '74')\n",
      "(0.014546825396825397, '58')\n",
      "(0.01449887196137196, '85')\n",
      "(0.014303755966255967, '61')\n",
      "(0.014077763902763904, '71')\n",
      "(0.013942843267843267, '88')\n"
     ]
    }
   ],
   "source": [
    "outputDir = '/user/miki/week09/randNet/top_all'\n",
    "\n",
    "top = 10\n",
    "\n",
    "topNPR(localFilename, top, 'hadoop', outputDir, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 9.4: Topic-specific PageRank implementation using MRJob\n",
    "\n",
    "Modify your PageRank implementation to produce a topic specific PageRank implementation\n",
    "\n",
    "## Initialization for a topic\n",
    "\n",
    "Since this corpus is small, we will read it in locally and create a topicCount file, so we know how many nodes belong to each topic. This file, along with the topics file will be fed into the initialization MRJob.\n",
    "\n",
    "In the final reducer of the MRJob (where we normalized the initial PageRank values to be 1/n), we also initialize the weights as follows:\n",
    "\n",
    "$$\n",
    "\\text{weight}_{ij} = \n",
    "\\begin{cases}\n",
    "\\frac{\\beta}{|T_j|}\n",
    "&\\mbox{if } i \\mbox{ in topic } T_j \\\\\n",
    "\\frac{1-\\beta}{N-|T_j|}\n",
    "&\\mbox{if } i \\mbox{ not in topic } T_j \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "In this case, we set $\\beta = 0.99$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "#We need to do some aggregation of topics\n",
    "topics = Counter()\n",
    "\n",
    "with open('randNet_topics.txt', 'r') as myfile:\n",
    "    for line in myfile:\n",
    "        fields = line.strip().split('\\t')\n",
    "        topic = fields[1]\n",
    "        topics[topic] += 1\n",
    "\n",
    "with open('randNet_topicCount.txt', 'w') as outfile:\n",
    "    outfile.writelines([str(item) + '\\t' + str(topics[item]) + '\\n' for item in topics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting TSPageRank_Initialize.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile TSPageRank_Initialize.py\n",
    "from __future__ import division\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class initialize(MRJob):\n",
    "        \n",
    "    #------------------\n",
    "    # Configurations: \n",
    "    \n",
    "    def configure_options(self):\n",
    "        super(initialize, self).configure_options()\n",
    "        self.add_passthrough_option('--beta', default=0.99, type='float')\n",
    "        self.add_passthrough_option('--topic', default='1', type='string')\n",
    "    \n",
    "    \"\"\"\n",
    "    Get all nodes\n",
    "    \"\"\"\n",
    "    \n",
    "    #------------------\n",
    "    # Mapper:\n",
    "    # - We need to make sure we emit a line for each node in the graph\n",
    "    # - Right now there are no lines for nodes with no neighbors\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        # Split fields\n",
    "        \n",
    "        fields = line.split('\\t')\n",
    "        key = fields[0]\n",
    "        stripe = eval(fields[1])\n",
    "        \n",
    "        # Emit the key and stripe\n",
    "        \n",
    "        yield key, stripe\n",
    "        \n",
    "        # For each neighbor, emit a 0\n",
    "        # We just do this so we catch all nodes\n",
    "        \n",
    "        for neighbor in stripe:\n",
    "            yield neighbor, 0\n",
    "            \n",
    "    #------------------\n",
    "    # Reducer:\n",
    "    # - We need to deduplicate each of our nodes\n",
    "    # - If we encounter a value that is a dictionary, these are the neighbors\n",
    "    # - If we do not encounter any dictionaries, then the node is dangling, we emit an empty neighbor list\n",
    "    \n",
    "    def reducer(self, key, values):       \n",
    "        stripe = {}\n",
    "        \n",
    "        # Loop through values for a key to see if it has neighbors\n",
    "        # If it does, we need to keep the neighbors\n",
    "        \n",
    "        for val in values:\n",
    "            if type(val) == type(stripe):\n",
    "                stripe = val\n",
    "                \n",
    "        # For each key, emit only one thing, which is the neighbor list\n",
    "        # We should now have a line for each node, even if the neighbor list is empty\n",
    "        \n",
    "        yield key, stripe\n",
    "        \n",
    "    \"\"\"\n",
    "    Initialize topic weights (v_ji)\n",
    "    Normalize length\n",
    "    \"\"\"\n",
    "    \n",
    "    #------------------\n",
    "    # Mapper:\n",
    "    # - Find total number of nodes\n",
    "    \n",
    "    # Initialize total to 0\n",
    "    def mapper_norm_init(self):\n",
    "        self.total = 0.0\n",
    "    \n",
    "    # For each key we encounter, increment total\n",
    "    # We know that we will only encounter each node once\n",
    "    def mapper_norm(self, key, value):\n",
    "        yield key, value\n",
    "        self.total += 1\n",
    "        \n",
    "    # Emit the total number of nodes we saw\n",
    "    def mapper_norm_final(self):\n",
    "        yield '*', self.total\n",
    "    \n",
    "    #------------------\n",
    "    # Combiner:\n",
    "    # - Partial sum of total nodes\n",
    "    \n",
    "    # To combine the totals if we have multiple mappers\n",
    "    def combiner_norm(self, key, values):\n",
    "        if key == '*':\n",
    "            yield key, sum(values)\n",
    "        else:\n",
    "            for val in values:\n",
    "                yield key, val\n",
    "     \n",
    "    #------------------\n",
    "    # Reducer:\n",
    "    # - Partial sum of total nodes\n",
    "    # - Calculate weight vector (v_ji) for each node\n",
    "    \n",
    "    # Initialize the totalNodes to 0\n",
    "    # Read topics and topicCount (only for topic of interest) into memory\n",
    "    def reducer_norm_init(self):\n",
    "        self.totalNodes = 0\n",
    "        self.topics = {}\n",
    "        self.topicCount = 0.0\n",
    "        \n",
    "        with open('randNet_topics.txt', 'r') as f1:\n",
    "            for line in f1:\n",
    "                fields = line.strip().split('\\t')\n",
    "                node = fields[0]\n",
    "                topic = fields[1]\n",
    "                self.topics[node] = topic\n",
    "        \n",
    "        with open('randNet_topicCount.txt', 'r') as f2:\n",
    "            for line in f2:\n",
    "                fields = line.strip().split('\\t')\n",
    "                topic = fields[0]\n",
    "                count = eval(fields[1])\n",
    "                if topic == self.options.topic:\n",
    "                    self.topicCount = count\n",
    "       \n",
    "    # If the key is '*', save the sum of the values\n",
    "    # Otherwise, yield the key, stripe, PageRank (1/n) and weight\n",
    "    def reducer_norm(self, key, values):\n",
    "        if key == '*':\n",
    "            self.totalNodes = sum(values)\n",
    "        else:\n",
    "            \n",
    "            # Is this key is in the topic of interest?\n",
    "            keyInTopic = self.topics[key] == self.options.topic\n",
    "            \n",
    "            # If the key is part of our topic, weight = beta / size of topic\n",
    "            if keyInTopic:\n",
    "                weight = self.options.beta / self.topicCount\n",
    "            \n",
    "            # Otherwise, weight = (1 - beta) / size of not-topic\n",
    "            else:\n",
    "                weight = (1 - self.options.beta) / (self.totalNodes - self.topicCount)\n",
    "                \n",
    "            for val in values:\n",
    "                yield key, (val, 1 / self.totalNodes, weight)\n",
    "    \n",
    "    \"\"\"\n",
    "    Multi-step pipeline\n",
    "    \"\"\"\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper,\n",
    "                   reducer=self.reducer),\n",
    "            MRStep(mapper_init=self.mapper_norm_init,\n",
    "                   mapper=self.mapper_norm,\n",
    "                   mapper_final=self.mapper_norm_final,\n",
    "                   combiner=self.combiner_norm,\n",
    "                   reducer_init=self.reducer_norm_init,\n",
    "                   reducer=self.reducer_norm,\n",
    "                   jobconf={'mapreduce.job.reduces': 1})\n",
    "        ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    initialize.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from TSPageRank_Initialize import initialize\n",
    "\n",
    "def initializeTSPR(filename, topic, beta, runnerType, outputDir, printOutput):\n",
    "    \n",
    "    if runnerType == 'local':\n",
    "        mr_job = initialize(args=[filename, '--no-strict-protocols', '--topic', topic, '--beta=' + str(beta),\n",
    "                                 '--file', 'randNet_topics.txt', '--file', 'randNet_topicCount.txt'])\n",
    "        \n",
    "    elif runnerType == 'hadoop':\n",
    "        !hdfs dfs -rm -r {outputDir}\n",
    "        mr_job = initialize(args=[filename, '--no-strict-protocols', '-r', 'hadoop', '--hadoop-home', '/usr/',\n",
    "                                 '--output-dir', outputDir, '--topic', topic, '--beta=' + str(beta),\n",
    "                                 '--file', 'randNet_topics.txt', '--file', 'randNet_topicCount.txt'])\n",
    "        \n",
    "    elif runnerType == 'emr':\n",
    "        !aws s3 rm --quiet {outputDir}\n",
    "        mr_job = initialize(args=[filename, '--no-strict-protocols', '--no-output', \n",
    "                                  '-r', 'emr', '--emr-job-flow-id', clusterId, '--output-dir', outputDir,\n",
    "                                  '--topic', topic, '--beta=' + str(beta),\n",
    "                                  '--file', 'randNet_topics.txt', '--file', 'randNet_topicCount.txt'])\n",
    "        \n",
    "\n",
    "    with mr_job.make_runner() as runner:\n",
    "        runner.run()\n",
    "\n",
    "        if printOutput:\n",
    "            for line in runner.stream_output():\n",
    "                print mr_job.parse_output_line(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check initialization output for topic 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes = 100\n",
      "Deleted /user/miki/week09/randNet/initialize1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.compat:Detected hadoop configuration property names that do not match hadoop version 2.6.0:\n",
      "The have been translated as follows\n",
      " mapred.reduce.tasks: mapreduce.job.reduces\n"
     ]
    }
   ],
   "source": [
    "inputFile = 'randNet.txt'\n",
    "outputDir = '/user/miki/week09/randNet/initialize'\n",
    "n = countNodes(inputFile)\n",
    "topic = '1'\n",
    "\n",
    "initializeTSPR(inputFile, topic, 0.99, 'hadoop', outputDir + topic, False)\n",
    "\n",
    "# Note: we have to do this to preserve JSON protocol for reading in on next job\n",
    "# For some reason, reading input from HDFS does not work :(\n",
    "localFilename = 'randNet/initialize' + topic + '.txt'\n",
    "!rm {localFilename}\n",
    "!hdfs dfs -copyToLocal {outputDir + topic + '/part-00000'} {localFilename}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration for a topic\n",
    "\n",
    "This should be the same logic as the non-topic-specific version, except when we calculate the PageRank (accounting for teleporting and distributing dangling mass), we use a different weighting. Formerly, we used the following formula:\n",
    "\n",
    "$$\n",
    "\\text{PR}_{\\text{new}} = (1 - \\alpha) \\bigg(\\frac{1}{n}\\bigg) + \\alpha \\bigg(\\frac{m}{n} + \\text{PR}_{\\text{old}}\\bigg)\n",
    "$$\n",
    "\n",
    "The first $1/n$ term indicated that we could jump to any other node with uniform probability. However, we now have a different probability, which we calculated in the initialization step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting TSPageRank_Iterate.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile TSPageRank_Iterate.py\n",
    "from __future__ import division\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.protocol import JSONProtocol\n",
    "\n",
    "class iterate(MRJob):\n",
    "\n",
    "    #------------------\n",
    "    # Configurations: \n",
    "    \n",
    "    def configure_options(self):\n",
    "        super(iterate, self).configure_options()\n",
    "        self.add_passthrough_option('--numNodes', default=1, type='int')\n",
    "        self.add_passthrough_option('--alpha', default=0.85, type='float')\n",
    "        self.add_passthrough_option('--iterations', default=1, type='int')\n",
    "    \n",
    "    INPUT_PROTOCOL = JSONProtocol\n",
    "    \n",
    "    #------------------\n",
    "    # Mapper:\n",
    "    # - Find the number of neighbors for the node\n",
    "    # - Distribute current PageRank among all neighbors\n",
    "    # - If there are no neighbors, keep track of dangling mass\n",
    "    \n",
    "    def mapper_dist(self, key, value):\n",
    "\n",
    "        # Divide the current PageRank by the number of neighbors\n",
    "        \n",
    "        numNeighbors = len(value[0])\n",
    "        PageRank = value[1]\n",
    "        \n",
    "        # If there are neighbors, distribute the PageRank to each neighbors\n",
    "        \n",
    "        if numNeighbors > 0:\n",
    "            for neighbor in value[0]:\n",
    "                yield neighbor, PageRank / numNeighbors\n",
    "                \n",
    "        # If there are no neighbors, we need to account for this dangling node\n",
    "        \n",
    "        else:\n",
    "            yield '*dangling', PageRank\n",
    "        \n",
    "        # Maintain the graph structure and weights\n",
    "        \n",
    "        yield key, (value[0], value[2])\n",
    "     \n",
    "    #------------------\n",
    "    # Reducer:\n",
    "    # - For each node, accumulate PageRank distributed from other nodes\n",
    "    # - Maintain graph structure\n",
    "    \n",
    "    def reducer_dist(self, key, values):\n",
    "        \n",
    "        new_PageRank = 0.0\n",
    "        weight = 0.0\n",
    "        neighbors = {}\n",
    "        \n",
    "        for val in values:\n",
    "            if type(val) == type(0.0):\n",
    "                new_PageRank += val\n",
    "            else:\n",
    "                neighbors = val[0]\n",
    "                weight = val[1]\n",
    "                \n",
    "        \n",
    "        yield key, (neighbors, new_PageRank, weight)\n",
    "\n",
    "    #------------------\n",
    "    # Mapper: \n",
    "    # - Account for teleportation\n",
    "    # - Distribute dangling mass to all nodes\n",
    "    \n",
    "    # Below is doing it with only one reducer\n",
    "    # This isn't a good way to do it, but couldn't figure out a better way\n",
    "    \n",
    "    def mapper_dangle(self, key, value):\n",
    "        yield key, value\n",
    "        \n",
    "    def reducer_init(self):\n",
    "        self.m = 0.0\n",
    "        \n",
    "    def reducer_dangle(self, key, values):\n",
    "        \n",
    "        PageRank = 0.0\n",
    "        neighbors = {}\n",
    "        weight = 0.0\n",
    "        \n",
    "        for val in values:\n",
    "            PageRank = val[1]\n",
    "            neighbors = val[0]\n",
    "            weight = val[2]\n",
    "            \n",
    "        if key == '*dangling':\n",
    "            self.m = PageRank\n",
    "        else:\n",
    "            a = self.options.alpha\n",
    "            n = self.options.numNodes\n",
    "            new_PageRank = (1 - a) * weight + a * (self.m / n + PageRank)\n",
    "            yield key, (neighbors, new_PageRank, weight)\n",
    "            \n",
    "    #------------------\n",
    "    # Pipeline:\n",
    "    \n",
    "    def steps(self):\n",
    "        return ([\n",
    "            MRStep(mapper=self.mapper_dist,\n",
    "                   reducer=self.reducer_dist),\n",
    "            MRStep(mapper=self.mapper_dangle,\n",
    "                   reducer_init=self.reducer_init,\n",
    "                   reducer=self.reducer_dangle,\n",
    "                   jobconf={'mapreduce.job.reduces': 1})\n",
    "            ] * self.options.iterations)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    iterate.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from TSPageRank_Iterate import iterate\n",
    "\n",
    "def iterateTSPR(filename, n, a, runnerType, outputDir, iterations, printOutput):\n",
    "\n",
    "    output = []\n",
    "\n",
    "    if runnerType == 'local':\n",
    "        mr_job = iterate(args=[filename, '--no-strict-protocols', '--numNodes=' + str(n),\n",
    "                               '--alpha=' + str(a), '--iterations=' + str(iterations)])\n",
    "\n",
    "    elif runnerType == 'hadoop':\n",
    "        !hdfs dfs -rm -r {outputDir}\n",
    "        mr_job = iterate(args=[filename, '--no-strict-protocols', '-r', 'hadoop', '--hadoop-home', '/usr/',\n",
    "                               '--output-dir', outputDir, '--numNodes=' + str(n),\n",
    "                               '--alpha=' + str(a), '--iterations=' + str(iterations)])\n",
    "\n",
    "    elif runnerType == 'emr':\n",
    "        !aws s3 rm --quiet {outputDir}\n",
    "        mr_job = iterate(args=[filename, '--no-strict-protocols', '--no-output', '--numNodes=' + str(n),\n",
    "                               '-r', 'emr', '--emr-job-flow-id', clusterId, '--output-dir', outputDir,\n",
    "                               '--alpha=' + str(a), '--iterations=' + str(iterations)])\n",
    "\n",
    "\n",
    "    with mr_job.make_runner() as runner:\n",
    "        runner.run()\n",
    "\n",
    "        if runnerType != 'emr':\n",
    "            for line in runner.stream_output():\n",
    "                out = mr_job.parse_output_line(line)\n",
    "                output.append(out)\n",
    "                if printOutput:\n",
    "                    print out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove `randNet/result1.txt': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "inputFile = 'randNet/initialize1.txt'\n",
    "outputDir = '/user/miki/week09/randNet/result1'\n",
    "\n",
    "topic = '1'\n",
    "k = 10\n",
    "\n",
    "iterateTSPR(inputFile, n, 0.85, 'hadoop', outputDir, k, False)\n",
    "\n",
    "# Note: we have to do this to preserve JSON protocol for reading in on next job\n",
    "# For some reason, reading input from HDFS does not work :(\n",
    "localFilename = 'randNet/result' + topic + '.txt'\n",
    "!rm {localFilename}\n",
    "!hdfs dfs -copyToLocal {outputDir + '/part-00000'} {localFilename}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/miki/week09/randNet/top1': No such file or directory\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.compat:Detected hadoop configuration property names that do not match hadoop version 2.6.0:\n",
      "The have been translated as follows\n",
      " mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "mapred.reduce.tasks: mapreduce.job.reduces\n",
      "mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.02064589832522501, '32')\n",
      "(0.020547569626787363, '77')\n",
      "(0.01975431310073244, '52')\n",
      "(0.019529238246263958, '92')\n",
      "(0.018565525448276277, '10')\n",
      "(0.01852253982706892, '27')\n",
      "(0.01784051057183225, '85')\n",
      "(0.01769238950839094, '98')\n",
      "(0.01751412867497823, '46')\n",
      "(0.01602812131786077, '74')\n"
     ]
    }
   ],
   "source": [
    "outputDir = '/user/miki/week09/randNet/top'\n",
    "\n",
    "top = 10\n",
    "\n",
    "topNPR(localFilename, top, 'hadoop', outputDir + topic, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Topic-specific PageRank for topics 1-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes = 100\n",
      "rm: `/user/miki/week09/randNet/initialize1': No such file or directory\n",
      "Deleted /user/miki/week09/randNet/result1\n",
      "Deleted /user/miki/week09/randNet/top1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.compat:Detected hadoop configuration property names that do not match hadoop version 2.6.0:\n",
      "The have been translated as follows\n",
      " mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "mapred.reduce.tasks: mapreduce.job.reduces\n",
      "mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes = 100\n",
      "rm: `/user/miki/week09/randNet/initialize2': No such file or directory\n",
      "rm: cannot remove `randNet/initialize2.txt': No such file or directory\n",
      "rm: `/user/miki/week09/randNet/result2': No such file or directory\n",
      "rm: cannot remove `randNet/result2.txt': No such file or directory\n",
      "rm: `/user/miki/week09/randNet/top2': No such file or directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.compat:Detected hadoop configuration property names that do not match hadoop version 2.6.0:\n",
      "The have been translated as follows\n",
      " mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "mapred.reduce.tasks: mapreduce.job.reduces\n",
      "mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove `randNet/top2.txt': No such file or directory\n",
      "Number of nodes = 100\n",
      "rm: `/user/miki/week09/randNet/initialize3': No such file or directory\n",
      "rm: cannot remove `randNet/initialize3.txt': No such file or directory\n",
      "rm: `/user/miki/week09/randNet/result3': No such file or directory\n",
      "rm: cannot remove `randNet/result3.txt': No such file or directory\n",
      "rm: `/user/miki/week09/randNet/top3': No such file or directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.compat:Detected hadoop configuration property names that do not match hadoop version 2.6.0:\n",
      "The have been translated as follows\n",
      " mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "mapred.reduce.tasks: mapreduce.job.reduces\n",
      "mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove `randNet/top3.txt': No such file or directory\n",
      "Number of nodes = 100\n",
      "rm: `/user/miki/week09/randNet/initialize4': No such file or directory\n",
      "rm: cannot remove `randNet/initialize4.txt': No such file or directory\n",
      "rm: `/user/miki/week09/randNet/result4': No such file or directory\n",
      "rm: cannot remove `randNet/result4.txt': No such file or directory\n",
      "rm: `/user/miki/week09/randNet/top4': No such file or directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.compat:Detected hadoop configuration property names that do not match hadoop version 2.6.0:\n",
      "The have been translated as follows\n",
      " mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "mapred.reduce.tasks: mapreduce.job.reduces\n",
      "mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove `randNet/top4.txt': No such file or directory\n",
      "Number of nodes = 100\n",
      "rm: `/user/miki/week09/randNet/initialize5': No such file or directory\n",
      "rm: cannot remove `randNet/initialize5.txt': No such file or directory\n",
      "rm: `/user/miki/week09/randNet/result5': No such file or directory\n",
      "rm: cannot remove `randNet/result5.txt': No such file or directory\n",
      "rm: `/user/miki/week09/randNet/top5': No such file or directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.compat:Detected hadoop configuration property names that do not match hadoop version 2.6.0:\n",
      "The have been translated as follows\n",
      " mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "mapred.reduce.tasks: mapreduce.job.reduces\n",
      "mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove `randNet/top5.txt': No such file or directory\n",
      "Number of nodes = 100\n",
      "rm: `/user/miki/week09/randNet/initialize6': No such file or directory\n",
      "rm: cannot remove `randNet/initialize6.txt': No such file or directory\n",
      "rm: `/user/miki/week09/randNet/result6': No such file or directory\n",
      "rm: cannot remove `randNet/result6.txt': No such file or directory\n",
      "rm: `/user/miki/week09/randNet/top6': No such file or directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.compat:Detected hadoop configuration property names that do not match hadoop version 2.6.0:\n",
      "The have been translated as follows\n",
      " mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "mapred.reduce.tasks: mapreduce.job.reduces\n",
      "mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove `randNet/top6.txt': No such file or directory\n",
      "Number of nodes = 100\n",
      "rm: `/user/miki/week09/randNet/initialize7': No such file or directory\n",
      "rm: cannot remove `randNet/initialize7.txt': No such file or directory\n",
      "rm: `/user/miki/week09/randNet/result7': No such file or directory\n",
      "rm: cannot remove `randNet/result7.txt': No such file or directory\n",
      "rm: `/user/miki/week09/randNet/top7': No such file or directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.compat:Detected hadoop configuration property names that do not match hadoop version 2.6.0:\n",
      "The have been translated as follows\n",
      " mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "mapred.reduce.tasks: mapreduce.job.reduces\n",
      "mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove `randNet/top7.txt': No such file or directory\n",
      "Number of nodes = 100\n",
      "rm: `/user/miki/week09/randNet/initialize8': No such file or directory\n",
      "rm: cannot remove `randNet/initialize8.txt': No such file or directory\n",
      "rm: `/user/miki/week09/randNet/result8': No such file or directory\n",
      "rm: cannot remove `randNet/result8.txt': No such file or directory\n",
      "rm: `/user/miki/week09/randNet/top8': No such file or directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.compat:Detected hadoop configuration property names that do not match hadoop version 2.6.0:\n",
      "The have been translated as follows\n",
      " mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "mapred.reduce.tasks: mapreduce.job.reduces\n",
      "mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove `randNet/top8.txt': No such file or directory\n",
      "Number of nodes = 100\n",
      "rm: `/user/miki/week09/randNet/initialize9': No such file or directory\n",
      "rm: cannot remove `randNet/initialize9.txt': No such file or directory\n",
      "rm: `/user/miki/week09/randNet/result9': No such file or directory\n",
      "rm: cannot remove `randNet/result9.txt': No such file or directory\n",
      "rm: `/user/miki/week09/randNet/top9': No such file or directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.compat:Detected hadoop configuration property names that do not match hadoop version 2.6.0:\n",
      "The have been translated as follows\n",
      " mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "mapred.reduce.tasks: mapreduce.job.reduces\n",
      "mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove `randNet/top9.txt': No such file or directory\n",
      "Number of nodes = 100\n",
      "rm: `/user/miki/week09/randNet/initialize10': No such file or directory\n",
      "rm: cannot remove `randNet/initialize10.txt': No such file or directory\n",
      "rm: `/user/miki/week09/randNet/result10': No such file or directory\n",
      "rm: cannot remove `randNet/result10.txt': No such file or directory\n",
      "rm: `/user/miki/week09/randNet/top10': No such file or directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.compat:Detected hadoop configuration property names that do not match hadoop version 2.6.0:\n",
      "The have been translated as follows\n",
      " mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "mapred.reduce.tasks: mapreduce.job.reduces\n",
      "mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove `randNet/top10.txt': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "def moveFromHadoop(source, destination):\n",
    "    # Note: we have to do this to preserve JSON protocol for reading in on next job\n",
    "    # For some reason, reading input from HDFS does not work :(   \n",
    "    !rm {destination}\n",
    "    !hdfs dfs -copyToLocal {source} {destination}\n",
    "\n",
    "    \n",
    "for i in range(10):\n",
    "    topic = str(i + 1)\n",
    "    \n",
    "    # Initialize with uniform PageRank and weight vector for topic i + 1\n",
    "    inputFile = 'randNet.txt'\n",
    "    outputDir = '/user/miki/week09/randNet/initialize'\n",
    "    n = countNodes(inputFile)\n",
    "\n",
    "    initializeTSPR(inputFile, topic, 0.99, 'hadoop', outputDir + topic, False)\n",
    "\n",
    "    localFilename = 'randNet/initialize' + topic + '.txt'\n",
    "    moveFromHadoop(outputDir + topic + '/part-00000', localFilename)\n",
    "    \n",
    "    # Iterate PageRank algorithm for 10 iterations\n",
    "    outputDir = '/user/miki/week09/randNet/result'\n",
    "\n",
    "    k = 10\n",
    "\n",
    "    iterateTSPR(localFilename, n, 0.85, 'hadoop', outputDir + topic, k, False)\n",
    "\n",
    "    localFilename = 'randNet/result' + topic + '.txt'\n",
    "    moveFromHadoop(outputDir + topic + '/part-00000', localFilename)\n",
    "    \n",
    "    # Find top 10 pages\n",
    "    outputDir = '/user/miki/week09/randNet/top'\n",
    "\n",
    "    top = 10\n",
    "\n",
    "    topNPR(localFilename, top, 'hadoop', outputDir + topic, False)\n",
    "    \n",
    "    localFilename = 'randNet/top' + topic + '.txt'\n",
    "    moveFromHadoop(outputDir + topic + '/part-00000', localFilename)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
