{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASCI W261: Machine Learning at Scale\n",
    "## Assignment Week 7\n",
    "Miki Seltzer (miki.seltzer@berkeley.edu)<br>\n",
    "W261-2, Spring 2016<br>\n",
    "Submission: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We will need these so we can reload modules as we modify them\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 7.0: Shortest path graph distances (toy networks)\n",
    "\n",
    "In this part of your assignment you will develop the base of your code for the week.\n",
    "\n",
    "Write MRJob classes to find shortest path graph distances, as described in the lectures. In addition to finding the distances, your code should also output a distance-minimizing path between the source and target. Work locally for this part of the assignment, and use both of the undirected and directed toy networks.\n",
    "\n",
    "![Toy networks](toy_graphs.png)\n",
    "\n",
    "To proof you code's function, run the following jobs\n",
    "\n",
    "- shortest path in the undirected network from node 1 to node 4\n",
    "Solution: 1,5,4 \n",
    "\n",
    "- shortest path in the directed network from node 1 to node 5\n",
    "Solution: 1,2,4,5\n",
    "\n",
    "and report your output -- make sure it is correct!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize graph structure\n",
    "\n",
    "We have the graph encoded as an adjacency list, but we need to keep track of state in each iteration\n",
    "- shortest distance from start node\n",
    "- node state (unvisited, queued, visited)\n",
    "- path taken to get to node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MRJob_Initiate.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MRJob_Initiate.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import sys\n",
    "\n",
    "class initiate(MRJob):\n",
    "        \n",
    "    # Specify some custom options so we only have to write one MRJob class for each join\n",
    "    def configure_options(self):\n",
    "        super(initiate, self).configure_options()\n",
    "        self.add_passthrough_option('--startNode', default='1')\n",
    "        \n",
    "    def mapper(self, _, line):\n",
    "        fields = line.strip().split('\\t')\n",
    "        name = fields[0]\n",
    "        neighbors = eval(fields[1])\n",
    "        if name == self.options.startNode:\n",
    "            yield name, [neighbors, 0, 'Q', [name]]\n",
    "        else:\n",
    "            yield name, [neighbors, sys.maxint, 'U', []]\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    initiate.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate through graph structure\n",
    "\n",
    "In each mapper iteration:\n",
    "- Expand each node that is in queued state, then mark that node as visited\n",
    "\n",
    "In each reducer iteration:\n",
    "- If any record for a node has a visited state, emit the visited record\n",
    "- When we keep track of state, if a record has a state of queued, then the node needs to be merged\n",
    "- If the record is truly unvisited, emit the unvisited node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MRJob_ShortestPath.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MRJob_ShortestPath.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import sys\n",
    "\n",
    "class shortestPath(MRJob):\n",
    "    \n",
    "    \"\"\"\n",
    "    Mapper: Iterate over each node in graph file\n",
    "    - Expand frontier if needed\n",
    "    - Update node statuses\n",
    "    \"\"\"\n",
    "    def mapper(self, _, line):\n",
    "\n",
    "        # Split text to get our data\n",
    "        fields = line.strip().split('\\t')\n",
    "        \n",
    "        # If running locally, don't need to eval\n",
    "        #name = fields[0]\n",
    "        \n",
    "        # If using EMR, need to eval the string\n",
    "        name = eval(fields[0])\n",
    "        value = eval(fields[1])\n",
    "        neighbors = value[0]\n",
    "        distance = int(value[1])\n",
    "        status = value[2]\n",
    "        path = value[3]\n",
    "        \n",
    "        # If this node is queued, expand the frontier\n",
    "        #  - mark current node as visited\n",
    "        #  - yield neighbor nodes into queue\n",
    "        if status == 'Q':\n",
    "            yield name, [neighbors, distance, 'V', path]\n",
    "            if neighbors:\n",
    "                for node in neighbors:\n",
    "                    temp_path = list(path)\n",
    "                    temp_path.append(node)\n",
    "                    yield node, [None, distance + 1, 'Q', temp_path]\n",
    "        else:\n",
    "            yield name, [neighbors, distance, status, path]\n",
    "        \n",
    "        \n",
    "    \"\"\"\n",
    "    Reducer: Aggregate expanded nodes\n",
    "    \"\"\"\n",
    "    def reducer(self, key, values):\n",
    "        neighbors = {}\n",
    "        distance = sys.maxint\n",
    "        status = None\n",
    "        path = []\n",
    "        \n",
    "        for val in values:\n",
    "            \n",
    "            # We've hit a visited node. Break out of the loop.\n",
    "            if val[2] == 'V':\n",
    "                neighbors = val[0]\n",
    "                distance = val[1]\n",
    "                status = val[2]\n",
    "                path = val[3]\n",
    "                break\n",
    "            \n",
    "            # We've hit an unvisited node. Collect the neighbors and the status\n",
    "            # If status is already Q, do not overwrite\n",
    "            elif val[0]: \n",
    "                neighbors = val[0]\n",
    "                if status != 'Q':\n",
    "                    status = val[2]\n",
    "            \n",
    "            # We've hit a queued node. Update status and path\n",
    "            else:\n",
    "                status = val[2]\n",
    "                path = val[3]\n",
    "                \n",
    "            # Update minimum distance if necessary\n",
    "            distance = min(distance, val[1])\n",
    "            \n",
    "        yield key, [neighbors, distance, status, path]\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    shortestPath.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shortest path in undirected_toy.txt\n",
      "['1', '5', '4']\n",
      "\n",
      "Shortest path in directed_toy.txt\n",
      "['1', '2', '4', '5']\n"
     ]
    }
   ],
   "source": [
    "from MRJob_Initiate import initiate\n",
    "from MRJob_ShortestPath import shortestPath\n",
    "\n",
    "def findShortestPath(filename, startNode, endNode):\n",
    "\n",
    "    filenameState = filename.replace('.', '_state.')\n",
    "    \n",
    "    # Initiate graph adjacency list to track state\n",
    "    mr_job_init = initiate(args=[filename, '--no-strict-protocols', '--startNode', startNode])\n",
    "        \n",
    "    myfile = open(filenameState, 'w')\n",
    "    \n",
    "    with mr_job_init.make_runner() as runner:\n",
    "        runner.run()\n",
    "        \n",
    "        for line in runner.stream_output():\n",
    "            out = mr_job_init.parse_output_line(line)\n",
    "            myfile.write(str(out[0]) + '\\t' + str(out[1]) + '\\n')\n",
    "\n",
    "    myfile.close()\n",
    "    \n",
    "    # Iterate over the adjacency list with state until all nodes are visited    \n",
    "    filenameTemp = filenameState.replace('.', 'Temp.')\n",
    "    \n",
    "    mr_job = shortestPath(args=[filenameState, '--no-strict-protocols'])\n",
    "\n",
    "    visitedEndNode = False\n",
    "    \n",
    "    while not visitedEndNode:\n",
    "\n",
    "        myfile = open(filenameTemp, 'w')\n",
    "        \n",
    "        with mr_job.make_runner() as runner: \n",
    "            # Run MRJob\n",
    "            runner.run()\n",
    "\n",
    "            # Write stream_output to file\n",
    "            for line in runner.stream_output():\n",
    "                out = mr_job.parse_output_line(line)\n",
    "                myfile.write(str(out[0]) + '\\t' + str(out[1]) + '\\n')\n",
    "                if out[0] == endNode and out[1][2] == 'V':\n",
    "                    visitedEndNode = True\n",
    "                    path = out[1][3]\n",
    "                    return path\n",
    "                \n",
    "        myfile.close()\n",
    "        !mv {filenameTemp} {filenameState}\n",
    "\n",
    "filename = 'undirected_toy.txt'\n",
    "print 'Shortest path in', filename\n",
    "path = findShortestPath(filename, '1', '4')\n",
    "print path\n",
    "\n",
    "filename = 'directed_toy.txt'\n",
    "print '\\nShortest path in', filename\n",
    "path = findShortestPath('directed_toy.txt', '1', '5')\n",
    "print path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 7.1: Exploratory data analysis (NLTK synonyms)\n",
    "\n",
    "For the NLTK data set, find:\n",
    "- Number of nodes\n",
    "- Number of links\n",
    "- Average degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MRJob_Explore.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MRJob_Explore.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class explore(MRJob):\n",
    "        \n",
    "    # Specify some custom options so we only have to write one MRJob class for each join\n",
    "    def configure_options(self):\n",
    "        super(explore, self).configure_options()\n",
    "        self.add_passthrough_option('--exploreType', default='nodes')\n",
    "        \n",
    "    \"\"\"\n",
    "    Find number of nodes\n",
    "    \"\"\"\n",
    "    def mapper_discoverNodes(self, _, line):\n",
    "        fields = line.strip().split('\\t')\n",
    "        name = fields[0]\n",
    "        neighbors = eval(fields[1])\n",
    "        yield name, 1\n",
    "        if neighbors:\n",
    "            for node in neighbors:\n",
    "                yield node, 1\n",
    "        \n",
    "    def reducer_discoverNodes(self, key, values):\n",
    "        yield key, 1\n",
    "        \n",
    "    def mapper_countNodes(self, key, value):\n",
    "        yield None, 1\n",
    "        \n",
    "    def reducer_countNodes(self, key, values):\n",
    "        yield None, sum(values)\n",
    "    \n",
    "    \"\"\"\n",
    "    Find number of links\n",
    "    \"\"\"\n",
    "    def mapper_links(self, _, line):\n",
    "        fields = line.strip().split('\\t')\n",
    "        name = fields[0]\n",
    "        neighbors = eval(fields[1])\n",
    "        if neighbors:\n",
    "            for node in neighbors:\n",
    "                yield None, 1\n",
    "        \n",
    "    def reducer_links(self, key, values):\n",
    "        yield None, sum(values)\n",
    "    \n",
    "    \"\"\"\n",
    "    Find average in-degree and out-degree\n",
    "    \"\"\"\n",
    "    \n",
    "    # For the mapper, we need to emit number of out and in links\n",
    "    # Count number of neighbors to find num links going out\n",
    "    # Count 1 for each node in neighbor list to find num links coming in\n",
    "    # Yield in the form of nodeName, (inLinks, outLinks)\n",
    "    def mapper_degrees(self, _, line):\n",
    "        fields = line.strip().split('\\t')\n",
    "        name = fields[0]\n",
    "        neighbors = eval(fields[1])\n",
    "        \n",
    "        # Yield number of links going out\n",
    "        yield name, (0, len(neighbors))\n",
    "        \n",
    "        # Yield 1 link coming in for each node in neighbors\n",
    "        if neighbors:\n",
    "            for node in neighbors:\n",
    "                yield node, (1, 0)\n",
    "            \n",
    "    def reducer_degrees(self, key, values):\n",
    "        inSum = 0\n",
    "        outSum = 0\n",
    "        for val in values:\n",
    "            inSum += val[0]\n",
    "            outSum += val[1]\n",
    "        yield key, (inSum, outSum)\n",
    "        \n",
    "    def mapper_degreesAvg(self, key, value):\n",
    "        yield None, (value[0], value[1], 1)\n",
    "        \n",
    "    def combiner_degreesAvg(self, key, values):\n",
    "        count = 0\n",
    "        inSum = 0\n",
    "        outSum = 0\n",
    "        for val in values:\n",
    "            inSum += val[0]\n",
    "            outSum += val[1]\n",
    "            count += val[2]\n",
    "        yield None, (inSum, outSum, count)\n",
    "        \n",
    "    def reducer_degreesAvg(self, key, values):\n",
    "        count = 0.0\n",
    "        inSum = 0.0\n",
    "        outSum = 0.0\n",
    "        for val in values:\n",
    "            inSum += val[0]\n",
    "            outSum += val[1]\n",
    "            count += val[2]\n",
    "        yield None, (inSum / count, outSum / count)\n",
    "        \n",
    "    \"\"\"\n",
    "    Multi-step pipeline\n",
    "    \"\"\"\n",
    "    def steps(self):\n",
    "        if self.options.exploreType == 'nodes':\n",
    "            return [\n",
    "                MRStep(mapper=self.mapper_discoverNodes,\n",
    "                       combiner=self.reducer_discoverNodes,\n",
    "                       reducer=self.reducer_discoverNodes),\n",
    "                MRStep(mapper=self.mapper_countNodes,\n",
    "                       combiner=self.reducer_countNodes,\n",
    "                       reducer=self.reducer_countNodes)\n",
    "            ]\n",
    "        elif self.options.exploreType == 'links':\n",
    "            return [\n",
    "                MRStep(mapper=self.mapper_links,\n",
    "                       combiner=self.reducer_links,\n",
    "                       reducer=self.reducer_links)\n",
    "            ]\n",
    "        elif self.options.exploreType == 'degrees':\n",
    "            return [\n",
    "                MRStep(mapper=self.mapper_degrees,\n",
    "                       combiner=self.reducer_degrees,\n",
    "                       reducer=self.reducer_degrees),\n",
    "                MRStep(mapper=self.mapper_degreesAvg,\n",
    "                       combiner=self.combiner_degreesAvg,\n",
    "                       reducer=self.reducer_degreesAvg)\n",
    "            ]\n",
    "\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    explore.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from MRJob_Explore import explore\n",
    "\n",
    "def exploreData(filename, exploreType):\n",
    "\n",
    "    mr_job = explore(args=[filename, '--no-strict-protocols', '--exploreType', exploreType])\n",
    "    output = []\n",
    "    \n",
    "    with mr_job.make_runner() as runner:\n",
    "        runner.run()\n",
    "        \n",
    "        for line in runner.stream_output():\n",
    "            out = mr_job.parse_output_line(line)\n",
    "            if exploreType == 'nodes' or exploreType == 'links':\n",
    "                print 'Number of', exploreType, '=', '{:,d}'.format(out[1])\n",
    "            else:\n",
    "                print 'Average in-links  =', '{:,.2f}'.format(out[1][0])\n",
    "                print 'Average out-links =', '{:,.2f}'.format(out[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test exploreData on toy data sets\n",
    "\n",
    "Using these graphs, test our job above\n",
    "\n",
    "![Toy networks](toy_graphs.png)\n",
    "\n",
    "Undirected graph has:\n",
    "- 5 nodes\n",
    "- 7 edges (bidirectional, so 14 links)\n",
    "- 14 in and out links total = 14/5 average in and out links\n",
    "\n",
    "Directed graph has:\n",
    "- 6 nodes\n",
    "- 12 links\n",
    "- average of 2 incoming and outgoing links per node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undirected toy data\n",
      "Number of nodes = 5\n",
      "Number of links = 14\n",
      "Average in-links  = 2.80\n",
      "Average out-links = 2.80\n",
      "\n",
      "Directed toy data\n",
      "Number of nodes = 6\n",
      "Number of links = 12\n",
      "Average in-links  = 2.00\n",
      "Average out-links = 2.00\n"
     ]
    }
   ],
   "source": [
    "print 'Undirected toy data'\n",
    "exploreData('undirected_toy.txt', 'nodes')\n",
    "exploreData('undirected_toy.txt', 'links')\n",
    "exploreData('undirected_toy.txt', 'degrees')\n",
    "\n",
    "print '\\nDirected toy data'\n",
    "exploreData('directed_toy.txt', 'nodes')\n",
    "exploreData('directed_toy.txt', 'links')\n",
    "exploreData('directed_toy.txt', 'degrees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK data\n",
      "Number of nodes = 8,271\n",
      "Number of links = 61,134\n",
      "Average in-links  = 7.39\n",
      "Average out-links = 7.39\n"
     ]
    }
   ],
   "source": [
    "print 'NLTK data'            \n",
    "exploreData('synNet/synNet.txt', 'nodes')\n",
    "exploreData('synNet/synNet.txt', 'links')\n",
    "exploreData('synNet/synNet.txt', 'degrees')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 7.2: Shortest path graph distances (NLTK synonyms)\n",
    "\n",
    "Find distance from \"walk\" (index 7827) to \"make\" (index 536)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def printPath(indexFile, path):\n",
    "    myDict = {}\n",
    "\n",
    "    with open(indexFile, 'r') as myfile:\n",
    "        for line in myfile:\n",
    "            fields = line.strip().split('\\t')\n",
    "            if fields[1] in path:\n",
    "                myDict[fields[1]] = fields[0]\n",
    "\n",
    "    print '{:<10s}{:<50s}'.format('INDEX', 'NAME')\n",
    "    for node in path:\n",
    "        print '{:<10s}{:<50s}'.format(node, myDict[node])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDEX     NAME                                              \n",
      "7827      walk                                              \n",
      "4655      passes                                            \n",
      "631       draw                                              \n",
      "536       make                                              \n"
     ]
    }
   ],
   "source": [
    "filename = 'synNet/synNet.txt'\n",
    "path = findShortestPath(filename, '7827', '536')\n",
    "printPath('synNet/indices.txt', path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup job on EMR\n",
    "\n",
    "We will need to modify our driver to use S3 buckets instead of storing locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using configs in /etc/mrjob.conf\n",
      "using existing scratch bucket mrjob-ac40f1afcc0b86ce\n",
      "using s3://mrjob-ac40f1afcc0b86ce/tmp/ as our scratch dir on S3\n",
      "Creating persistent job flow to run several jobs in...\n",
      "creating tmp directory /tmp/no_script.cloudera.20160306.182759.293738\n",
      "writing master bootstrap script to /tmp/no_script.cloudera.20160306.182759.293738/b.py\n",
      "Copying non-input files into s3://mrjob-ac40f1afcc0b86ce/tmp/no_script.cloudera.20160306.182759.293738/files/\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Creating Elastic MapReduce job flow\n",
      "Job flow created with ID: j-1TFS8W6PX3DIQ\n",
      "j-1TFS8W6PX3DIQ\n"
     ]
    }
   ],
   "source": [
    "# Create job flow so that we don't need to keep spinning up clusters\n",
    "!python -m mrjob.tools.emr.create_job_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clusterId = 'j-1TFS8W6PX3DIQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from MRJob_Initiate import initiate\n",
    "from MRJob_ShortestPath import shortestPath\n",
    "\n",
    "def findShortestPathEMR(filename, startNode, endNode, clusterId):\n",
    "\n",
    "    # Set the name of our output folder, and make sure it does not already exist\n",
    "    outputDirState = '/'.join(['s3://ms-w261-hw07', filename.replace('.txt', '_state2').split('/')[-1]])\n",
    "#     !aws s3 rm --recursive --quiet {outputDirState + '/'}\n",
    "    \n",
    "#     # Initiate graph adjacency list to track state\n",
    "#     mr_job_init = initiate(args=[filename, '--no-strict-protocols', '--startNode', startNode,\n",
    "#                                  '-r', 'emr', '--emr-job-flow-id', clusterId,\n",
    "#                                  '--no-output', '--output-dir', outputDirState])\n",
    "    \n",
    "#     with mr_job_init.make_runner() as runner:\n",
    "#         runner.run()\n",
    "    \n",
    "    # Iterate over the adjacency list with state until all nodes are visited    \n",
    "    inputDir = outputDirState + '/'\n",
    "    outputDir = outputDirState + 'Temp1'\n",
    "\n",
    "    visitedEndNode = False\n",
    "    i = 1\n",
    "\n",
    "    while not visitedEndNode:\n",
    "        \n",
    "        mr_job = shortestPath(args=[inputDir, '--no-strict-protocols',\n",
    "                                    '-r', 'emr', '--emr-job-flow-id', clusterId,\n",
    "                                    '--output-dir', outputDir])\n",
    "        \n",
    "        with mr_job.make_runner() as runner: \n",
    "            # Run MRJob\n",
    "            runner.run()\n",
    "\n",
    "            # Write stream_output to file\n",
    "            for line in runner.stream_output():\n",
    "                out = mr_job.parse_output_line(line)\n",
    "                if out[0] == endNode and out[1][2] != 'U':\n",
    "                    visitedEndNode = True\n",
    "                    return out[1][3]\n",
    "                    \n",
    "        # Update inputDir and outputDir for next iteration\n",
    "        inputDir = outputDirState + 'Temp' + str(i) + '/'\n",
    "        outputDir = outputDirState + 'Temp' + str(i + 1)\n",
    "        \n",
    "        i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synNet/synNet.txt\n",
      "Shortest path from 7827 to 631 = ['7827', '4655', '631']\n"
     ]
    }
   ],
   "source": [
    "# Test on small data set\n",
    "filename = 'synNet/synNet.txt'\n",
    "startNode = '7827'\n",
    "endNode = '631'\n",
    "\n",
    "print filename\n",
    "path = findShortestPathEMR(filename, startNode, endNode, clusterId)\n",
    "# path = ['7827', '4655', '631']\n",
    "print 'Shortest path from', startNode, 'to', endNode, '=', path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 7.3: Exploratory data analysis (Wikipedia)\n",
    "\n",
    "Using MRJob, explore the Wikipedia network data on the AWS cloud. Reuse your code from HW 7.1 -- does is scale well? Be cautioned that Wikipedia is a directed network, where links are not symmetric. So, even though a node may be linked to, it will not appear as a primary record itself if it has no out-links. This means that you may have to ADJUST your code (depending on its design). To be sure of your code's functionality in this context, run a systems test on the directed_toy.txt network.\n",
    "\n",
    "We will need to modify our driver to use EMR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from MRJob_Explore import explore\n",
    "\n",
    "def exploreDataEMR(filename, exploreType, clusterId):\n",
    "\n",
    "    mr_job = explore(args=[filename, '--no-strict-protocols', '--exploreType', exploreType,\n",
    "                           '-r', 'emr', '--emr-job-flow-id', clusterId,])\n",
    "    output = []\n",
    "    \n",
    "    with mr_job.make_runner() as runner:\n",
    "        runner.run()\n",
    "        \n",
    "        for line in runner.stream_output():\n",
    "            out = mr_job.parse_output_line(line)\n",
    "            if exploreType == 'nodes' or exploreType == 'links':\n",
    "                print 'Number of', exploreType, '=', '{:,d}'.format(out[1])\n",
    "            else:\n",
    "                print 'Average in-links  =', '{:,.2f}'.format(out[1][0])\n",
    "                print 'Average out-links =', '{:,.2f}'.format(out[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.conf:Got unexpected keyword arguments: ssh_tunnel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes = 15,192,277\n"
     ]
    }
   ],
   "source": [
    "exploreDataEMR('s3://ucb-mids-mls-networks/wikipedia/all-pages-indexed-out.txt', 'nodes', clusterId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.conf:Got unexpected keyword arguments: ssh_tunnel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of links = 142,114,057\n"
     ]
    }
   ],
   "source": [
    "exploreDataEMR('s3://ucb-mids-mls-networks/wikipedia/all-pages-indexed-out.txt', 'links', clusterId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.conf:Got unexpected keyword arguments: ssh_tunnel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average in-links  = 54.96\n",
      "Average out-links = 54.96\n"
     ]
    }
   ],
   "source": [
    "exploreDataEMR('s3://ucb-mids-mls-networks/wikipedia/all-pages-indexed-out.txt', 'degrees', clusterId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 7.4: Shortest path graph distances (Wikipedia)\n",
    "\n",
    "Using MRJob, find shortest path graph distances in the Wikipedia network on the AWS cloud. Reuse your code from 7.2, but once again be warned of Wikipedia being a directed network. To be sure of your code's functionality in this context, run a systems test on the directed_toy.txt network.\n",
    "\n",
    "When running your code on the Wikipedia network, proof its function by running the job:\n",
    "\n",
    "- shortest path from \"Ireland\" (index=6176135) to \"University of California, Berkeley\" (index=13466359),\n",
    "\n",
    "and show your code's output.\n",
    "\n",
    "Once your code is running, find some other shortest paths and report your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.conf:Got unexpected keyword arguments: ssh_tunnel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  s3://ms-w261-hw07/all-pages-indexed-out_state/\n",
      "Output: s3://ms-w261-hw07/all-pages-indexed-out_stateTemp1\n",
      "\n",
      "\n",
      "Input: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.conf:Got unexpected keyword arguments: ssh_tunnel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " s3://ms-w261-hw07/all-pages-indexed-out_stateTemp1/\n",
      "Output: s3://ms-w261-hw07/all-pages-indexed-out_stateTemp2\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filename = 's3://ucb-mids-mls-networks/wikipedia/all-pages-indexed-out.txt'\n",
    "path = findShortestPathEMR(filename, '6176135', '13466359', clusterId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDEX     NAME                                              \n",
      "6176135   Ireland                                           \n",
      "11607791  Seamus Heaney                                     \n",
      "13466359  University of California, Berkeley                \n"
     ]
    }
   ],
   "source": [
    "printPath('wikipedia/indices.txt', path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.conf:Got unexpected keyword arguments: ssh_tunnel\n",
      "WARNING:mrjob.conf:Got unexpected keyword arguments: ssh_tunnel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDEX     NAME                                              \n",
      "8375315   Margo Seltzer                                     \n",
      "8730327   Michael Stonebraker                               \n",
      "14181325  Yale University                                   \n"
     ]
    }
   ],
   "source": [
    "filename = 's3://ucb-mids-mls-networks/wikipedia/all-pages-indexed-out.txt'\n",
    "\n",
    "# Find distance between:\n",
    "# - 'Margo Seltzer' (my aunt, prof of Computer Science)\n",
    "# - 'Yale University' (my alma mater)\n",
    "path = findShortestPathEMR(filename, '8375315', '14181325', clusterId)\n",
    "\n",
    "printPath('wikipedia/indices.txt', path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 7.5: Conceptual exercise: Largest single-source network distances\n",
    "\n",
    "#### Suppose you wanted to find the largest network distance from a single source, i.e., a node that is the furthest (but still reachable) from a single source.\n",
    "\n",
    "#### How would you implement this task?\n",
    "I would traverse the entire graph in a similar fashion to when we find the shortest path. Instead of initializing the distances from the source node to infinity, we initialize to zero. Then when a new node is discovered, we compare distances and choose the larger.\n",
    "\n",
    "#### How is this different from finding the shortest path graph distances?\n",
    "This is different because we MUST traverse the entire graph to find the longest distance. With shortest distance between a source and a target, we can stop once the target is reached. Additionally, the longest distance requires that the graph be a directed acyclic graph. If there are cycles in the graph, then there is no longest path, since we can just traverse a cycle inifinite times. We should also restrict our search to simple paths, where no node is repeated.\n",
    "\n",
    "#### Is this task more difficult to implement than the shortest path distance? As you respond, please comment on program structure, runtimes, iterations, general system requirements, etc...\n",
    "This task is more difficult to implement, and is in fact NP-hard if the graph is not a directed acyclic graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using configs in /etc/mrjob.conf\n",
      "using existing scratch bucket mrjob-ac40f1afcc0b86ce\n",
      "using s3://mrjob-ac40f1afcc0b86ce/tmp/ as our scratch dir on S3\n",
      "Terminated job flow j-1TFS8W6PX3DIQ\n"
     ]
    }
   ],
   "source": [
    "# Terminate job flow so we don't rack up AWS expenses\n",
    "!python -m mrjob.tools.emr.terminate_job_flow {clusterId}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
