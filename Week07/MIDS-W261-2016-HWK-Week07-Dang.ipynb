{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Student**: Minhchau Dang\n",
    "* **Email Address**: minhchau.dang@berkeley.edu\n",
    "* **Course**: 2016-0111 DATASCI W261: Machine Learning at Scale\n",
    "* **Section**: Spring 2016, Section 2\n",
    "* **Assignment**: Homework 7, Week 9\n",
    "* **Submission Date**: March 10, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook requires some nbextensions.\n",
    "\n",
    "* [ruler](https://github.com/ipython-contrib/IPython-notebook-extensions/tree/master/nbextensions/usability/ruler) highlights a maximum line number so that you can avoid writing code that exceeds that character count\n",
    "* [toc2](https://github.com/ipython-contrib/IPython-notebook-extensions/tree/master/nbextensions/usability/toc2) provides a button to create a floating table of contents\n",
    "* [toggle_all_line_numbers](https://github.com/ipython-contrib/IPython-notebook-extensions/tree/master/nbextensions/usability/toggle_all_line_numbers) provides a button to see line numbers for all code cells\n",
    "* [autosaveclasses](https://github.com/holatuwol/jupyter-magic/tree/master/nbextensions/autosaveclasses.js) avoids usage of `%%writefile` (cells with a class definition are saved to disk when run)\n",
    "\n",
    "If they are not yet installed, run the following cell and restart the notebook server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "IPYTHON_PROFILE_HOME=$(ipython locate)\n",
    "GITHUB_REPO=\n",
    "\n",
    "nbextdl() {\n",
    "    if [ ! -f $IPYTHON_PROFILE_HOME/nbextensions/$1/$2 ]; then\n",
    "        mkdir -p $IPYTHON_PROFILE_HOME/nbextensions/$1\n",
    "        curl --silent -L \\\n",
    "            \"https://raw.githubusercontent.com/$GITHUB_REPO/master/nbextensions/$1/$2\" \\\n",
    "            > \"$IPYTHON_PROFILE_HOME/nbextensions/$1/$2\"\n",
    "    fi\n",
    "}\n",
    "\n",
    "GITHUB_REPO=ipython-contrib/IPython-notebook-extensions\n",
    "\n",
    "nbextdl usability/ruler main.js\n",
    "nbextdl usability/ruler icon.png\n",
    "\n",
    "nbextdl usability/toc2 main.js\n",
    "nbextdl usability/toc2 main.css\n",
    "nbextdl usability/toc2 icon.png\n",
    "nbextdl usability/toc2 image.png\n",
    "\n",
    "nbextdl usability/toggle_all_line_numbers main.js\n",
    "nbextdl usability/toggle_all_line_numbers icon.png\n",
    "\n",
    "GITHUB_REPO=holatuwol/jupyter-magic\n",
    "\n",
    "nbextdl . autosaveclasses.js"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the extensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "require(['base/js/utils'], function(utils) {\n",
    "    utils.load_extensions('usability/toc2/main');\n",
    "    utils.load_extensions('usability/ruler/main');\n",
    "    utils.load_extensions('usability/toggle_all_line_numbers/main');\n",
    "    utils.load_extensions('autosaveclasses');\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the ruler to the point at which Github wraps long lines, which is slightly smaller than where the PDF conversion will truncate lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from notebook.services.config import ConfigManager\n",
    "ip = get_ipython()\n",
    "cm = ConfigManager(parent=ip)\n",
    "cm.update('notebook', {\"ruler_column\": 90})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Hadoop settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdfs_base_folder = '/user/ubuntu'\n",
    "mapper_count = 10\n",
    "reducer_count = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In this assignment you will explore networks and develop MRJob code for \n",
    "finding shortest path graph distances. To build up to large data \n",
    "you will develop your code on some very simple, toy networks.\n",
    "After this you will take your developed code forward and modify it and \n",
    "apply it to two larger datasets (performing EDA along the way)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\"\"\"\n",
    "Utility method to split the file to make sure that multiple mapper tasks (and hopefully\n",
    "multiple reducer tasks) get created for our jobs.\n",
    "\"\"\"\n",
    "def split_file(file_name):\n",
    "    !mkdir -p input/$file_name\n",
    "    !split $file_name -l 1000 input/$file_name/\n",
    "\n",
    "    !hdfs dfs -mkdir -p input\n",
    "    !hdfs dfs -copyFromLocal input/$file_name $hdfs_base_folder/input\n",
    "\n",
    "\"\"\"\n",
    "Utility method which downloads a Dropbox file from the folder for this assignment.\n",
    "\"\"\"\n",
    "def get_dropbox_file(folder_name, file_name):\n",
    "    if os.path.isfile('input/' + file_name):\n",
    "        return\n",
    "\n",
    "    !curl -Ls https://www.dropbox.com/sh/2zl2gbgtgiegw2v/$folder_name/$file_name \\\n",
    "        > $file_name\n",
    "    \n",
    "    split_file(file_name)\n",
    "\n",
    "\"\"\"\n",
    "Utility method which downloads a file from an S3 bucket for this assignment.\n",
    "\"\"\"\n",
    "def get_s3_file(folder_name, file_name):\n",
    "    if os.path.isfile('input/' + file_name):\n",
    "        return\n",
    "\n",
    "    !aws s3 --region us-west-2 cp \\\n",
    "        s3://ucb-mids-mls-networks/$folder_name/$file_name \\\n",
    "        $file_name\n",
    "\n",
    "    split_file(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undirected toy network dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In an undirected network all links are symmetric, \n",
    "i.e., for a pair of nodes 'A' and 'B,' both of the links:\n",
    "\n",
    "> ```\n",
    "A -> B and B -> A\n",
    "```\n",
    "\n",
    "> will exist. \n",
    "\n",
    "> The toy data are available in a sparse (stripes) representation:\n",
    "\n",
    "> ```\n",
    "(node) \\t (dictionary of links)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_dropbox_file('AADsfCBmhApc5Y9NZoe-zHfza', 'undirected_toy.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In the dictionary, target nodes are keys, link weights are values \n",
    "(here, all weights are 1, i.e., the network is unweighted)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directed toy network dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In a directed network all links are not necessarily symmetric, \n",
    "i.e., for a pair of nodes 'A' and 'B,' it is possible for only one of:\n",
    "\n",
    "> ```\n",
    "A -> B or B -> A\n",
    "```\n",
    "\n",
    "> to exist. \n",
    "\n",
    "> These toy data are available in a sparse (stripes) representation:\n",
    "\n",
    "> ```\n",
    "(node) \\t (dictionary of links)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_dropbox_file('AAAn6J0Fvww44HdamkIubBT7a', 'directed_toy.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In the dictionary, target nodes are keys, link weights are values \n",
    "(here, all weights are 1, i.e., the network is unweighted)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main dataset 1: NLTK synonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In the next part of this assignment you will explore a network derived from\n",
    "the NLTK synonym database used for evaluation in HW 5. At a high level, this\n",
    "network is undirected, defined so that there exists link between two nodes/words \n",
    "if the pair or words are a synonym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_dropbox_file('AAAW2EQjw8PNUBa_P57nQ6Tja/Data/synNet', 'indices.txt')\n",
    "get_dropbox_file('AACV0btWMAbPQJjhBp-oAeJRa/Data/synNet', 'synNet.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> where `synNet.txt` contains a sparse representation of the network:\n",
    "\n",
    "> ```\n",
    "(index) \\t (dictionary of links)\n",
    "```\n",
    "\n",
    "> in indexed form, and `indices.txt` contains a lookup list\n",
    "\n",
    "> ```\n",
    "(word) \\t (index)\n",
    "```\n",
    "\n",
    "> of indices and words. This network is small enough for you to explore and run\n",
    "scripts locally, but will also be good for a systems test (for later) on AWS.\n",
    "\n",
    "> In the dictionary, target nodes are keys, link weights are values \n",
    "(here, all weights are 1, i.e., the network is unweighted)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main dataset 2: English Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The dataset is built from the Sept. 2015 XML snapshot of English Wikipedia.\n",
    "For this directed network, a link between articles: \n",
    "\n",
    "> ```\n",
    "A -> B\n",
    "```\n",
    "\n",
    "> is defined by the existence of a hyperlink in A pointing to B.\n",
    "This network also exists in the indexed format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_s3_file('wikipedia', 'all-pages-indexed-out.txt')\n",
    "get_s3_file('wikipedia', 'all-pages-indexed-in.txt')\n",
    "get_s3_file('wikipedia', 'indices.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> but has an index with more detailed data:\n",
    "\n",
    "> ```\n",
    "(article name) \\t (index) \\t (in degree) \\t (out degree)\n",
    "```\n",
    "\n",
    "> In the dictionary, target nodes are keys, link weights are values .\n",
    "Here, a weight indicates the number of time a page links to another.\n",
    "However, for the sake of this assignment, treat this an unweighted network,\n",
    "and set all weights to 1 upon data input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 7.0: Shortest path graph distances (toy networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In this part of your assignment you will develop the base of your code for the week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a job for shortest path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Write MRJob classes to find shortest path graph distances, \n",
    "as described in the lectures. In addition to finding the distances, \n",
    "your code should also output a distance-minimizing path between the source and target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import ReprProtocol\n",
    "from mrjob.step import MRStep\n",
    "import sys\n",
    "\n",
    "class ShortestPathGraphDistanceJob(MRJob):\n",
    "    INPUT_PROTOCOL = ReprProtocol\n",
    "    INTERNAL_PROTOCOL = ReprProtocol\n",
    "    OUTPUT_PROTOCOL = ReprProtocol\n",
    "    \n",
    "    def configure_options(self):\n",
    "        super(ShortestPathGraphDistanceJob, self).configure_options()\n",
    "\n",
    "        self.add_passthrough_option('--mapper-count', type='int', default=1)\n",
    "        self.add_passthrough_option('--reducer-count', type='int', default=1)\n",
    "        \n",
    "        self.add_passthrough_option('--batch-size', type='int', default=1)\n",
    "        self.add_passthrough_option('--source-node', type='string')\n",
    "        \n",
    "    def mapper(self, key, value):\n",
    "        \n",
    "        # Python might read in the first column as an integer under normal instances, so\n",
    "        # we make sure that it's converted into a string.\n",
    "        \n",
    "        key = str(key)\n",
    "        \n",
    "        if isinstance(value, tuple):\n",
    "            neighbors, own_state, own_ancestors, own_distance = value\n",
    "        else:\n",
    "            neighbors = value\n",
    "            \n",
    "            # Handle the initial input. If this is the source node, then it is queued,\n",
    "            # while all other nodes will be marked as unvisited.\n",
    "\n",
    "            if key == self.options.source_node:\n",
    "                own_state = 'Q'\n",
    "                own_ancestors = [ key ]\n",
    "                own_distance = 0\n",
    "            else:\n",
    "                own_state = 'U'\n",
    "                own_ancestors = None\n",
    "                own_distance = None\n",
    "            \n",
    "        # If this is not a queued node, then we simply yield the key and neighbors as-is\n",
    "        # and continue.\n",
    "\n",
    "        if own_state != 'Q':\n",
    "            yield key, (neighbors, own_state, own_ancestors, own_distance)\n",
    "            return\n",
    "\n",
    "        # We now emit ourselves as visited, and all of our neighbors should be understood\n",
    "        # to be queued with ourselves added as an ancestor.\n",
    "\n",
    "        yield key, (neighbors, 'V', own_ancestors, own_distance)\n",
    "\n",
    "        if neighbors is None:\n",
    "            return\n",
    "    \n",
    "        with open('/tmp/debug.txt', 'a') as debug_file:\n",
    "            print >> debug_file, key, neighbors\n",
    "        \n",
    "        for neighbor, distance in neighbors.iteritems():\n",
    "            visitor_ancestors = own_ancestors + [ neighbor ]\n",
    "            visitor_distance = own_distance + distance\n",
    "            yield neighbor, (None, 'Q', visitor_ancestors, visitor_distance)\n",
    "    \n",
    "    def combiner(self, key, summaries):\n",
    "        yield key, self.merge_summaries(summaries)\n",
    "    \n",
    "    def reducer(self, key, summaries):\n",
    "        yield key, self.merge_summaries(summaries)\n",
    "\n",
    "    def steps(self):\n",
    "        \n",
    "        # It takes awhile for each iteration to bootstrap, so add the ability to batch\n",
    "        # multiple steps in a single job.\n",
    "        \n",
    "        step = MRStep(\n",
    "            mapper = self.mapper, combiner = self.combiner, reducer = self.reducer,\n",
    "            jobconf = {\n",
    "                'mapreduce.job.maps': self.options.mapper_count,\n",
    "                'mapreduce.job.reduces': self.options.reducer_count\n",
    "            })\n",
    "        \n",
    "        return [step] * self.options.batch_size\n",
    "\n",
    "    def merge_summaries(self, summaries):\n",
    "\n",
    "        # We will yield just one summary from all the summaries we have been provided, so\n",
    "        # track this merged summary.\n",
    "\n",
    "        best_neighbors = None\n",
    "        best_state = None\n",
    "        best_ancestors = None\n",
    "        best_distance = None\n",
    "        \n",
    "        for neighbors, state, ancestors, distance in summaries:\n",
    "\n",
    "            # Only one element will have the neighbors information (though we may not\n",
    "            # receive it on the combiner side), but if we see it, use it.\n",
    "\n",
    "            if neighbors is not None:\n",
    "                best_neighbors = neighbors\n",
    "            \n",
    "            # If this is the first distance we've seen, or if this is the best distance\n",
    "            # we've seen, or it ties with the best but it is the original visited node,\n",
    "            # we use this as the new best.\n",
    "            \n",
    "            if best_distance is not None:\n",
    "                if distance is None:\n",
    "                    continue\n",
    "                \n",
    "                if distance > best_distance:\n",
    "                    continue\n",
    "                \n",
    "                if distance == best_distance and state != 'V':\n",
    "                    continue\n",
    "\n",
    "            best_state = state\n",
    "            best_ancestors = ancestors\n",
    "            best_distance = distance\n",
    "        \n",
    "        return (best_neighbors, best_state, best_ancestors, best_distance)\n",
    "\n",
    "if __name__ == '__main__' and sys.argv[0].find('ipykernel') == -1:\n",
    "    ShortestPathGraphDistanceJob().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Create a driver for shortest path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "from ShortestPathGraphDistanceJob import ShortestPathGraphDistanceJob\n",
    "\n",
    "class ShortestPathGraphDistanceDriver():\n",
    "    def __init__(self, options):\n",
    "        self.options = options\n",
    "        \n",
    "    def get_iteration_folder(self, parent_folder, iteration):\n",
    "        return '%s/%04d' % (parent_folder, iteration)\n",
    "    \n",
    "    def run(self):\n",
    "        iteration = 0\n",
    "        finished = False\n",
    "\n",
    "        while not finished:\n",
    "\n",
    "            # Instantiate a job which stores the iteration output to a specific folder,\n",
    "            # allowing us to debug multiple iterations.\n",
    "\n",
    "            if iteration == 0:\n",
    "                iteration_input_folder = self.options.input_file\n",
    "            else:\n",
    "                iteration_input_folder = self.get_iteration_folder(\n",
    "                    self.options.output_folder, iteration)\n",
    "\n",
    "            # Execute the job and print a debug message indicating that we've started the\n",
    "            # requested iteration.\n",
    "            \n",
    "            now = datetime.datetime.today()\n",
    "            \n",
    "            if self.options.batch_size == 1:\n",
    "                print now, 'Running iteration', iteration + 1\n",
    "            else:\n",
    "                print now, 'Running iterations', iteration + 1, 'through', \\\n",
    "                    iteration + self.options.batch_size\n",
    "\n",
    "            iteration += self.options.batch_size\n",
    "            iteration_output_folder = self.get_iteration_folder(\n",
    "                self.options.output_folder, iteration)\n",
    "            \n",
    "            mr_job = ShortestPathGraphDistanceJob([\n",
    "                '-r', self.options.runner_type,\n",
    "                '--mapper-count=' + str(self.options.mapper_count),\n",
    "                '--reducer-count=' + str(self.options.reducer_count),\n",
    "                '--strict-protocols',\n",
    "                '--batch-size=' + str(self.options.batch_size),\n",
    "                '--source-node=' + self.options.source_node,\n",
    "                '--output-dir=' + iteration_output_folder,\n",
    "                iteration_input_folder\n",
    "            ])\n",
    "            \n",
    "            finished = self.run_once(mr_job)\n",
    "\n",
    "        print 'Completed in', iteration, 'iterations'\n",
    "            \n",
    "    def run_once(self, mr_job):        \n",
    "        with mr_job.make_runner() as runner:\n",
    "            runner.run()\n",
    "\n",
    "            # Check to see if we have finished by reading in the output from the runner.\n",
    "            # If anything has a state that is not visited, we have not finished.\n",
    "\n",
    "            has_queued = True\n",
    "            min_queued_distance = sys.float_info.max\n",
    "            \n",
    "            target_summary = None\n",
    "            \n",
    "            for line in runner.stream_output():\n",
    "                key, summary = mr_job.parse_output_line(line)\n",
    "\n",
    "                if key == self.options.target_node:\n",
    "                    target_summary = summary\n",
    "                \n",
    "                # If we've encountered a queued node, then we'll want to check to see if\n",
    "                # the minimum queued distance is higher than the path we wish to examine.\n",
    "                # If it is, then we've already found our minimum distance.\n",
    "                \n",
    "                if summary[1] == 'Q':\n",
    "                    has_queued = True\n",
    "                    min_queued_distance = min(min_queued_distance, summary[3])\n",
    "\n",
    "            # If there are queued nodes remaining and we haven't found our item yet,\n",
    "            # then whether we are done depends on whether we have queued nodes left. If\n",
    "            # there are no queued nodes left, then we're done.\n",
    "            \n",
    "            if target_summary is None or target_summary[3] is None:\n",
    "                if not has_queued:\n",
    "                    print self.options.source_node, '->', self.options.target_node, \\\n",
    "                        '(Unreachable)'\n",
    "\n",
    "                return not has_queued\n",
    "        \n",
    "            # We are done if we found our target node and its distance from the source\n",
    "            # is not greater than anything in the queue, or if there are no more queued\n",
    "            # nodes remaining.\n",
    "            \n",
    "            if target_summary[3] <= min_queued_distance or not has_queued:\n",
    "                print self.options.source_node, '->', self.options.target_node, \\\n",
    "                    '=', target_summary[3], '(' + ', '.join(target_summary[2]) + ')'\n",
    "                return True\n",
    "\n",
    "            # Otherwise, we still have items in the queue. Therefore, we can assume that\n",
    "            # the iteration must continue.\n",
    "\n",
    "            return False\n",
    "            \n",
    "if __name__ == '__main__' and sys.argv[0].find('ipykernel') == -1:\n",
    "\n",
    "    # Use an argument parser to handle the large number of variables we are passing to\n",
    "    # the driver code.\n",
    "    \n",
    "    parser = argparse.ArgumentParser(add_help = False)\n",
    "    \n",
    "    parser.add_argument('--runner-type', default='local')    \n",
    "    parser.add_argument('--mapper-count', default=1, type=int)    \n",
    "    parser.add_argument('--reducer-count', default=1, type=int)    \n",
    "\n",
    "    parser.add_argument('--input-file')\n",
    "    parser.add_argument('--output-folder')\n",
    "\n",
    "    parser.add_argument('--batch-size', default=1, type=int)\n",
    "    parser.add_argument('--source-node')\n",
    "    parser.add_argument('--target-node')\n",
    "    \n",
    "    # Parse the arguments and instantiate the driver.\n",
    "    \n",
    "    options = parser.parse_args(sys.argv[1:])\n",
    "    driver = ShortestPathGraphDistanceDriver(options)\n",
    "    driver.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the driver for shortest path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Work locally for this part of the assignment, and use \n",
    "both of the undirected and directed toy networks.\n",
    "\n",
    "> To proof you code's function, run the following jobs\n",
    "- shortest path in the undirected network from node 1 to node 4\n",
    "\n",
    "> Solution: 1,5,4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_file = 'undirected_toy.txt'\n",
    "output_folder = 'undirected_toy.shortest_path'\n",
    "\n",
    "!rm -rf $output_folder\n",
    "!hdfs dfs -rm -r -f -skipTrash $hdfs_base_folder/$output_folder\n",
    "\n",
    "%time !python ShortestPathGraphDistanceDriver.py \\\n",
    "    --runner-type inline \\\n",
    "    --input-file input/$input_file \\\n",
    "    --output-folder $output_folder \\\n",
    "    --batch-size 5 \\\n",
    "    --source-node 1 \\\n",
    "    --target-node 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Work locally for this part of the assignment, and use \n",
    "both of the undirected and directed toy networks.\n",
    "\n",
    "> To proof you code's function, run the following jobs\n",
    "- shortest path in the directed network from node 1 to node 5\n",
    "\n",
    "> Solution: 1,2,4,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_file = 'directed_toy.txt'\n",
    "output_folder = 'directed_toy.shortest_path'\n",
    "\n",
    "!rm -rf $output_folder\n",
    "!hdfs dfs -rm -r -f -skipTrash $hdfs_base_folder/$output_folder\n",
    "\n",
    "%time !python ShortestPathGraphDistanceDriver.py \\\n",
    "    --runner-type inline \\\n",
    "    --input-file input/$input_file \\\n",
    "    --output-folder $output_folder \\\n",
    "    --batch-size 1 \\\n",
    "    --source-node 1 \\\n",
    "    --target-node 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# HW 7.1: Exploratory data analysis (NLTK synonyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a job for exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> Using MRJob, explore the synonyms network data.\n",
    "Consider plotting the degree distribution (does it follow a power law?)\n",
    "Using MRJob, explore the synonyms network data.\n",
    "Determine some of the key features, like:\n",
    "* number of nodes\n",
    "* number links\n",
    "* or the average degree (i.e., the average number of links per node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run EDA with a local runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run EDA with a Hadoop runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 7.2: Shortest path graph distances (NLTK synonyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Write (reuse your code from 7.0) an MRJob class to find shortest path graph distances, \n",
    "and apply it to the NLTK synonyms network dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Run: Batch Size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/ubuntu/synNet.shortest_path\n",
      "2016-03-07 08:54:23.501812 Running iteration 1\n",
      "2016-03-07 08:56:44.302100 Running iteration 2\n",
      "2016-03-07 08:58:28.859901 Running iteration 3\n",
      "7827 -> 536 = 3 (7827, 1426, 3553, 536)\n",
      "Completed in 3 iterations\n",
      "CPU times: user 5.24 s, sys: 1.16 s, total: 6.4 s\n",
      "Wall time: 5min 43s\n"
     ]
    }
   ],
   "source": [
    "input_file = 'synNet.txt'\n",
    "output_folder = 'synNet.shortest_path'\n",
    "\n",
    "!rm -rf $output_folder\n",
    "!hdfs dfs -rm -r -f -skipTrash $hdfs_base_folder/$output_folder\n",
    "\n",
    "%time !python ShortestPathGraphDistanceDriver.py \\\n",
    "    --runner-type hadoop \\\n",
    "    --mapper-count $mapper_count \\\n",
    "    --reducer-count $reducer_count \\\n",
    "    --input-file hdfs://$hdfs_base_folder/input/$input_file \\\n",
    "    --output-folder hdfs://$hdfs_base_folder/$output_folder \\\n",
    "    --batch-size 1 \\\n",
    "    --source-node 7827 \\\n",
    "    --target-node 536"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Run: Batch Size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/ubuntu/synNet.shortest_path\n",
      "2016-03-07 09:00:10.840554 Running iterations 1 through 3\n",
      "7827 -> 536 = 3 (7827, 1426, 3553, 536)\n",
      "Completed in 3 iterations\n",
      "CPU times: user 3.28 s, sys: 823 ms, total: 4.1 s\n",
      "Wall time: 3min 43s\n"
     ]
    }
   ],
   "source": [
    "input_file = 'synNet.txt'\n",
    "output_folder = 'synNet.shortest_path'\n",
    "\n",
    "!rm -rf $output_folder\n",
    "!hdfs dfs -rm -r -f -skipTrash $hdfs_base_folder/$output_folder\n",
    "\n",
    "%time !python ShortestPathGraphDistanceDriver.py \\\n",
    "    --runner-type hadoop \\\n",
    "    --mapper-count $mapper_count \\\n",
    "    --reducer-count $reducer_count \\\n",
    "    --input-file hdfs://$hdfs_base_folder/input/$input_file \\\n",
    "    --output-folder hdfs://$hdfs_base_folder/$output_folder \\\n",
    "    --batch-size 3 \\\n",
    "    --source-node 7827 \\\n",
    "    --target-node 536"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 7.3: Exploratory data analysis (Wikipedia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Using MRJob, explore the Wikipedia network data on the AWS cloud. Reuse your code from HW 7.1---does is scale well? \n",
    "\n",
    "> Be cautioned that Wikipedia is a directed network, where links are not symmetric. \n",
    "So, even though a node may be linked to, it will not appear as a primary record itself if it has no out-links. \n",
    "This means that you may have to ADJUST your code (depending on its design)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 7.4: Shortest path graph distances (Wikipedia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Using MRJob, find shortest path graph distances in the Wikipedia network on the AWS cloud.\n",
    "Reuse your code from 7.2, but once again be warned of Wikipedia being a directed network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> When running your code on the Wikipedia network, proof its function by running the job:\n",
    "\n",
    "> - shortest path from \"Ireland\" (index=6176135) to \"University of California, Berkeley\" (index=13466359)\n",
    "\n",
    "> and show your code's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 7.5: Conceptual exercise: Largest single-source network distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Suppose you wanted to find the largest network distance from a single source,\n",
    "i.e., a node that is the furthest (but still reachable) from a single source.\n",
    "> As you respond, please comment on program structure, runtimes, iterations, general system requirements, etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 7.5a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> How would you implement this task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 7.5b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> How is this different from finding the shortest path graph distances?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 7.5c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Is this task more difficult to implement than the shortest path distance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 7.6: Computational exercise: Largest single-source network distances (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Using MRJob, write a code to find the largest graph distance and distance-maximizing nodes from a single-source.\n",
    "Test your code first on the toy networks and synonyms network to proof its function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": false,
   "toc_threshold": 4,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
