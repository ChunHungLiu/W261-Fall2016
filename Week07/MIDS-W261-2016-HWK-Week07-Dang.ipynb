{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Student**: Minhchau Dang\n",
    "* **Email Address**: minhchau.dang@berkeley.edu\n",
    "* **Course**: 2016-0111 DATASCI W261: Machine Learning at Scale\n",
    "* **Section**: Spring 2016, Section 2\n",
    "* **Assignment**: Homework 7, Week 9\n",
    "* **Submission Date**: March 10, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook requires some nbextensions.\n",
    "\n",
    "* [ruler](https://github.com/ipython-contrib/IPython-notebook-extensions/tree/master/nbextensions/usability/ruler) highlights a maximum line number so that you can avoid writing code that exceeds that character count\n",
    "* [toc2](https://github.com/ipython-contrib/IPython-notebook-extensions/tree/master/nbextensions/usability/toc2) provides a button to create a floating table of contents\n",
    "* [toggle_all_line_numbers](https://github.com/ipython-contrib/IPython-notebook-extensions/tree/master/nbextensions/usability/toggle_all_line_numbers) provides a button to see line numbers for all code cells\n",
    "* [autosaveclasses](https://github.com/holatuwol/jupyter-magic/tree/master/nbextensions/autosaveclasses.js) avoids usage of `%%writefile` (cells with a class definition are saved to disk when run)\n",
    "\n",
    "If they are not yet installed, run the following cell and restart the notebook server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "IPYTHON_PROFILE_HOME=$(ipython locate)\n",
    "GITHUB_REPO=\n",
    "\n",
    "nbextdl() {\n",
    "    if [ ! -f $IPYTHON_PROFILE_HOME/nbextensions/$1/$2 ]; then\n",
    "        mkdir -p $IPYTHON_PROFILE_HOME/nbextensions/$1\n",
    "        curl --silent -L \\\n",
    "            \"https://raw.githubusercontent.com/$GITHUB_REPO/master/nbextensions/$1/$2\" \\\n",
    "            > \"$IPYTHON_PROFILE_HOME/nbextensions/$1/$2\"\n",
    "    fi\n",
    "}\n",
    "\n",
    "GITHUB_REPO=ipython-contrib/IPython-notebook-extensions\n",
    "\n",
    "nbextdl usability/ruler main.js\n",
    "nbextdl usability/ruler icon.png\n",
    "\n",
    "nbextdl usability/toc2 main.js\n",
    "nbextdl usability/toc2 main.css\n",
    "nbextdl usability/toc2 icon.png\n",
    "nbextdl usability/toc2 image.png\n",
    "\n",
    "nbextdl usability/toggle_all_line_numbers main.js\n",
    "nbextdl usability/toggle_all_line_numbers icon.png\n",
    "\n",
    "GITHUB_REPO=holatuwol/jupyter-magic\n",
    "\n",
    "nbextdl . autosaveclasses.js"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the extensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "require(['base/js/utils'], function(utils) {\n",
    "    utils.load_extensions('usability/toc2/main');\n",
    "    utils.load_extensions('usability/ruler/main');\n",
    "    utils.load_extensions('usability/toggle_all_line_numbers/main');\n",
    "    utils.load_extensions('autosaveclasses');\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the ruler to the point at which Github wraps long lines, which is slightly smaller than where the PDF conversion will truncate lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from notebook.services.config import ConfigManager\n",
    "ip = get_ipython()\n",
    "cm = ConfigManager(parent=ip)\n",
    "cm.update('notebook', {\"ruler_column\": 90})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Hadoop settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdfs_base_folder = '/user/ubuntu'\n",
    "mapper_count = 10\n",
    "reducer_count = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In this assignment you will explore networks and develop MRJob code for \n",
    "finding shortest path graph distances. To build up to large data \n",
    "you will develop your code on some very simple, toy networks.\n",
    "After this you will take your developed code forward and modify it and \n",
    "apply it to two larger datasets (performing EDA along the way)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\"\"\"\n",
    "Utility method to split the file to make sure that multiple mapper tasks (and hopefully\n",
    "multiple reducer tasks) get created for our jobs.\n",
    "\"\"\"\n",
    "def split_file(file_name):\n",
    "    !mkdir -p input/$file_name\n",
    "    !split $file_name -l 100000 input/$file_name/\n",
    "\n",
    "    !hdfs dfs -mkdir -p input\n",
    "    !hdfs dfs -copyFromLocal input/$file_name $hdfs_base_folder/input\n",
    "\n",
    "\"\"\"\n",
    "Utility method which downloads a Dropbox file from the folder for this assignment.\n",
    "\"\"\"\n",
    "def get_dropbox_file(folder_name, file_name, add_to_hdfs = True):\n",
    "    dropbox_url = 'https://www.dropbox.com/sh/2zl2gbgtgiegw2v'\n",
    "    \n",
    "    if not os.path.isfile(file_name):\n",
    "        !curl -Ls $dropbox_url/$folder_name/$file_name > $file_name\n",
    "    \n",
    "    if add_to_hdfs and not os.path.isdir('input/' + file_name):\n",
    "        split_file(file_name)\n",
    "\n",
    "\"\"\"\n",
    "Utility method which downloads a file from an S3 bucket for this assignment.\n",
    "\"\"\"\n",
    "def get_s3_file(folder_name, file_name, add_to_hdfs = True):\n",
    "    if not os.path.isfile(file_name):\n",
    "        !aws s3 --region us-west-2 cp \\\n",
    "            s3://ucb-mids-mls-networks/$folder_name/$file_name \\\n",
    "            $file_name\n",
    "\n",
    "    if add_to_hdfs and not os.path.isdir('input/' + file_name):\n",
    "        split_file(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undirected toy network dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In an undirected network all links are symmetric, \n",
    "i.e., for a pair of nodes 'A' and 'B,' both of the links:\n",
    "\n",
    "> ```\n",
    "A -> B and B -> A\n",
    "```\n",
    "\n",
    "> will exist. \n",
    "\n",
    "> The toy data are available in a sparse (stripes) representation:\n",
    "\n",
    "> ```\n",
    "(node) \\t (dictionary of links)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_dropbox_file('AADsfCBmhApc5Y9NZoe-zHfza', 'undirected_toy.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In the dictionary, target nodes are keys, link weights are values \n",
    "(here, all weights are 1, i.e., the network is unweighted)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directed toy network dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In a directed network all links are not necessarily symmetric, \n",
    "i.e., for a pair of nodes 'A' and 'B,' it is possible for only one of:\n",
    "\n",
    "> ```\n",
    "A -> B or B -> A\n",
    "```\n",
    "\n",
    "> to exist. \n",
    "\n",
    "> These toy data are available in a sparse (stripes) representation:\n",
    "\n",
    "> ```\n",
    "(node) \\t (dictionary of links)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_dropbox_file('AAAn6J0Fvww44HdamkIubBT7a', 'directed_toy.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In the dictionary, target nodes are keys, link weights are values \n",
    "(here, all weights are 1, i.e., the network is unweighted)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main dataset 1: NLTK synonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In the next part of this assignment you will explore a network derived from\n",
    "the NLTK synonym database used for evaluation in HW 5. At a high level, this\n",
    "network is undirected, defined so that there exists link between two nodes/words \n",
    "if the pair or words are a synonym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_dropbox_file(\n",
    "    'AACV0btWMAbPQJjhBp-oAeJRa/Data/synNet', 'synNet.txt')\n",
    "get_dropbox_file(\n",
    "    'AAAW2EQjw8PNUBa_P57nQ6Tja/Data/synNet', 'indices.txt', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> where `synNet.txt` contains a sparse representation of the network:\n",
    "\n",
    "> ```\n",
    "(index) \\t (dictionary of links)\n",
    "```\n",
    "\n",
    "> in indexed form, and `indices.txt` contains a lookup list\n",
    "\n",
    "> ```\n",
    "(word) \\t (index)\n",
    "```\n",
    "\n",
    "> of indices and words. This network is small enough for you to explore and run\n",
    "scripts locally, but will also be good for a systems test (for later) on AWS.\n",
    "\n",
    "> In the dictionary, target nodes are keys, link weights are values \n",
    "(here, all weights are 1, i.e., the network is unweighted)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main dataset 2: English Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The dataset is built from the Sept. 2015 XML snapshot of English Wikipedia.\n",
    "For this directed network, a link between articles: \n",
    "\n",
    "> ```\n",
    "A -> B\n",
    "```\n",
    "\n",
    "> is defined by the existence of a hyperlink in A pointing to B.\n",
    "This network also exists in the indexed format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_s3_file('wikipedia', 'all-pages-indexed-out.txt')\n",
    "#get_s3_file('wikipedia', 'all-pages-indexed-in.txt')\n",
    "get_s3_file('wikipedia', 'indices.txt', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> but has an index with more detailed data:\n",
    "\n",
    "> ```\n",
    "(article name) \\t (index) \\t (in degree) \\t (out degree)\n",
    "```\n",
    "\n",
    "> In the dictionary, target nodes are keys, link weights are values .\n",
    "Here, a weight indicates the number of time a page links to another.\n",
    "However, for the sake of this assignment, treat this an unweighted network,\n",
    "and set all weights to 1 upon data input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 7.0: Shortest path graph distances (toy networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In this part of your assignment you will develop the base of your code for the week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a job for shortest path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Write MRJob classes to find shortest path graph distances, \n",
    "as described in the lectures. In addition to finding the distances, \n",
    "your code should also output a distance-minimizing path between the source and target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import ReprProtocol\n",
    "from mrjob.step import MRStep\n",
    "import sys\n",
    "\n",
    "class ShortestPathGraphDistanceJob(MRJob):\n",
    "    INPUT_PROTOCOL = ReprProtocol\n",
    "    INTERNAL_PROTOCOL = ReprProtocol\n",
    "    OUTPUT_PROTOCOL = ReprProtocol\n",
    "    \n",
    "    def configure_options(self):\n",
    "        super(ShortestPathGraphDistanceJob, self).configure_options()\n",
    "\n",
    "        self.add_passthrough_option('--mapper-count', type='int', default=1)        \n",
    "        self.add_passthrough_option('--reducer-count', type='int', default=1)        \n",
    "        self.add_passthrough_option('--batch-size', type='int', default=1)\n",
    "        self.add_passthrough_option('--source-node', type='string')\n",
    "        self.add_passthrough_option('--target-node', type='string')\n",
    "        self.add_passthrough_option('--uniform-weight', type='int', default=1)\n",
    "        \n",
    "    def mapper(self, key, value):\n",
    "        \n",
    "        # Python might read in the first column as an integer under normal instances, so\n",
    "        # we make sure that it's converted into a string.\n",
    "        \n",
    "        key = str(key)\n",
    "        \n",
    "        # If the key is an asterisk, that means it's a debug symbol from the previous\n",
    "        # iteration, so we can skip it.\n",
    "\n",
    "        if key == '*':\n",
    "            return\n",
    "\n",
    "        # Check for the first iteration.\n",
    "        \n",
    "        if isinstance(value, tuple):\n",
    "            neighbors, own_state, own_ancestors, own_distance = value\n",
    "        else:\n",
    "            neighbors = value\n",
    "            \n",
    "            # Handle the initial input. If this is the source node, then it is queued,\n",
    "            # while all other nodes will be marked as unvisited.\n",
    "\n",
    "            if key == self.options.source_node:\n",
    "                own_state = 'Q'\n",
    "                own_ancestors = [ key ]\n",
    "                own_distance = 0\n",
    "            else:\n",
    "                own_state = 'U'\n",
    "                own_ancestors = None\n",
    "                own_distance = None\n",
    "            \n",
    "        # If this is not a queued node, then we simply yield the key and neighbors as-is\n",
    "        # and continue.\n",
    "\n",
    "        if own_state != 'Q':\n",
    "            yield key, (neighbors, own_state, own_ancestors, own_distance)\n",
    "            return\n",
    "\n",
    "        # We now emit ourselves as visited, and all of our neighbors should be understood\n",
    "        # to be queued with ourselves added as an ancestor.\n",
    "\n",
    "        yield key, (neighbors, 'V', own_ancestors, own_distance)\n",
    "\n",
    "        if neighbors is None:\n",
    "            return\n",
    "        \n",
    "        for neighbor, distance in neighbors.iteritems():\n",
    "            visitor_ancestors = own_ancestors + [ neighbor ]\n",
    "            \n",
    "            if self.options.uniform_weight != 0:\n",
    "                distance = self.options.uniform_weight\n",
    "                \n",
    "            visitor_distance = own_distance + distance\n",
    "            yield neighbor, (None, 'Q', visitor_ancestors, visitor_distance)\n",
    "    \n",
    "    def combiner(self, key, summaries):\n",
    "        yield key, self.merge_summaries(summaries)\n",
    "    \n",
    "    def reducer_init(self):\n",
    "        self.target_distance = None\n",
    "        self.target_ancestors = None        \n",
    "        self.min_queued_distance = None\n",
    "        \n",
    "    def reducer(self, key, summaries):\n",
    "        # Yield the output for this iteration.\n",
    "        \n",
    "        summary = self.merge_summaries(summaries)\n",
    "        yield key, summary\n",
    "\n",
    "        # If this is the target node, remember its distance.\n",
    "        \n",
    "        neighbors, state, ancestors, distance = summary\n",
    "\n",
    "        if key == self.options.target_node:\n",
    "            self.target_distance = distance\n",
    "            self.target_ancestors = ancestors\n",
    "            return\n",
    "        \n",
    "        # If this node is not queued, or we've already emitted a stop symbol, there isn't\n",
    "        # any additional work to do.\n",
    "\n",
    "        if state != 'Q':\n",
    "            return\n",
    "        \n",
    "        # If we've seen the target node, and the new minimum distance is smaller than the\n",
    "        # target node distance, we can stop.\n",
    "        \n",
    "        if self.min_queued_distance is None or distance < self.min_queued_distance:\n",
    "            self.min_queued_distance = distance\n",
    "            \n",
    "    def reducer_final(self):\n",
    "        yield '*', (self.target_distance, self.target_ancestors, self.min_queued_distance)\n",
    "\n",
    "    def steps(self):\n",
    "        \n",
    "        # It takes awhile for each iteration to bootstrap, so add the ability to batch\n",
    "        # multiple steps in a single job.\n",
    "\n",
    "        step = MRStep(\n",
    "            mapper = self.mapper, combiner = self.combiner,\n",
    "            reducer_init = self.reducer_init, reducer = self.reducer,\n",
    "            reducer_final = self.reducer_final,\n",
    "            jobconf = {\n",
    "                'mapreduce.job.maps': self.options.mapper_count,\n",
    "                'mapreduce.job.reduces': self.options.reducer_count\n",
    "            })\n",
    "        \n",
    "        return [step] * self.options.batch_size\n",
    "\n",
    "    def merge_summaries(self, summaries):\n",
    "\n",
    "        # We will yield just one summary from all the summaries we have been provided, so\n",
    "        # track this merged summary.\n",
    "\n",
    "        best_neighbors = None\n",
    "        best_state = None\n",
    "        best_ancestors = None\n",
    "        best_distance = None\n",
    "        \n",
    "        for neighbors, state, ancestors, distance in summaries:\n",
    "\n",
    "            # Only one element will have the neighbors information (though we may not\n",
    "            # receive it on the combiner side), but if we see it, use it.\n",
    "\n",
    "            if neighbors is not None:\n",
    "                best_neighbors = neighbors\n",
    "            \n",
    "            # If this is the first distance we've seen, or if this is the best distance\n",
    "            # we've seen, or it ties with the best but it is the original visited node,\n",
    "            # we use this as the new best.\n",
    "            \n",
    "            if best_distance is not None:\n",
    "                if distance is None:\n",
    "                    continue\n",
    "                \n",
    "                if distance > best_distance:\n",
    "                    continue\n",
    "                \n",
    "                if distance == best_distance and state != 'V':\n",
    "                    continue\n",
    "\n",
    "            best_state = state\n",
    "            best_ancestors = ancestors\n",
    "            best_distance = distance\n",
    "        \n",
    "        return (best_neighbors, best_state, best_ancestors, best_distance)\n",
    "\n",
    "if __name__ == '__main__' and sys.argv[0].find('ipykernel') == -1:\n",
    "    ShortestPathGraphDistanceJob().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Create a driver for shortest path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "from ShortestPathGraphDistanceJob import ShortestPathGraphDistanceJob\n",
    "\n",
    "class ShortestPathGraphDistanceDriver():\n",
    "    def __init__(self, options):\n",
    "        self.options = options\n",
    "        \n",
    "    def get_iteration_folder(self, parent_folder, iteration):\n",
    "        return '%s/%04d' % (parent_folder, iteration)\n",
    "    \n",
    "    def run(self):\n",
    "        iteration = 0\n",
    "        finished = False\n",
    "\n",
    "        while not finished:\n",
    "\n",
    "            # Instantiate a job which stores the iteration output to a specific folder,\n",
    "            # allowing us to debug multiple iterations.\n",
    "\n",
    "            if iteration == 0:\n",
    "                iteration_input_folder = self.options.input_file\n",
    "            else:\n",
    "                iteration_input_folder = self.get_iteration_folder(\n",
    "                    self.options.output_folder, iteration)\n",
    "\n",
    "            # Execute the job and print a debug message indicating that we've started the\n",
    "            # requested iteration.\n",
    "            \n",
    "            now = datetime.datetime.today()\n",
    "            \n",
    "            if self.options.batch_size == 1:\n",
    "                print now, 'Running iteration', iteration + 1\n",
    "            else:\n",
    "                print now, 'Running iterations', iteration + 1, 'through', \\\n",
    "                    iteration + self.options.batch_size\n",
    "\n",
    "            iteration += self.options.batch_size\n",
    "            iteration_output_folder = self.get_iteration_folder(\n",
    "                self.options.output_folder, iteration)\n",
    "            \n",
    "            mr_job = ShortestPathGraphDistanceJob([\n",
    "                '-r', self.options.runner_type,\n",
    "                '--mapper-count=' + str(self.options.mapper_count),\n",
    "                '--reducer-count=' + str(self.options.reducer_count),\n",
    "                '--strict-protocols',\n",
    "                '--batch-size=' + str(self.options.batch_size),\n",
    "                '--source-node=' + self.options.source_node,\n",
    "                '--target-node=' + self.options.target_node,\n",
    "                '--output-dir=' + iteration_output_folder,\n",
    "                iteration_input_folder\n",
    "            ])\n",
    "            \n",
    "            finished = self.run_once(mr_job)\n",
    "\n",
    "        print 'Completed in', iteration, 'iterations'\n",
    "            \n",
    "    def run_once(self, mr_job):        \n",
    "        with mr_job.make_runner() as runner:\n",
    "            runner.run()\n",
    "\n",
    "            first_line = True\n",
    "\n",
    "            distance = None\n",
    "            ancestors = None\n",
    "            min_queued_distance = None\n",
    "            \n",
    "            for line in runner.stream_output():\n",
    "                if first_line:\n",
    "                    first_line = False\n",
    "                    now = datetime.datetime.today()\n",
    "                    print now, 'Streaming output from runner'\n",
    "                \n",
    "                key, value = line.split('\\t', 1)\n",
    "                \n",
    "                # Only attempt to parse the value if we see the '*' indicating that we\n",
    "                # read the finish flag.\n",
    "\n",
    "                if key != \"'*'\":\n",
    "                    continue\n",
    "\n",
    "                new_distance, new_ancestors, new_min_queued_distance = eval(value)\n",
    "                \n",
    "                # Only one line will contain anything related to the target node.\n",
    "                \n",
    "                if new_distance is not None:\n",
    "                    distance = new_distance\n",
    "                    ancestors = new_ancestors\n",
    "                \n",
    "                # Out of all the minimum distances we saw, choose the lowest so that we\n",
    "                # can do a global comparison.\n",
    "                \n",
    "                if min_queued_distance is None:\n",
    "                    min_queued_distance = new_min_queued_distance    \n",
    "                elif new_min_queued_distance is not None and \\\n",
    "                    new_min_queued_distance < min_queued_distance:\n",
    "\n",
    "                    min_queued_distance = new_min_queued_distance    \n",
    "\n",
    "            # If we have no items in the queue (which we know from seeing a minimum\n",
    "            # queued distance of None), then we emit no matter what.\n",
    "            \n",
    "            if min_queued_distance is None:\n",
    "                self.print_shortest_path(distance, ancestors)\n",
    "                return True\n",
    "        \n",
    "            # If we haven't visited the target node yet (the distance is None), then we\n",
    "            # continue to iterate.\n",
    "            \n",
    "            if distance is None:\n",
    "                return False\n",
    "                    \n",
    "            # If we still have items in the queue (which we know from whether the\n",
    "            # minimum queued distance is not None), then we compare it to the target node\n",
    "            # distance. If the target node has a smaller distance than the minimum, then\n",
    "            # we emit this as the shortest path.\n",
    "            \n",
    "            if distance <= min_queued_distance:\n",
    "                self.print_shortest_path(distance, ancestors)\n",
    "                return True\n",
    "\n",
    "            # Otherwise, we have not found a stop condition, and we should continue on to\n",
    "            # the next iteration.\n",
    "            \n",
    "            return False            \n",
    "\n",
    "    def print_shortest_path(self, distance, ancestors):\n",
    "        if distance is None:\n",
    "            print self.options.source_node, '->', self.options.target_node, \\\n",
    "                '(Unreachable)'\n",
    "        else:\n",
    "            print self.options.source_node, '->', self.options.target_node, \\\n",
    "                '=', distance, '(' + ', '.join(ancestors) + ')'\n",
    "            \n",
    "if __name__ == '__main__' and sys.argv[0].find('ipykernel') == -1:\n",
    "\n",
    "    # Use an argument parser to handle the large number of variables we are passing to\n",
    "    # the driver code.\n",
    "    \n",
    "    parser = argparse.ArgumentParser(add_help = False)\n",
    "    \n",
    "    parser.add_argument('--runner-type', default='local')\n",
    "    parser.add_argument('--mapper-count', default=1, type=int)    \n",
    "    parser.add_argument('--reducer-count', default=1, type=int)    \n",
    "\n",
    "    parser.add_argument('--input-file')\n",
    "    parser.add_argument('--output-folder')\n",
    "\n",
    "    parser.add_argument('--batch-size', default=1, type=int)\n",
    "    parser.add_argument('--source-node')\n",
    "    parser.add_argument('--target-node')\n",
    "    parser.add_argument('--uniform-weight', default=1, type=int)\n",
    "    \n",
    "    # Parse the arguments and instantiate the driver.\n",
    "    \n",
    "    options = parser.parse_args(sys.argv[1:])\n",
    "    driver = ShortestPathGraphDistanceDriver(options)\n",
    "    driver.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the driver for shortest path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Work locally for this part of the assignment, and use \n",
    "both of the undirected and directed toy networks.\n",
    "\n",
    "> To proof you code's function, run the following jobs\n",
    "- shortest path in the undirected network from node 1 to node 4\n",
    "\n",
    "> Solution: 1,5,4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_file = 'undirected_toy.txt'\n",
    "output_folder = 'undirected_toy.shortest_path'\n",
    "\n",
    "!rm -rf output/$output_folder\n",
    "!hdfs dfs -rm -r -f -skipTrash $hdfs_base_folder/output/$output_folder\n",
    "\n",
    "%time !python ShortestPathGraphDistanceDriver.py \\\n",
    "    --runner-type inline \\\n",
    "    --input-file input/$input_file \\\n",
    "    --output-folder output/$output_folder \\\n",
    "    --batch-size 1 \\\n",
    "    --source-node 1 \\\n",
    "    --target-node 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Work locally for this part of the assignment, and use \n",
    "both of the undirected and directed toy networks.\n",
    "\n",
    "> To proof you code's function, run the following jobs\n",
    "- shortest path in the directed network from node 1 to node 5\n",
    "\n",
    "> Solution: 1,2,4,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_file = 'directed_toy.txt'\n",
    "output_folder = 'directed_toy.shortest_path'\n",
    "\n",
    "!rm -rf output/$output_folder\n",
    "!hdfs dfs -rm -r -f -skipTrash $hdfs_base_folder/output/$output_folder\n",
    "\n",
    "%time !python ShortestPathGraphDistanceDriver.py \\\n",
    "    --runner-type inline \\\n",
    "    --input-file input/$input_file \\\n",
    "    --output-folder output/$output_folder \\\n",
    "    --batch-size 1 \\\n",
    "    --source-node 1 \\\n",
    "    --target-node 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# HW 7.1: Exploratory data analysis (NLTK synonyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a job for exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> Using MRJob, explore the synonyms network data.\n",
    "Consider plotting the degree distribution (does it follow a power law?)\n",
    "Using MRJob, explore the synonyms network data.\n",
    "Determine some of the key features, like:\n",
    "* number of nodes\n",
    "* number links\n",
    "* or the average degree (i.e., the average number of links per node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run EDA with a local runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run EDA with a Hadoop runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 7.2: Shortest path graph distances (NLTK synonyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Write (reuse your code from 7.0) an MRJob class to find shortest path graph distances, \n",
    "and apply it to the NLTK synonyms network dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_file = 'synNet.txt'\n",
    "output_folder = 'synNet.shortest_path'\n",
    "\n",
    "!rm -rf output/$output_folder\n",
    "!hdfs dfs -rm -r -f -skipTrash $hdfs_base_folder/output/$output_folder\n",
    "\n",
    "%time !python ShortestPathGraphDistanceDriver.py \\\n",
    "    --runner-type hadoop \\\n",
    "    --mapper-count $mapper_count \\\n",
    "    --reducer-count $reducer_count \\\n",
    "    --input-file hdfs://$hdfs_base_folder/input/$input_file \\\n",
    "    --output-folder hdfs://$hdfs_base_folder/output/$output_folder \\\n",
    "    --batch-size 1 \\\n",
    "    --source-node 7827 \\\n",
    "    --target-node 536"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 7.3: Exploratory data analysis (Wikipedia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Using MRJob, explore the Wikipedia network data on the AWS cloud. Reuse your code from HW 7.1---does is scale well? \n",
    "\n",
    "> Be cautioned that Wikipedia is a directed network, where links are not symmetric. \n",
    "So, even though a node may be linked to, it will not appear as a primary record itself if it has no out-links. \n",
    "This means that you may have to ADJUST your code (depending on its design)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 7.4: Shortest path graph distances (Wikipedia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Using MRJob, find shortest path graph distances in the Wikipedia network on the AWS cloud.\n",
    "Reuse your code from 7.2, but once again be warned of Wikipedia being a directed network.\n",
    "When running your code on the Wikipedia network, proof its function by running the job:\n",
    "\n",
    "> - shortest path from \"Ireland\" (index=6176135) to \"University of California, Berkeley\" (index=13466359)\n",
    "\n",
    "> and show your code's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/ubuntu/output/wikipedia.shortest_path\n",
      "2016-03-08 12:01:20.072573 Running iteration 1\n",
      "2016-03-08 12:18:47.889750 Streaming output from runner\n",
      "2016-03-08 12:19:32.111096 Running iteration 2\n",
      "2016-03-08 12:30:42.751660 Streaming output from runner\n",
      "6176135 -> 13466359 = 2 (6176135, 11607791, 13466359)\n",
      "Completed in 2 iterations\n",
      "CPU times: user 23.5 s, sys: 2.43 s, total: 25.9 s\n",
      "Wall time: 30min 10s\n"
     ]
    }
   ],
   "source": [
    "input_file = 'all-pages-indexed-out.txt'\n",
    "output_folder = 'wikipedia.shortest_path'\n",
    "\n",
    "!rm -rf output/$output_folder\n",
    "!hdfs dfs -rm -r -f -skipTrash $hdfs_base_folder/output/$output_folder\n",
    "\n",
    "%time !python ShortestPathGraphDistanceDriver.py \\\n",
    "    --runner-type hadoop \\\n",
    "    --mapper-count $mapper_count \\\n",
    "    --reducer-count $reducer_count \\\n",
    "    --input-file hdfs://$hdfs_base_folder/input/$input_file \\\n",
    "    --output-folder hdfs://$hdfs_base_folder/output/$output_folder \\\n",
    "    --batch-size 1 \\\n",
    "    --source-node 6176135 \\\n",
    "    --target-node 13466359"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 7.5: Conceptual exercise: Largest single-source network distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Suppose you wanted to find the largest network distance from a single source,\n",
    "i.e., a node that is the furthest (but still reachable) from a single source.\n",
    "> As you respond, please comment on program structure, runtimes, iterations, general system requirements, etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 7.5a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> How would you implement this task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 7.5b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> How is this different from finding the shortest path graph distances?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 7.5c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Is this task more difficult to implement than the shortest path distance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 7.6: Computational exercise: Largest single-source network distances (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Using MRJob, write a code to find the largest graph distance and distance-maximizing nodes from a single-source.\n",
    "Test your code first on the toy networks and synonyms network to proof its function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": false,
   "toc_threshold": 4,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
