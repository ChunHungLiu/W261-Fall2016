{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASCI W261: Machine Learning at Scale\n",
    "## Midterm Exam\n",
    "Miki Seltzer (miki.seltzer@berkeley.edu)<br>\n",
    "W261-2, Spring 2016<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We will need these so we can reload modules as we modify them\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using configs in /etc/mrjob.conf\n",
      "using existing scratch bucket mrjob-ac40f1afcc0b86ce\n",
      "using s3://mrjob-ac40f1afcc0b86ce/tmp/ as our scratch dir on S3\n",
      "Creating persistent job flow to run several jobs in...\n",
      "creating tmp directory /tmp/no_script.cloudera.20160302.234446.138781\n",
      "writing master bootstrap script to /tmp/no_script.cloudera.20160302.234446.138781/b.py\n",
      "Copying non-input files into s3://mrjob-ac40f1afcc0b86ce/tmp/no_script.cloudera.20160302.234446.138781/files/\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Creating Elastic MapReduce job flow\n",
      "Job flow created with ID: j-V48FF69EFILM\n",
      "j-V48FF69EFILM\n"
     ]
    }
   ],
   "source": [
    "# Just in case we need a cluster\n",
    "# Create job flow so that we don't need to keep spinning up clusters\n",
    "!python -m mrjob.tools.emr.create_job_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clusterId = 'j-V48FF69EFILM'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the MRJob Class below  calculate the  KL divergence of the following two objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing kltext.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile kltext.txt\n",
    "1.Data Science is an interdisciplinary field about processes and systems to extract knowledge or insights from large volumes of data in various forms (data in various forms, data in various forms, data in various forms), either structured or unstructured,[1][2] which is a continuation of some of the data analysis fields such as statistics, data mining and predictive analytics, as well as Knowledge Discovery in Databases.\n",
    "2.Machine learning is a subfield of computer science[1] that evolved from the study of pattern recognition and computational learning theory in artificial intelligence.[1] Machine learning explores the study and construction of algorithms that can learn from and make predictions on data.[2] Such algorithms operate by building a model from example inputs in order to make data-driven predictions or decisions,[3]:2 rather than following strictly static program instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MRjob class for calculating pairwise similarity using K-L Divergence as the similarity measure\n",
    "\n",
    "Job 1: create inverted index (assume just two objects) <P>\n",
    "Job 2: calculate the similarity of each pair of objects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0986122886681098"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.log(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting kldivergence.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile kldivergence.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "class kldivergence(MRJob):\n",
    "    \n",
    "    def mapper1(self, _, line):\n",
    "        index = int(line.split('.',1)[0])\n",
    "        letter_list = re.sub(r\"[^A-Za-z]+\", '', line).lower()\n",
    "        count = {}\n",
    "        for l in letter_list:\n",
    "            if count.has_key(l):\n",
    "                count[l] += 1\n",
    "            else:\n",
    "                count[l] = 1\n",
    "        for key in count:\n",
    "            # without smoothing\n",
    "            #yield key, [index, (count[key]) * 1.0 / (len(letter_list))]\n",
    "            \n",
    "            # with smoothing\n",
    "            yield key, [index, (count[key] + 1) * 1.0 / (len(letter_list) + 24)]\n",
    "\n",
    "\n",
    "    def reducer1(self, key, values):\n",
    "        postings = {1:None, 2:None}\n",
    "        for val in values:\n",
    "            postings[val[0]] = val[1]\n",
    "        sim = np.log(postings[1]/postings[2]) * postings[1]\n",
    "        yield None, sim\n",
    "    \n",
    "    def reducer2(self, key, values):\n",
    "        kl_sum = 0\n",
    "        for value in values:\n",
    "            kl_sum = kl_sum + value\n",
    "        yield None, kl_sum\n",
    "            \n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.mapper1,\n",
    "                       reducer=self.reducer1),\n",
    "                MRStep(reducer=self.reducer2)]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    kldivergence.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO SMOOTHING\n",
      "(None, 0.08088278445318145)\n"
     ]
    }
   ],
   "source": [
    "from kldivergence import kldivergence\n",
    "\n",
    "mr_job = kldivergence(args=['kltext.txt', '--no-strict-protocol'])\n",
    "print \"NO SMOOTHING\"\n",
    "\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        print mr_job.parse_output_line(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WITH SMOOTHING\n",
      "(None, 0.06726997279170038)\n"
     ]
    }
   ],
   "source": [
    "from kldivergence import kldivergence\n",
    "\n",
    "mr_job = kldivergence(args=['kltext.txt', '--no-strict-protocol'])\n",
    "print \"WITH SMOOTHING\"\n",
    "\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        print mr_job.parse_output_line(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MrJob class for Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Kmeans.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Kmeans.py\n",
    "from numpy import argmin, array, random\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from itertools import chain\n",
    "\n",
    "#Calculate find the nearest centroid for data point \n",
    "def MinDist(datapoint, centroid_points):\n",
    "    datapoint = array(datapoint)\n",
    "    centroid_points = array(centroid_points)\n",
    "    diff = datapoint - centroid_points \n",
    "    diffsq = diff**2\n",
    "    \n",
    "    distances = (diffsq.sum(axis = 1))**0.5\n",
    "    # Get the nearest centroid for each instance\n",
    "    min_idx = argmin(distances)\n",
    "    return min_idx\n",
    "\n",
    "#Euclidean norm\n",
    "def norm(x):\n",
    "    return (x[0]**2 + x[1]**2)**0.5\n",
    "    \n",
    "#Check whether centroids converge\n",
    "def stop_criterion(centroid_points_old, centroid_points_new,T):\n",
    "    oldvalue = list(chain(*centroid_points_old))\n",
    "    newvalue = list(chain(*centroid_points_new))\n",
    "    Diff = [abs(x-y) for x, y in zip(oldvalue, newvalue)]\n",
    "    Flag = True\n",
    "    for i in Diff:\n",
    "        if(i>T):\n",
    "            Flag = False\n",
    "            break\n",
    "    return Flag\n",
    "\n",
    "\n",
    "class MRKmeans(MRJob):\n",
    "    centroid_points=[]\n",
    "    k=3    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init = self.mapper_init, mapper=self.mapper,combiner = self.combiner,reducer=self.reducer)\n",
    "               ]\n",
    "    \n",
    "    #load centroids info from file\n",
    "    def mapper_init(self):\n",
    "        self.centroid_points = [map(float,s.split('\\n')[0].split(',')) for s in open(\"Centroids.txt\").readlines()]\n",
    "        open('Centroids.txt', 'w').close()\n",
    "    \n",
    "    #load data and output the nearest centroid index and data point\n",
    "    ##### THIS IS WHERE WE ACCOUNT FOR WEIGHTS #####\n",
    "    def mapper(self, _, line):\n",
    "        D = (map(float,line.split(',')))\n",
    "        idx = MinDist(D,self.centroid_points)\n",
    "        w = 1/norm(D)\n",
    "        yield int(idx), (w * D[0], w * D[1], w)\n",
    "    \n",
    "    #Combine sum of data points locally\n",
    "    def combiner(self, idx, inputdata):\n",
    "        sumx = sumy = num = 0\n",
    "        for x,y,n in inputdata:\n",
    "            num = num + n\n",
    "            sumx = sumx + x\n",
    "            sumy = sumy + y\n",
    "        yield int(idx),(sumx,sumy,num)\n",
    "    \n",
    "    #Aggregate sum for each cluster and then calculate the new centroids\n",
    "    def reducer(self, idx, inputdata): \n",
    "        centroids = []\n",
    "        num = [0]*self.k \n",
    "        distances = 0\n",
    "        for i in range(self.k):\n",
    "            centroids.append([0,0])\n",
    "        for x, y, n in inputdata:\n",
    "            num[idx] = num[idx] + n\n",
    "            centroids[idx][0] = centroids[idx][0] + x\n",
    "            centroids[idx][1] = centroids[idx][1] + y\n",
    "        centroids[idx][0] = centroids[idx][0]/num[idx]\n",
    "        centroids[idx][1] = centroids[idx][1]/num[idx]\n",
    "        with open('Centroids.txt', 'a') as f:\n",
    "            f.writelines(str(centroids[idx][0]) + ',' + str(centroids[idx][1]) + '\\n')\n",
    "        yield idx,(centroids[idx][0],centroids[idx][1])\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRKmeans.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration1:\n",
      "0 [-2.6816121341554244, 0.4387800225117981]\n",
      "1 [5.203939274722273, 0.18108381085421293]\n",
      "2 [0.2798236662882328, 5.147133354098043]\n",
      "\n",
      "\n",
      "iteration2:\n",
      "0 [-4.499453073691768, 0.1017143951710932]\n",
      "1 [4.7342756092123475, -0.035081051175915486]\n",
      "2 [0.10883719601553689, 4.724161916864905]\n",
      "\n",
      "\n",
      "iteration3:\n",
      "0 [-4.618233072986696, 0.01209570625589213]\n",
      "1 [4.7342756092123475, -0.035081051175915486]\n",
      "2 [0.05163332299537063, 4.637075828035132]\n",
      "\n",
      "\n",
      "iteration4:\n",
      "0 [-4.618233072986696, 0.01209570625589213]\n",
      "1 [4.7342756092123475, -0.035081051175915486]\n",
      "2 [0.05163332299537063, 4.637075828035132]\n",
      "\n",
      "\n",
      "iteration5:\n",
      "0 [-4.618233072986696, 0.01209570625589213]\n",
      "1 [4.7342756092123475, -0.035081051175915486]\n",
      "2 [0.05163332299537063, 4.637075828035132]\n",
      "\n",
      "\n",
      "iteration6:\n",
      "0 [-4.618233072986696, 0.01209570625589213]\n",
      "1 [4.7342756092123475, -0.035081051175915486]\n",
      "2 [0.05163332299537063, 4.637075828035132]\n",
      "\n",
      "\n",
      "iteration7:\n",
      "0 [-4.618233072986696, 0.01209570625589213]\n",
      "1 [4.7342756092123475, -0.035081051175915486]\n",
      "2 [0.05163332299537063, 4.637075828035132]\n",
      "\n",
      "\n",
      "iteration8:\n",
      "0 [-4.618233072986696, 0.01209570625589213]\n",
      "1 [4.7342756092123475, -0.035081051175915486]\n",
      "2 [0.05163332299537063, 4.637075828035132]\n",
      "\n",
      "\n",
      "iteration9:\n",
      "0 [-4.618233072986696, 0.01209570625589213]\n",
      "1 [4.7342756092123475, -0.035081051175915486]\n",
      "2 [0.05163332299537063, 4.637075828035132]\n",
      "\n",
      "\n",
      "iteration10:\n",
      "0 [-4.618233072986696, 0.01209570625589213]\n",
      "1 [4.7342756092123475, -0.035081051175915486]\n",
      "2 [0.05163332299537063, 4.637075828035132]\n",
      "\n",
      "\n",
      "Centroids\n",
      "\n",
      "[[-4.618233072986696, 0.01209570625589213], [4.7342756092123475, -0.035081051175915486], [0.05163332299537063, 4.637075828035132]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import random, array\n",
    "from Kmeans import MRKmeans, stop_criterion\n",
    "mr_job = MRKmeans(args=['Kmeandata.csv', '--file', 'Centroids.txt','--no-strict-protocol'])\n",
    "\n",
    "#Geneate initial centroids\n",
    "centroid_points = [[0,0],[6,3],[3,6]]\n",
    "k = 3\n",
    "with open('Centroids.txt', 'w+') as f:\n",
    "        f.writelines(','.join(str(j) for j in i) + '\\n' for i in centroid_points)\n",
    "\n",
    "# Update centroids iteratively\n",
    "for i in range(10):\n",
    "    # save previous centoids to check convergency\n",
    "    centroid_points_old = centroid_points[:]\n",
    "    print \"iteration\"+str(i+1)+\":\"\n",
    "    with mr_job.make_runner() as runner: \n",
    "        runner.run()\n",
    "        # stream_output: get access of the output \n",
    "        for line in runner.stream_output():\n",
    "            key,value =  mr_job.parse_output_line(line)\n",
    "            print key, value\n",
    "            centroid_points[key] = value\n",
    "    print \"\\n\"\n",
    "    i = i + 1\n",
    "print \"Centroids\\n\"\n",
    "print centroid_points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Go through each data point, find closest centroid, then find weighted distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5932559652\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from numpy import argmin, array, random\n",
    "\n",
    "#Euclidean norm\n",
    "def norm(x):\n",
    "    return (x[0]**2 + x[1]**2)**0.5\n",
    "\n",
    "#Calculate find the nearest centroid for data point \n",
    "def smallestDist(datapoint, centroid_points):\n",
    "    datapoint = array(datapoint)\n",
    "    centroid_points = array(centroid_points)\n",
    "    diff = datapoint - centroid_points \n",
    "    diffsq = diff**2\n",
    "    \n",
    "    distances = (diffsq.sum(axis = 1))**0.5\n",
    "    # Get the nearest centroid for each instance\n",
    "    min_idx = argmin(distances)\n",
    "    return distances[min_idx]\n",
    "\n",
    "data = []\n",
    "\n",
    "centroids = [[-4.5,0.0],[4.5,0.0],[0.0,4.5]]\n",
    "\n",
    "num = 0.0\n",
    "den = 0.0\n",
    "\n",
    "with open('Kmeandata.csv', 'r') as infile:\n",
    "    for line in csv.reader(infile):\n",
    "        point = [float(line[0]), float(line[1])]\n",
    "        weight = 1/norm(point)\n",
    "        num += smallestDist(point, centroids) * weight\n",
    "        den += weight\n",
    "\n",
    "print num / den\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
