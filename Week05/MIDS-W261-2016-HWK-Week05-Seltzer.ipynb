{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASCI W261: Machine Learning at Scale\n",
    "## Assignment Week 5\n",
    "Miki Seltzer (miki.seltzer@berkeley.edu)<br>\n",
    "W261-2, Spring 2016<br>\n",
    "Submission: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## HW5.0:\n",
    "### What is a data warehouse?\n",
    "A data warehouse is a repository for one or multiple data sources. Data warehouses can contain relational databases.\n",
    "\n",
    "### What is a star schema?\n",
    "A star schema relates multiple fact and dimension tables, and is similar to the snowflake schema. In both schemas, fact tables are referenced by dimension tables (one or multiple). However, star schemas are denormalized, whereas snowflake schemas are normalized.\n",
    "\n",
    "### When is it used?\n",
    "A star schema is used to organize the meta data of a relational database, such as which tables can be joined, and the keys on which they can be joined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW5.1:\n",
    "### In the database world, what is 3NF?\n",
    "3NF is shorthand for third normal form. A table is in third normal form if the following conditions hold:\n",
    "- The table is already in second normal form\n",
    "- Non-prime attributes of the table are non-transitively dependent on every key in the table\n",
    "\n",
    "### Does machine learning use data in 3NF?\n",
    "ML can, but does not always use data in 3NF.\n",
    "\n",
    "### If so, why?\n",
    "3NF can save a significant amount of disk space because data duplication is avoided. Additionally, if data is denormalized, then fields in the data set might be related to each other and create dependencies. This may be problematic if we are using algorithms that require independent features.\n",
    "\n",
    "### In what form does ML consume data?\n",
    "Typically, ML requires all data to be fed into an algorithm to be collected into a single source. Thus, the easiest way for ML to ingest data is for it to be denormalized.\n",
    "\n",
    "### Why would one use log files that are denormalized?\n",
    "If one needs to perform real-time analysis on log files, it may be too time consuming to join normalized log files with other tables. If log files are denormalized, they may not need any further processing (joins) to be fed into other steps of a pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.2: Using MRJob, implement a hashside join (memory-backed map-side) for left, right and inner joins. Run your code on the  data used in HW 4.4.\n",
    "\n",
    "### Justify which table you chose as left table in this hashside join\n",
    "The two tables used were:\n",
    "- **anonymous-msweb-preprocess.data:** The log file of visitors and each page that they visited (processed rows prefixed by 'C' or 'V')\n",
    "- **attributes.csv:** The page ID, page name and URL of each page (prefixed by 'A' in the original data)\n",
    "\n",
    "The attributes.csv file was very small (only 294 lines), so this is the file that I chose to store in memory. This became my **right** table.\n",
    "\n",
    "The log file was much larger, so I streamed through this file, and used it as the **left** table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# We will need these so we can reload modules as we modify them\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapSideJoin.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapSideJoin.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    " \n",
    "class join(MRJob):\n",
    "    \n",
    "    # Specify some custom options so we only have to write one MRJob class for each join\n",
    "    def configure_options(self):\n",
    "        super(join, self).configure_options()\n",
    "        self.add_passthrough_option('--joinType', default='inner', )\n",
    "    \n",
    "    # Store attributes.csv into memory\n",
    "    #  - account for multiple occurrences of keys\n",
    "    #  - self.pages is dict with a list of [pageName, pageURL] pairs\n",
    "    # Set joinType variable\n",
    "    def mapper_init(self):\n",
    "        self.pages = {}\n",
    "        with open('attributes.csv','r') as myfile:\n",
    "            for line in myfile:\n",
    "                fields = line.strip().split(',')\n",
    "                if fields[0] not in self.pages:\n",
    "                    self.pages[fields[0]]=[]\n",
    "                self.pages[fields[0]].append([fields[1], fields[2]])\n",
    "        self.joinType = self.options.joinType\n",
    "        self.seenRight = set()\n",
    "    \n",
    "    # RIGHT table is stored in memory (self.pages)\n",
    "    # LEFT table is streamed\n",
    "    # We need so keep track of which RIGHT keys we have seen\n",
    "    def mapper(self, _, line):\n",
    "        fields = line.split(',')\n",
    "        key = fields[0]\n",
    "        self.seenRight.add(key)\n",
    "        if key in self.pages:\n",
    "            for i in self.pages[key]:\n",
    "                yield key, (fields[1], fields[2], i[0], i[1])\n",
    "        elif self.joinType == 'left':\n",
    "            yield key, (fields[1], fields[2], None, None)\n",
    "\n",
    "    # We need to emit all of the RIGHT keys that we never saw while streaming through LEFT\n",
    "    # We will need to deduplicate these in the reducer in case we have multiple mappers\n",
    "    def mapper_final(self):\n",
    "        if self.joinType == 'right':\n",
    "            for key in self.pages:\n",
    "                if key not in self.seenRight:\n",
    "                    for value in self.pages[key]:\n",
    "                        yield key, (None, None, value[0], value[1])\n",
    "    \n",
    "    # Need to persist variables\n",
    "    def reducer_init(self):\n",
    "        self.joinType = self.options.joinType\n",
    "    \n",
    "    # We need to unpack and emit each record\n",
    "    # We also need to do some work emitting records for the right join\n",
    "    def reducer(self, key, values):\n",
    "        emptyRight = True\n",
    "        for val in values:\n",
    "            if self.joinType == 'inner' or self.joinType == 'left':\n",
    "                yield key, val\n",
    "            elif self.joinType == 'right':\n",
    "                if val[:2] != [None]*2:\n",
    "                    emptyRight = False\n",
    "                    yield key, val\n",
    "                else: emptyRecord = val\n",
    "        if emptyRight and self.joinType == 'right':\n",
    "            yield key, emptyRecord\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    join.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    }
   ],
   "source": [
    "from mapSideJoin import join\n",
    "\n",
    "def runJoin(joinType):\n",
    "\n",
    "    mr_job = join(args=['TopVisitors.txt', '--file', 'attributes.csv', '--joinType', joinType])\n",
    "    output = []\n",
    "\n",
    "    with mr_job.make_runner() as runner: \n",
    "        # Run MRJob\n",
    "        runner.run()\n",
    "\n",
    "        # Write stream_output to file\n",
    "        for line in runner.stream_output():\n",
    "            output.append(mr_job.parse_output_line(line))\n",
    "    \n",
    "    return output\n",
    "            \n",
    "outInner = runJoin('inner')\n",
    "outLeft = runJoin('left')\n",
    "outRight = runJoin('right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows resulting from join type:\n",
      "\n",
      "inner   285\n",
      "left    285\n",
      "right   294\n"
     ]
    }
   ],
   "source": [
    "print \"Rows resulting from join type:\\n\"\n",
    "for joinType in ['inner', 'left', 'right']:\n",
    "    if joinType == 'inner': out = outInner\n",
    "    elif joinType == 'left': out = outLeft\n",
    "    elif joinType == 'right': out = outRight\n",
    "    \n",
    "    print \"{:7s}{:>4,d}\".format(joinType, len(out))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW5.3: Do some EDA on this data set using MRJob:\n",
    "- Longest 5-gram (number of characters)\n",
    "- Top 10 most frequent words (count), i.e., unigrams\n",
    "- Most/least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency\n",
    "- Distribution of 5-gram sizes (counts) sorted in decreasing order of relative frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "docs = {}\n",
    "docs['A'] = ['X']*20\n",
    "docs['A'].extend(['Y']*30)\n",
    "docs['A'].extend(['Z']*5)\n",
    "docs['B'] = ['X']*100\n",
    "docs['B'].extend(['Y']*20)\n",
    "docs['C'] = ['M']*5\n",
    "docs['C'].extend(['N']*20)\n",
    "docs['C'].extend(['Z']*5)\n",
    "\n",
    "\n",
    "# Create the file for unit testing\n",
    "with open('unitTest.txt', 'w') as myfile:\n",
    "    outWriter = csv.writer(myfile)\n",
    "    for doc in docs:\n",
    "        row = [doc]\n",
    "        row.extend(docs[doc])\n",
    "        outWriter.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MRJob5_4.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MRJob5_4.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    " \n",
    "class job(MRJob):\n",
    "    \n",
    "    # Specify some custom options so we only have to write one MRJob class for each part\n",
    "    def configure_options(self):\n",
    "        super(job, self).configure_options()\n",
    "        self.add_passthrough_option('--part', default='1')\n",
    "    \n",
    "    \"\"\"\n",
    "    Find the longest 5-gram\n",
    "    - In this case, in each mapper, we only need to store the length of the longest 5-gram we have seen\n",
    "    - After the mapper has run, we emit the longest 5-gram from this mapper\n",
    "    - All results will be sent to the same reducer (key = None)\n",
    "    \"\"\"\n",
    "    \n",
    "    def mapper_longest5gram_init(self):\n",
    "        self.maxLength = 0\n",
    "    \n",
    "    def mapper_longest5gram(self, _, line):\n",
    "        fields = line.strip().split('\\t')\n",
    "        if len(fields[0]) > self.maxLength: \n",
    "            self.maxLength = len(fields[0])\n",
    "            \n",
    "    def mapper_longest5gram_final(self):\n",
    "        yield None, self.maxLength\n",
    "    \n",
    "    def reducer_longest5gram(self, key, values):\n",
    "        yield None, max(values)\n",
    "    \n",
    "    \"\"\"\n",
    "    Top 10 most frequent words\n",
    "    \"\"\"\n",
    "\n",
    "    def mapper_top10words_init(self):\n",
    "        self.\n",
    "    \n",
    "    def mapper_top10words(self, _, line):\n",
    "        fields = line.strip().split('\\t')\n",
    "        words = fields[0].lower().split()\n",
    "        count, pages_count, books_count = fields[1], fields[2], fields[3]\n",
    "        for word in words:\n",
    "            yield word, (count, pages_count)\n",
    "    \n",
    "#     def mapper_top10words\n",
    "        \n",
    "    \n",
    "    # Multi-step pipeline definition\n",
    "    def steps(self):\n",
    "        self.part = self.options.part\n",
    "        if self.part == '1':\n",
    "            return [\n",
    "                MRStep(mapper_init=self.mapper_longest5gram_init,\n",
    "                       mapper=self.mapper_longest5gram,\n",
    "                       mapper_final=self.mapper_longest5gram_final,\n",
    "                       reducer=self.reducer_longest5gram)\n",
    "            ]\n",
    "        elif self.part == '2':\n",
    "            return [\n",
    "                MRStep(mapper=self.mapper_top10words)\n",
    "            ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'bill', 'for', 'establishing', 'religious']\n",
      "['a', 'biography', 'of', 'general', 'george']\n",
      "['a', 'case', 'study', 'in', 'government']\n",
      "['a', 'case', 'study', 'of', 'female']\n",
      "['a', 'case', 'study', 'of', 'limited']\n",
      "['a', \"child's\", 'christmas', 'in', 'wales']\n",
      "['a', 'circumstantial', 'narrative', 'of', 'the']\n",
      "['a', 'city', 'by', 'the', 'sea']\n",
      "['a', 'collection', 'of', 'fairy', 'tales']\n",
      "['a', 'collection', 'of', 'forms', 'of']\n",
      "['a', 'commentary', 'on', 'his', 'apology']\n",
      "['a', 'comparative', 'study', 'of', 'juvenile']\n",
      "['a', 'comparison', 'of', 'the', 'properties']\n",
      "['a', 'conceptual', 'framework', 'and', 'the']\n",
      "['a', 'conceptual', 'framework', 'for', 'life']\n",
      "['a', 'concise', 'bibliography', 'of', 'the']\n",
      "['a', 'continuation', 'of', 'the', 'letters']\n",
      "['a', 'critical', 'review', 'and', 'a']\n",
      "['a', 'critique', 'and', 'a', 'guide']\n",
      "['a', 'defence', 'of', 'the', 'royal']\n",
      "['a', 'defence', 'of', 'the', 'short']\n",
      "['a', 'discovery', 'of', 'the', 'real']\n",
      "['a', 'further', 'look', 'at', 'the']\n",
      "['a', 'festschrift', 'in', 'honour', 'of']\n",
      "['a', 'funny', 'dirty', 'little', 'war']\n",
      "['a', 'game', 'of', \"cat's\", 'cradle']\n",
      "['a', 'guide', 'to', \"america's\", 'censorship']\n",
      "['a', 'handbook', 'on', 'theodolite', 'surveying']\n",
      "['a', 'history', 'of', 'travel', 'in']\n",
      "['a', 'history', 'of', 'aerial', 'navigation']\n",
      "['a', 'history', 'of', 'modern', 'southeast']\n",
      "['a', 'history', 'of', 'postwar', 'american']\n",
      "['a', 'history', 'of', 'railroads', 'in']\n",
      "['a', 'history', 'of', 'and', 'for']\n"
     ]
    }
   ],
   "source": [
    "from MRJob5_4 import job\n",
    "\n",
    "def runJob(part):\n",
    "\n",
    "    mr_job = job(args=['./filtered-5Grams/short-5gram.txt', '--part', str(part)])\n",
    "    #mr_job = job(args=['./filtered-5Grams/googlebooks-eng-all-5gram-20090715-0-filtered.txt', '--part', str(part)])\n",
    "    output = []\n",
    "\n",
    "    with mr_job.make_runner() as runner: \n",
    "        # Run MRJob\n",
    "        runner.run()\n",
    "\n",
    "        # Write stream_output to file\n",
    "        for line in runner.stream_output():\n",
    "            print mr_job.parse_output_line(line)\n",
    "            \n",
    "runJob(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW5.4\n",
    "### 1. Build stripes of word co-occurrence for the top 10,000\n",
    "### 2. Using two (symmetric) comparison methods of your choice, pairwise compare all stripes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
