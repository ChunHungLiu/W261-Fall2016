{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASCI W261: Machine Learning at Scale\n",
    "## Assignment Week 5\n",
    "Miki Seltzer (miki.seltzer@berkeley.edu)<br>\n",
    "W261-2, Spring 2016<br>\n",
    "Submission: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## HW5.0:\n",
    "### What is a data warehouse?\n",
    "A data warehouse is a repository for one or multiple data sources. Data warehouses can contain relational databases.\n",
    "\n",
    "### What is a star schema?\n",
    "A star schema relates multiple fact and dimension tables, and is similar to the snowflake schema. In both schemas, fact tables are referenced by dimension tables (one or multiple). However, star schemas are denormalized, whereas snowflake schemas are normalized.\n",
    "\n",
    "### When is it used?\n",
    "A star schema is used to organize the meta data of a relational database, such as which tables can be joined, and the keys on which they can be joined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW5.1:\n",
    "### In the database world, what is 3NF?\n",
    "3NF is shorthand for third normal form. A table is in third normal form if the following conditions hold:\n",
    "- The table is already in second normal form\n",
    "- Non-prime attributes of the table are non-transitively dependent on every key in the table\n",
    "\n",
    "### Does machine learning use data in 3NF?\n",
    "ML can, but does not always use data in 3NF.\n",
    "\n",
    "### If so, why?\n",
    "3NF can save a significant amount of disk space because data duplication is avoided. Additionally, if data is denormalized, then fields in the data set might be related to each other and create dependencies. This may be problematic if we are using algorithms that require independent features.\n",
    "\n",
    "### In what form does ML consume data?\n",
    "Typically, ML requires all data to be fed into an algorithm to be collected into a single source. Thus, the easiest way for ML to ingest data is for it to be denormalized.\n",
    "\n",
    "### Why would one use log files that are denormalized?\n",
    "If one needs to perform real-time analysis on log files, it may be too time consuming to join normalized log files with other tables. If log files are denormalized, they may not need any further processing (joins) to be fed into other steps of a pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.2: Using MRJob, implement a hashside join (memory-backed map-side) for left, right and inner joins. Run your code on the  data used in HW 4.4.\n",
    "\n",
    "### Justify which table you chose as left table in this hashside join\n",
    "The two tables used were:\n",
    "- **anonymous-msweb-preprocess.data:** The log file of visitors and each page that they visited (processed rows prefixed by 'C' or 'V')\n",
    "- **attributes.csv:** The page ID, page name and URL of each page (prefixed by 'A' in the original data)\n",
    "\n",
    "The attributes.csv file was very small (only 294 lines), so this is the file that I chose to store in memory. This became my **right** table.\n",
    "\n",
    "The log file was much larger, so I streamed through this file, and used it as the **left** table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We will need these so we can reload modules as we modify them\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapSideJoin.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapSideJoin.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    " \n",
    "class join(MRJob):\n",
    "    \n",
    "    # Specify some custom options so we only have to write one MRJob class for each join\n",
    "    def configure_options(self):\n",
    "        super(join, self).configure_options()\n",
    "        self.add_passthrough_option('--joinType', default='inner', )\n",
    "    \n",
    "    # Store attributes.csv into memory\n",
    "    #  - account for multiple occurrences of keys\n",
    "    #  - self.pages is dict with a list of [pageName, pageURL] pairs\n",
    "    # Set joinType variable\n",
    "    def mapper_init(self):\n",
    "        self.pages = {}\n",
    "        with open('attributes.csv','r') as myfile:\n",
    "            for line in myfile:\n",
    "                fields = line.strip().split(',')\n",
    "                if fields[0] not in self.pages:\n",
    "                    self.pages[fields[0]]=[]\n",
    "                self.pages[fields[0]].append([fields[1], fields[2]])\n",
    "        self.joinType = self.options.joinType\n",
    "        self.seenRight = set()\n",
    "    \n",
    "    # RIGHT table is stored in memory (self.pages)\n",
    "    # LEFT table is streamed\n",
    "    # We need so keep track of which RIGHT keys we have seen\n",
    "    def mapper(self, _, line):\n",
    "        fields = line.split(',')\n",
    "        key = fields[0]\n",
    "        self.seenRight.add(key)\n",
    "        if key in self.pages:\n",
    "            for i in self.pages[key]:\n",
    "                yield key, (fields[1], fields[2], i[0], i[1])\n",
    "        elif self.joinType == 'left':\n",
    "            yield key, (fields[1], fields[2], None, None)\n",
    "\n",
    "    # We need to emit all of the RIGHT keys that we never saw while streaming through LEFT\n",
    "    # We will need to deduplicate these in the reducer in case we have multiple mappers\n",
    "    def mapper_final(self):\n",
    "        if self.joinType == 'right':\n",
    "            for key in self.pages:\n",
    "                if key not in self.seenRight:\n",
    "                    for value in self.pages[key]:\n",
    "                        yield key, (None, None, value[0], value[1])\n",
    "    \n",
    "    # Need to persist variables\n",
    "    def reducer_init(self):\n",
    "        self.joinType = self.options.joinType\n",
    "    \n",
    "    # We need to unpack and emit each record\n",
    "    # We also need to do some work emitting records for the right join\n",
    "    def reducer(self, key, values):\n",
    "        emptyRight = True\n",
    "        for val in values:\n",
    "            if self.joinType == 'inner' or self.joinType == 'left':\n",
    "                yield key, val\n",
    "            elif self.joinType == 'right':\n",
    "                if val[:2] != [None]*2:\n",
    "                    emptyRight = False\n",
    "                    yield key, val\n",
    "                else: emptyRecord = val\n",
    "        if emptyRight and self.joinType == 'right':\n",
    "            yield key, emptyRecord\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    join.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    }
   ],
   "source": [
    "from mapSideJoin import join\n",
    "\n",
    "def runJoin(joinType):\n",
    "\n",
    "    mr_job = join(args=['TopVisitors.txt', '--file', 'attributes.csv', '--joinType', joinType])\n",
    "    output = []\n",
    "\n",
    "    with mr_job.make_runner() as runner: \n",
    "        # Run MRJob\n",
    "        runner.run()\n",
    "\n",
    "        # Write stream_output to file\n",
    "        for line in runner.stream_output():\n",
    "            output.append(mr_job.parse_output_line(line))\n",
    "    \n",
    "    return output\n",
    "            \n",
    "outInner = runJoin('inner')\n",
    "outLeft = runJoin('left')\n",
    "outRight = runJoin('right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows resulting from join type:\n",
      "\n",
      "inner   285\n",
      "left    285\n",
      "right   294\n"
     ]
    }
   ],
   "source": [
    "print \"Rows resulting from join type:\\n\"\n",
    "for joinType in ['inner', 'left', 'right']:\n",
    "    if joinType == 'inner': out = outInner\n",
    "    elif joinType == 'left': out = outLeft\n",
    "    elif joinType == 'right': out = outRight\n",
    "    \n",
    "    print \"{:7s}{:>4,d}\".format(joinType, len(out))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW5.3: Do some EDA on this data set using MRJob:\n",
    "1. Longest 5-gram (number of characters)\n",
    "2. Top 10 most frequent words (count), i.e., unigrams\n",
    "3. Top 20 most/least densely appearing words (count/pages_count) sorted in decreasing order\n",
    "4. Distribution of 5-gram sizes (counts) sorted in decreasing order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MRJob5_3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MRJob5_3.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class job(MRJob):\n",
    "    \n",
    "    # Specify some custom options so we only have to write one MRJob class for each part\n",
    "    def configure_options(self):\n",
    "        super(job, self).configure_options()\n",
    "        self.add_passthrough_option('--part', default='1')\n",
    "    \n",
    "    \"\"\"\n",
    "    Find the longest 5-gram\n",
    "    - In this case, in each mapper, we only need to store the length of the longest 5-gram we have seen\n",
    "    - After the mapper has run, we emit the longest 5-gram from this mapper\n",
    "    - All results will be sent to the same reducer (we specify this)\n",
    "    - Then we loop through the records in the reducer and emit the remaining longest 5-gram\n",
    "    \"\"\"\n",
    "    \n",
    "    def mapper_longest5Gram_init(self):\n",
    "        self.maxLength = 0\n",
    "        self.longest5Gram = None\n",
    "    \n",
    "    def mapper_longest5Gram(self, _, line):\n",
    "        fields = line.strip().split('\\t')\n",
    "        if len(fields[0]) > self.maxLength: \n",
    "            self.maxLength = len(fields[0])\n",
    "            self.longest5Gram = fields[0]\n",
    "            \n",
    "    def mapper_longest5Gram_final(self):\n",
    "        yield self.longest5Gram, self.maxLength\n",
    "     \n",
    "    def reducer_longest5Gram_init(self):\n",
    "        self.maxLength = 0\n",
    "        self.longest5Gram = None\n",
    "    \n",
    "    def reducer_longest5Gram(self, key, values):\n",
    "        for val in values:\n",
    "            if val > self.maxLength:\n",
    "                self.maxLength = val\n",
    "                self.longest5Gram = key\n",
    "        \n",
    "    def reducer_longest5Gram_final(self):\n",
    "        yield self.maxLength, self.longest5Gram\n",
    "    \n",
    "    \"\"\"\n",
    "    Top 10 most frequent words\n",
    "    - This is our standard word count\n",
    "    - Loop through each word in the 5-gram and emit (word, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    def mapper_topWords(self, _, line):\n",
    "        fields = line.strip().split('\\t')\n",
    "        words = fields[0].lower().split()\n",
    "        count, pages_count, books_count = int(fields[1]), int(fields[2]), int(fields[3])\n",
    "        for word in words:\n",
    "            self.increment_counter('total', 'words', count)\n",
    "            yield word, count\n",
    "        \n",
    "    def combiner_topWords(self, key, values):\n",
    "        yield key, sum(values)\n",
    "        \n",
    "    def reducer_topWords(self, key, values):\n",
    "        yield key, sum(values)\n",
    " \n",
    "    \"\"\"\n",
    "    Densely appearing words\n",
    "    - For each word, emit count and pages_count\n",
    "    - Combiner sums count and pages_count\n",
    "    - Reducer sums count and pages_count, then emits count/pages_count\n",
    "    \"\"\"\n",
    "    \n",
    "    def mapper_denseWords(self, _, line):\n",
    "        fields = line.strip().split('\\t')\n",
    "        words = fields[0].lower().split()\n",
    "        count, pages_count, books_count = int(fields[1]), int(fields[2]), int(fields[3])\n",
    "        for word in words:\n",
    "            yield word, (count, pages_count)\n",
    "        \n",
    "    def combiner_denseWords(self, key, values):\n",
    "        count, pages_count = 0.0, 0.0\n",
    "        for val in values:\n",
    "            count += val[0]\n",
    "            pages_count += val[1]\n",
    "        yield key, (count, pages_count)\n",
    "        \n",
    "    def reducer_denseWords(self, key, values):\n",
    "        count, pages_count = 0.0, 0.0\n",
    "        for val in values:\n",
    "            count += val[0]\n",
    "            pages_count += val[1]\n",
    "        yield key, count/pages_count\n",
    "\n",
    "    \"\"\"\n",
    "    Frequent5-grams\n",
    "    - Use count to determine the most frequent 5-gram\n",
    "    - Sum counts in combiner and reducer\n",
    "    \"\"\"\n",
    "        \n",
    "    def mapper_frequent5Gram(self, _, line):\n",
    "        fields = line.strip().split('\\t')\n",
    "        yield len(fields[0]), float(fields[1])\n",
    "        \n",
    "    def combiner_frequent5Gram(self, key, values):\n",
    "        yield key, sum(values)\n",
    "        \n",
    "    def reducer_frequent5Gram(self, key, values):\n",
    "        yield key, sum(values)\n",
    "        \n",
    "    \"\"\"\n",
    "    Sorting functions\n",
    "    - We need these to get the top and bottom values\n",
    "    - Utilize only one reducer instead of writing a custom partitioner\n",
    "    \"\"\"\n",
    "    def mapper_sort(self, key, value):\n",
    "        yield float(value), key\n",
    "        \n",
    "    def reducer_sort_init(self):\n",
    "        self.count = 0\n",
    "    \n",
    "    def reducer_top10(self, key, values):\n",
    "        for val in values:\n",
    "            if self.count < 10:\n",
    "                yield key, val\n",
    "                self.count += 1\n",
    "                \n",
    "    def reducer_top100(self, key, values):\n",
    "        for val in values:\n",
    "            if self.count < 100:\n",
    "                yield key, val\n",
    "                self.count += 1\n",
    "                \n",
    "    def reducer_top10000(self, key, values):\n",
    "        for val in values:\n",
    "            if self.count < 10000:\n",
    "                yield key, val\n",
    "                self.count += 1\n",
    "                \n",
    "    def reducer_all(self, key, values):\n",
    "        for val in values:\n",
    "            yield key, val\n",
    "\n",
    "    \"\"\"\n",
    "    Multi-step pipeline definitions\n",
    "    Based on user input when calling runner function\n",
    "    \"\"\"\n",
    "    def steps(self):\n",
    "        self.part = self.options.part\n",
    "        if self.part == '1':\n",
    "            return [\n",
    "                MRStep(mapper_init=self.mapper_longest5Gram_init,\n",
    "                       mapper=self.mapper_longest5Gram,\n",
    "                       mapper_final=self.mapper_longest5Gram_final,\n",
    "                       reducer_init=self.reducer_longest5Gram_init,\n",
    "                       reducer=self.reducer_longest5Gram,\n",
    "                       reducer_final=self.reducer_longest5Gram_final,\n",
    "                       jobconf={'mapred.reduce.tasks': 1})\n",
    "            ]\n",
    "        elif self.part == '2':\n",
    "            return [\n",
    "                MRStep(mapper=self.mapper_topWords,\n",
    "                       combiner=self.combiner_topWords,\n",
    "                       reducer=self.reducer_topWords),\n",
    "                MRStep(mapper=self.mapper_sort,\n",
    "                       reducer_init=self.reducer_sort_init,\n",
    "                       reducer=self.reducer_all,\n",
    "                       jobconf={'mapred.output.key.comparator.class':'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                                'mapred.text.key.partitioner.options':'-k1,1',\n",
    "                                'stream.num.map.output.key.fields':1,\n",
    "                                'mapred.text.key.comparator.options':'-k1,1nr',\n",
    "                                'mapred.reduce.tasks': 1})\n",
    "            ]\n",
    "        elif self.part == '3':\n",
    "            return [\n",
    "                MRStep(mapper=self.mapper_denseWords,\n",
    "                       combiner=self.combiner_denseWords,\n",
    "                       reducer=self.reducer_denseWords),\n",
    "                MRStep(mapper=self.mapper_sort,\n",
    "                       reducer_init=self.reducer_sort_init,\n",
    "                       reducer=self.reducer_all,\n",
    "                       jobconf={'mapred.output.key.comparator.class':'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                                'mapred.text.key.partitioner.options':'-k1,1',\n",
    "                                'stream.num.map.output.key.fields':1,\n",
    "                                'mapred.text.key.comparator.options':'-k1,1nr',\n",
    "                                'mapred.reduce.tasks': 1})\n",
    "            ]\n",
    "        elif self.part == '4':\n",
    "            return [\n",
    "                MRStep(mapper=self.mapper_frequent5Gram,\n",
    "                       combiner=self.combiner_frequent5Gram,\n",
    "                       reducer=self.reducer_frequent5Gram),\n",
    "                MRStep(mapper=self.mapper_sort,\n",
    "                       reducer_init=self.reducer_sort_init,\n",
    "                       reducer=self.reducer_all,\n",
    "                       jobconf={'mapred.output.key.comparator.class':'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                                'mapred.text.key.partitioner.options':'-k1,1',\n",
    "                                'stream.num.map.output.key.fields':1,\n",
    "                                'mapred.text.key.comparator.options':'-k1,1nr',\n",
    "                                'mapred.reduce.tasks': 1})\n",
    "            ]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using configs in /etc/mrjob.conf\n",
      "using existing scratch bucket mrjob-ac40f1afcc0b86ce\n",
      "using s3://mrjob-ac40f1afcc0b86ce/tmp/ as our scratch dir on S3\n",
      "Creating persistent job flow to run several jobs in...\n",
      "creating tmp directory /tmp/no_script.cloudera.20160216.000045.000938\n",
      "writing master bootstrap script to /tmp/no_script.cloudera.20160216.000045.000938/b.py\n",
      "Copying non-input files into s3://mrjob-ac40f1afcc0b86ce/tmp/no_script.cloudera.20160216.000045.000938/files/\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Creating Elastic MapReduce job flow\n",
      "Job flow created with ID: j-2GY8DK2A59MD0\n",
      "j-2GY8DK2A59MD0\n"
     ]
    }
   ],
   "source": [
    "# Create job flow so that we don't need to keep spinning up clusters\n",
    "!python -m mrjob.tools.emr.create_job_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clusterId = 'j-2GY8DK2A59MD0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create our runner and supporting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from MRJob5_3 import job\n",
    "\n",
    "def runJob5_3(filename, part, s3bucket):\n",
    "\n",
    "    #mr_job = job(args=[filename, '--part', str(part)])\n",
    "    #mr_job = job(args=[filename, '--part', str(part), '-r', 'hadoop', '--hadoop-home', '/usr/'])\n",
    "    mr_job = job(args=[filename, '--part', str(part), '--no-output', '--output-dir', s3bucket,\n",
    "                       '-r', 'emr', '--emr-job-flow-id', clusterId])\n",
    "    \n",
    "    output = []\n",
    "\n",
    "    with mr_job.make_runner() as runner: \n",
    "        # Run MRJob\n",
    "        runner.run()\n",
    "\n",
    "        # Write stream_output to file\n",
    "        for line in runner.stream_output():\n",
    "            output.append(mr_job.parse_output_line(line))\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_output(output, part):\n",
    "    if part == 1:\n",
    "        print \"Longest 5-gram (number of characters)\"\n",
    "        print '{:>21s}   {:<10s}'.format('length', '5gram')\n",
    "    elif part == 2:\n",
    "        print \"Top 10 words (using count information)\"\n",
    "        print '{:>21s}   {:<10s}'.format('count', 'word')\n",
    "    elif part == 3:\n",
    "        print \"Top 20 most/least dense words\"\n",
    "        print '{:>21s}   {:<10s}'.format('density', 'word')\n",
    "    elif part == 4:\n",
    "        print \"Distribution of 5gram sizes\"\n",
    "        print '{:>21s}   {:<10s}'.format('count', 'length')\n",
    "    \n",
    "    print '-------------------------------------------------------'\n",
    "    \n",
    "    for item in output:\n",
    "        if part == 3:\n",
    "            print '{:21.5f}   {:<50s}'.format(item[0], item[1])\n",
    "        elif part == 4:\n",
    "            print '{:21,d}   {:<50d}'.format(int(item[0]), item[1])\n",
    "        else:\n",
    "            print '{:21,d}   {:<50s}'.format(int(item[0]), item[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run each part of the script on the full 5gram dataset on EMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myfile = 's3://filtered-5grams/'\n",
    "#myfile = './filtered-5Grams/short-5gram.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Longest 5-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest 5-gram (number of characters)\n",
      "               length   5gram     \n",
      "-------------------------------------------------------\n",
      "                  159   AIOPJUMRXUYVASLYHYPSIBEMAPODIKR UFRYDIUUOLBIGASUAURUSREXLISNAYE RNOONDQSRUNSUBUNOUGRABBERYAIRTC UTAHRAPTOREDILEIPMILBDUMMYUVERI SYEVRAHVELOCYALLOSAURUSLINROTSR\n"
     ]
    }
   ],
   "source": [
    "output_bucket = 's3://ms-w261-hw05/hw5_3a'\n",
    "\n",
    "!aws s3 rm --recursive {output_bucket}\n",
    "\n",
    "part = 1\n",
    "output = runJob5_3(myfile, part, output_bucket)\n",
    "format_output(output, part)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words (using count information)\n",
      "                count   word      \n",
      "-------------------------------------------------------\n",
      "        5,490,815,394   the                                               \n",
      "        3,698,583,299   of                                                \n",
      "        2,227,866,570   to                                                \n",
      "        1,421,312,776   in                                                \n",
      "        1,361,123,022   a                                                 \n",
      "        1,149,577,477   and                                               \n",
      "          802,921,147   that                                              \n",
      "          758,328,796   is                                                \n",
      "          688,707,130   be                                                \n",
      "          492,170,314   as                                                \n"
     ]
    }
   ],
   "source": [
    "output_bucket = 's3://ms-w261-hw05/hw5_3b'\n",
    "\n",
    "!aws s3 rm --recursive {output_bucket}\n",
    "\n",
    "part = 2\n",
    "output = runJob5_3(myfile, part, output_bucket)\n",
    "format_output(output[:10], part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 20 most/least dense words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 most/least dense words\n",
      "              density   word      \n",
      "-------------------------------------------------------\n",
      "             11.55729   xxxx                                              \n",
      "              8.07416   blah                                              \n",
      "              7.53333   nnn                                               \n",
      "              6.20175   na                                                \n",
      "              4.92188   oooooooooooooooo                                  \n",
      "              4.85431   nd                                                \n",
      "              4.51163   llll                                              \n",
      "              4.16965   oooooo                                            \n",
      "              3.85864   ooooo                                             \n",
      "              3.76245   lillelu                                           \n",
      "              3.57692   pfeffermann                                       \n",
      "              3.57692   madarassy                                         \n",
      "              3.56000   meteoritical                                      \n",
      "              3.50000   xxxxxxxx                                          \n",
      "              3.22904   beep                                              \n",
      "              3.18868   latha                                             \n",
      "              2.91912   iyengar                                           \n",
      "              2.82500   counterfeiteth                                    \n",
      "              2.81982   nonmorular                                        \n",
      "              2.81982   nonsquamous                                       \n",
      "\n",
      "\n",
      "Top 20 most/least dense words\n",
      "              density   word      \n",
      "-------------------------------------------------------\n",
      "              1.00000   hosier's                                          \n",
      "              1.00000   hosp                                              \n",
      "              1.00000   hospites                                          \n",
      "              1.00000   hostein                                           \n",
      "              1.00000   hostelrie                                         \n",
      "              1.00000   hostelries                                        \n",
      "              1.00000   hostlers                                          \n",
      "              1.00000   hostus                                            \n",
      "              1.00000   hote                                              \n",
      "              1.00000   hotest                                            \n",
      "              1.00000   hotheaded                                         \n",
      "              1.00000   hothouse                                          \n",
      "              1.00000   hothoused                                         \n",
      "              1.00000   hothouses                                         \n",
      "              1.00000   hotsumi                                           \n",
      "              1.00000   hott                                              \n",
      "              1.00000   hottse                                            \n",
      "              1.00000   houard's                                          \n",
      "              1.00000   houbraken                                         \n",
      "              1.00000   houbraken's                                       \n"
     ]
    }
   ],
   "source": [
    "output_bucket = 's3://ms-w261-hw05/hw5_3c'\n",
    "\n",
    "!aws s3 rm --recursive {output_bucket}\n",
    "\n",
    "part = 3\n",
    "output = runJob5_3(myfile, part, output_bucket)\n",
    "format_output(output[:20], part)\n",
    "print '\\n'\n",
    "format_output(output[-20:], part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogram of 5-gram lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.conf:Got unexpected keyword arguments: ssh_tunnel\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    }
   ],
   "source": [
    "output_bucket = 's3://ms-w261-hw05/hw5_3d'\n",
    "\n",
    "!aws s3 rm --recursive {output_bucket}\n",
    "\n",
    "part = 4\n",
    "output = runJob5_3(myfile, part, output_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA74AAAGJCAYAAACtnu89AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xu4rWVdL/zvD5akCXhqS6+giEKobBGtELVyZQeBEjon\n+r6plVGK0AEzV7WZ9O54NbHSTaYkUpqKorbDHRl55coy85QICiSeOImYB/BMHH7vH+NZNtZkzrkG\nrDnWnPPh87muca3xnH9j3Czm+s77fu6nujsAAAAwVrutdQEAAAAwT4IvAAAAoyb4AgAAMGqCLwAA\nAKMm+AIAADBqgi8AAACjJvgCsC5U1Yer6vvWuo61VFU/XlVXVtWXquoRa13Pzqiqs6vq99bo2p+s\nqiesxbUBWJ8EXwDmbqkgUlVPq6p/2rbc3f+9u9+5g/PsX1W3VtVYf369KMmzunvv7v7Q4o1V9amq\n+toQjL9UVW9bgxrXlbUM2ABsHJvWugAA7tT6du5fwzE1h1pSVbt39y3zOPeM9k9yyQrbO8mPdPc7\ndvZCVVXdfXu/fwDYkMb6G3MANpjpXuGq+u6qel9V3VBV11bV6cNu/zj8ef3Q4/nomvidoTf0M1X1\n51W199R5f27Y9h/DftPXOaWqzq2q11TV9UmeNlz7X6rqi1V1TVX9r6raNHW+W6vqV6rqo0N9v1dV\nD6qqd1XV9VV1zvT+iz7jUrXuVVV7VNWXM/m5fFFVXb7SV7XMuXerqhcPn/PjVfXs6d7xqnpHVf3P\nqvrnqvpqkgOq6ulVdcnwXX6sqn5p6nyPr6qrquq5VXXd8F0cW1VHVdW/V9Xnqur5O2jW6fp+tKo+\nOHyv/1xVD5/a9smq+o2q+tCw/fVVtcfU9t+sqk9X1dVV9QvD53pQVT0zyVOT/ObwGf566pKPXOp8\nVXWfqnrrsP7zVfWPAWD0BF8A1spKvbYvSfLH3X2PJA9O8sZh/bZ7gPcehgO/J8kzkvxckscneVCS\nvZKckSRV9bAkf5LkuCT/V5J7JLnfomsdk+SN3X3PJK9NcnOSX01y7ySPSfKEJM9adMwPJ3lkkiOS\n/GaSVyR5SpL7J3n4cL2lLFXrn3T3f3b3XsN38vDuPmiF7+a1QxB9W1UdOrX+l5I8McmhSR6V5Mdy\n2x71/zvJLw7XvTLJdUmO7u69h9r+qKoOm9r/25Pskcl3dkqSP8skaD4yk7b43araf4VakyRV9cgk\nZyV5Zibf6yuSnFdVd5na7acz+V4PSPKIJE8fjj0yk/Z4QpIDk2ze9rm6+88yabM/GP57OHZH50vy\nG0muSnKfJPdNsmVH9QOw8Qm+AOwq/7uqvrDtlUkgXc5/Jjmwqu7T3V/r7vcu2j4dmp+S5A+7+4ru\n/lqS5yf52aGn8yeTnNfd7+7um5P8jyWu9e7ufmuSdPeN3f3B7n5vT1yZ5MxMguq0F3b3V7v70iQf\nTnLBcP0vJ/nbTILhUpaq9cm1/T3LK/1C4ClJHpjJkOitSf5uqnf7p5O8pLuv7e4bkrxgieP/vLsv\n6+5bu/vm7v7b7v7U8Nn/KckFSb53av//THLaMPz7nCTflskvJL7W3ZdkMix7lkm4npnk5d39/uF7\nfU2SGzP5xcE2L+nu67r7+iRvTbItgP90krOHur+RZGGG6610vpsy+SXIAd19S3e/a8bzAbCBCb4A\n7CrHdve9t71y217Uab+Q5OAkl1XVe6rqR1bY935JrphaviKTOSz2GbZdtW1Dd389yecXHX/V9EJV\nHTQMhb12GP78+5kEvmmfnXr/9Ux6TqeX97wDte7QEOBv7O5vdPcLklyf/wqq233WRe+XXDcMW373\nMOT3i0mOyvaf9fNT9wF/ffhz8Wdf7rNO2z/Jb0z94uOLSfbL9r3v09/h16bOu9TnmuUe7+XO96Ik\nH09ywTC8+3kznAuADU7wBWBXmXlCqu7+eHc/pbv/W5I/SPKmqrpblp4M69OZBKtt9s9kuPJ1Sa7N\nJGBNCpic4z6LL7do+U+TXJrkwcPw59++PbXvwFK13pTtQ9rtMT3R13afNckDltk/STLc8/qmTL7f\n/9bd98qkt3oeE4ddleT3p37xca/u3rO73zDDsUt9ruk2u10TdHX3V7r75O5+cCbD3H+9qr7/9pwD\ngI1H8AVg3amqp1bVtp7HGzIJN7cm+Y/hzwdP7f76JL9WVQ+sqj0z6aE9p7tvzSTYPamqjhjuJ12Y\n4fJ7JflSd3+tqh6S5FdW5UPtuNYVVdX9q+qxVXWXqvqWqnpuJiF+21DdNyY5qaruV1X3zOTe45Xs\nMbw+1923VtVRmdwTOw9/luSXq+rwJKmqu1fV0VV19xmOfWOSZ1TVQ6rqW5P8zqLt12Vyv/RMqupH\nqmrbfz9fzuSXJDv8/gHY2ARfAHaFWXrlpvc5MslHqupLSf4oyc8OQ3y/nklYfNcwZPbwJK9K8pok\n78xkCOvXkpyYJMN9qM9J8oZMelu/lMlQ3RtXqOPkJE8drv2KTO5tXemz3J4ex2VrneFce2XSG/2F\nJFdnElKP7O4vDtv/LJN7dC9K8oEkf5Pk5qlQvd25u/srw7XPHe65fnKS6VmRl3J7Pvs3t3X3BzK5\nz/eM4VofTfK0Wc7T3W9L8tIk7xiOe/ewaVsbnpXkkOG/h7fMUNdBSd4+zKL9rkwmFzOzM8DI1bwf\n4TfMxvjHmYTss7r7hYu2H5zk7ExmoNzS3X8467EAcHsMPYzXJzmwu6/Y0f4b2fAz9E+7+4C1rmU1\nDb3wFyf5lll6ygEgmXOP7zBL5RmZPF7hkCTHDT+wpn0+k9/Gv+gOHAsAKxqeH3u3IfS+OMlFYwy9\nVXXXYbKq3atq30weP/SWHR23EVTVj9XkWcf3SvLCTGbqFnoBmNm8hzofnuTy4bENN2UyXGz6GXvp\n7s8NQ6Buvr3HAsAMjs1kmPPVmdwb/OS1LWduKsmpmQyF/kCSj2QSfsfg+EyGqF+eyWRgK80IDgC3\nsWnO59832z+C4OpMAu28jwWAJEl3PzOT+0tHbbj/eZQ/J7v7qLWuAYCNzeRWAAAAjNq8e3yvyfbP\nEdxvWLeqx1bVfGfoAgAAYM109049Z37ePb7vS3JgVe1fVXtkcl/VeSvsP/1hbtex3e21AV+nnHLK\nmtfgpf3ujC9tt7Ff2m9jv7Tfxn1pu4390n4b97Ua5trj2923VNUJmTxXcNsjiS6tquMnm/vMqton\nyfszeT7hrVV1UpKHdfdXljp2nvUCAAAwPvMe6pyePHj+4EXrXjH1/rok95/1WAAAALg9TG7Fmtq8\nefNal8BO0H4bl7bb2LTfxqb9Ni5tt7Fpvzu3Wq0x02upqnoMnwMAAIDtVVV6nU9uBQAAAGtK8AUA\nAGDUBF8AAABGTfAFAABg1ARfAAAARk3wBQAAYNQEXwAAAEZN8AUAAGDUBF8AAABGTfAFAABg1ARf\nAAAARk3wBQAAYNQEXwAAAEZN8AUAAGDUBF8AAABGTfAFAABg1ARfAAAARm3TWhcAO2vLwunbLZ+2\ncPIaVQIAAKxHenwBAAAYNcEXAACAURN8AQAAGDXBFwAAgFETfAEAABg1wRcAAIBR8zgjNozFjy1K\nPLoIAADYMT2+AAAAjJrgCwAAwKgZ6swoGRYNAABso8cXAACAURN8AQAAGDXBFwAAgFETfAEAABg1\nwRcAAIBRE3wBAAAYNcEXAACAURN8AQAAGDXBFwAAgFETfAEAABg1wRcAAIBRE3wBAAAYNcEXAACA\nURN8AQAAGDXBFwAAgFETfAEAABg1wRcAAIBRE3wBAAAYNcEXAACAURN8AQAAGDXBFwAAgFHbtNYF\nwK60ZeH026w7beHkNagEAADYVQRf1qXFAVU4BQAA7ihDnQEAABg1wRcAAIBRE3wBAAAYNcEXAACA\nURN8AQAAGLW5B9+qOrKqLquqj1bV85bZ56VVdXlVXVhVh02t/7Wq+nBVXVRVr62qPeZdLwAAAOMy\n1+BbVbslOSPJE5MckuS4qnrIon2OSvLg7j4oyfFJXj6sv1+S5yR5VHcfmsmjl548z3oBAAAYn3n3\n+B6e5PLuvqK7b0pyTpJjF+1zbJJXJ0l3vyfJPapqn2Hb7knuXlWbknxrkk/PuV4AAABGZt7Bd98k\nV00tXz2sW2mfa5Ls292fTvLiJFcO667v7rfPsVYAAABGaN1OblVV98ykN3j/JPdLsmdVPWVtqwIA\nAGCj2TTn81+T5AFTy/sN6xbvc/8l9vnBJJ/o7i8kSVW9Jcljk7xuqQstLCx88/3mzZuzefPmnasc\nAACAXW7r1q3ZunXrqp5z3sH3fUkOrKr9k1ybyeRUxy3a57wkz07yhqo6IpMhzddV1ZVJjqiquya5\nMckPDOdb0nTwBQAAYGNa3JF56qmn7vQ55xp8u/uWqjohyQWZDKs+q7svrarjJ5v7zO4+v6qOrqqP\nJflqkmcMx763qt6U5INJbhr+PHOe9QIAADA+8+7xTXe/LcnBi9a9YtHyCcsce2qSnY/3AAAA3Gmt\n28mtAAAAYDUIvgAAAIya4AsAAMCoCb4AAACMmuALAADAqAm+AAAAjJrgCwAAwKgJvgAAAIya4AsA\nAMCoCb4AAACMmuALAADAqAm+AAAAjJrgCwAAwKgJvgAAAIzaprUuANaDLQunb7d82sLJa1QJAACw\n2vT4AgAAMGqCLwAAAKMm+AIAADBqgi8AAACjJvgCAAAwaoIvAAAAoyb4AgAAMGqCLwAAAKMm+AIA\nADBqgi8AAACjJvgCAAAwaoIvAAAAoyb4AgAAMGqCLwAAAKMm+AIAADBqm9a6AO7ctiycfpt1py2c\nvAaVAAAAY6XHFwAAgFETfAEAABg1wRcAAIBRE3wBAAAYNcEXAACAURN8AQAAGDXBFwAAgFETfAEA\nABg1wRcAAIBRE3wBAAAYNcEXAACAURN8AQAAGDXBFwAAgFETfAEAABg1wRcAAIBRE3wBAAAYNcEX\nAACAURN8AQAAGLVNa10ArFdbFk6/zbrTFk5eg0oAAICdscMe36rafVcUAgAAAPMwy1Dny6vqRVX1\nsLlXAwAAAKtsluD7iCQfTfLKqvrXqvqlqtp7znUBAADAqthh8O3uL3f3n3X3Y5M8L8kpSa6tqr+o\nqgPnXiEAAADshJnu8a2qY6rqr5L8cZIXJ3lQkrcmOX/O9QEAAMBOmWVW58uTvCPJi7r7X6bWv6mq\nvm8+ZQEAAMDqmCX4HtrdX1lqQ3efuMr1AAAAwKqaZXKrP6mqe25bqKp7VdWr5lgTAAAArJpZgu+h\n3X39toXu/mKSR856gao6sqouq6qPVtXzltnnpVV1eVVdWFWHTa2/R1WdW1WXVtVHqurRs14XAAAA\nktmC725Vda9tC1V178w2RDpVtVuSM5I8MckhSY6rqocs2ueoJA/u7oOSHJ/k5VObX5Lk/O5+aCaP\nVbp0lusCAADANrME2BcneXdVnZukkvxUkt+f8fyHJ7m8u69Ikqo6J8mxSS6b2ufYJK9Oku5+z9DL\nu0+Sryf53u5++rDt5iRfmvG6AAAAkGSG4Nvdr66qDyT5/mHVT3T3JTOef98kV00tX51JGF5pn2uG\ndbck+VxVnZ1Jb+/7k5zU3V+f8doAAAAw01DnZNJD+5Yk5yX5SlU9YH4lfdOmJI9K8ifd/agkX0vy\nW7vgugAAAIzIDnt8q+o5SU5Jcl0mvbCVpJMcOsP5r0kyHZL3G9Yt3uf+y+xzVXe/f3j/piRLTo6V\nJAsLC998v3nz5mzevHmG8gAAAFhPtm7dmq1bt67qOWe5x/ekJAd39+fvwPnfl+TAqto/ybVJnpzk\nuEX7nJfk2UneUFVHJLm+u69Lkqq6qqq+o7s/muQHkiw7xHo6+AIAALAxLe7IPPXUU3f6nLME36uS\n3HBHTt7dt1TVCUkuyGRY9VndfWlVHT/Z3Gd29/lVdXRVfSzJV5M8Y+oUJyZ5bVXdJcknFm0DAACA\nHZol+H4iydaq+pskN25b2d1/OMsFuvttSQ5etO4Vi5ZPWObYDyX57lmuAwAAAEuZJfheObz2GF4A\nAACwYczyOKNTk6SqvrW7vzb/kgAAAGD17PBxRlX1mKq6JJNHGqWqHlFVL5t7ZQAAALAKZnmO7x8n\neWKSzyffvO/2++ZZFAAAAKyWWYJvuvuqRatumUMtAAAAsOpmepxRVT02SQ+PFTopyaXzLQsAAABW\nxyw9vr+c5NlJ9k1yTZLDhmUAAABY92aZ1flzSZ66C2oBAACAVbfD4FtVZyfpxeu7++fnUhEAAACs\nolnu8f0/U+/vmuTHk3x6PuUAAADA6pplqPObp5er6vVJ/nluFQEAAMAqmulxRosclOS+q10IAAAA\nzMMs9/h+OZN7fGv48zNJnjfnugAAAGBVzDLUea9dUQgAAADMwyw9vo9aaXt3/9vqlQMAAACra5ZZ\nnV+W5FFJLspkuPOhSd6f5BuZDH1+wtyqAwAAgJ00y+RWn07ynd39Xd39nUkemeSa7v7+7hZ6AQAA\nWNdmCb4Hd/fF2xa6+8NJHjq/kgAAAGD1zDLU+aKqemWSvxyWn5rJsGe4U9qycPp2y6ctnLxGlQAA\nALOYJfg+I8mvJDlpWH5nkj+dW0UAAACwimZ5nNE3qurlSc7v7n/fBTUBAADAqpnlcUbHJHlRkj2S\nHFBVhyX5ve4+Zt7FMS6GCAMAAGthlsmtTklyeJLrk6S7L0xywDyLAgAAgNUyS/C9qbtvWLSu51EM\nAAAArLZZJrf6SFU9JcnuVXVQkhOT/Mt8ywIAAIDVMUuP73OSHJLkxiSvS3JDkl+dZ1EAAACwWlbs\n8a2q3TOZyOrkJL+9a0oCAACA1bNij29335Lke3ZRLQAAALDqZrnH94NVdV6Sc5N8ddvK7n7L3KoC\nAACAVTJL8L1rks8necLUuk4i+AIAALDuLRt8q+qF3f28JOd397m7sCYAAABYNSvd43t0VVWS5++q\nYgAAAGC1rTTU+W1Jvphkz6r60tT6StLdvfdcKwMAAIBVsGyPb3c/t7vvmeRvunvvqddeQi8AAAAb\nxYqPM0qS7j52VxQCAAAA87DD4AsAAAAbmeALAADAqM0UfKvqblV18LyLAQAAgNW2w+BbVU9KcmEm\nszynqg6rqvPmXRgAAACshll6fBeSHJ7k+iTp7guTHDDHmgAAAGDVzBJ8b+ruGxat63kUAwAAAKtt\n0wz7fKSqnpJk96o6KMmJSf5lvmUBAADA6pilx/c5SQ5JcmOS1yW5IcmvzrMoAAAAWC2z9Pg+pLt/\nO8lvz7sYAAAAWG2z9Pi+uKourar/t6r++9wrAgAAgFW0w+Db3d+f5PuT/EeSV1TVxVX1O3OvDAAA\nAFbBLD2+6e7PdPdLk/xyJs/0/R9zrQoAAABWyQ6Db1U9tKoWquriJP8rkxmd95t7ZQAAALAKZpnc\n6lVJ3pDkid396TnXAwAAAKtqh8G3ux+zKwoBAACAeVg2+FbVG7v7Z4Yhzj29KUl396Fzrw42iC0L\np99m3WkLJ69BJQAAwGIr9fieNPz5o7uiEAAAAJiHZSe36u5rh7fP6u4rpl9JnrVrygMAAICdM8vj\njH5oiXVHrXYhAAAAMA8r3eP7K5n07D6oqi6a2rRXknfNuzAAAABYDSvd4/u6JH+b5P9L8ltT67/c\n3V+Ya1UAAACwSpYNvt19Q5IbkhyXJFV13yR3TbJnVe3Z3VfumhIBAADgjtvhPb5V9aSqujzJJ5P8\nY5JPZdITDAAAAOveLJNb/c8kRyT5aHcfkOQHkvzrrBeoqiOr6rKq+mhVPW+ZfV5aVZdX1YVVddii\nbbtV1b9V1XmzXhMAAAC2mSX43tTdn0+yW1Xt1t3vSPJds5y8qnZLckaSJyY5JMlxVfWQRfscleTB\n3X1QkuOTvHzRaU5Kcsks1wMAAIDFZgm+11fVnknemeS1VfWSJF+d8fyHJ7l8eP7vTUnOSXLson2O\nTfLqJOnu9yS5R1XtkyRVtV+So5O8csbrAQAAwHZmCb7HJvl6kl9L8rYkH0/ypBnPv2+Sq6aWrx7W\nrbTPNVP7/FGS5ybpGa8HAAAA21npcUZJku6e7t39iznWsp2q+pEk13X3hVW1OUntqmsDAAAwHssG\n36r6crbvaa1huZJ0d+89w/mvSfKAqeX9hnWL97n/Evv8VJJjquroJHdLsldVvbq7f26pCy0sLHzz\n/ebNm7N58+YZygMAAGA92bp1a7Zu3bqq51zpOb57rcL535fkwKraP8m1SZ6c4bnAU85L8uwkb6iq\nI5Jc393XJdkyvFJVj0/yG8uF3mT74AsAAMDGtLgj89RTT93pc+5wqHOSVNX3JDmou8+uqm9Lsld3\nf3JHx3X3LVV1QpILMrmf+KzuvrSqjp9s7jO7+/yqOrqqPpbJpFnPuOMfBwAAALa3w+BbVadk8vii\ng5OcnWSPJH+Z5HGzXKC73zYcO73uFYuWT9jBOf4xyT/Ocj0AAACYNsuszj+e5JgMjzDq7k8nWY1h\n0AAAADB3swTf/+zuzjDRVVXdfb4lAQAAwOqZJfi+sapekeSeVfXMJG9P8sr5lgUAAACrY5bn+J5e\nVT+U5EuZ3Kv7P7r77+deGQAAAKyCmWZ1HoLu3ydJVe1WVU/t7tfOtTIAAABYBcsOda6qvavq+VV1\nRlX9cE2ckOQTSX5m15UIAAAAd9xKPb6vSfLFJO9O8otJtiSpJD/W3RfugtoAAABgp60UfB/U3Q9P\nkqp6ZZJrkzygu7+xSyoDAACAVbDSrM43bXvT3bckuVroBQAAYKNZqcf3EVX1peF9JbnbsFxJurv3\nnnt1AAAAsJOWDb7dvfuuLAQAAADmYaWhzgAAALDhCb4AAACM2kr3+AI7acvC6dstn7Zw8hpVAgAA\nd156fAEAABg1wRcAAIBRE3wBAAAYNcEXAACAURN8AQAAGDWzOrPqzGQMAACsJ3p8AQAAGDXBFwAA\ngFETfAEAABg1wRcAAIBRE3wBAAAYNcEXAACAURN8AQAAGDXBFwAAgFETfAEAABg1wRcAAIBRE3wB\nAAAYNcEXAACAURN8AQAAGDXBFwAAgFETfAEAABg1wRcAAIBRE3wBAAAYNcEXAACAURN8AQAAGLVN\na10A3BltWTh9u+XTFk5eo0oAAGD89PgCAAAwaoIvAAAAoyb4AgAAMGqCLwAAAKMm+AIAADBqgi8A\nAACjJvgCAAAwaoIvAAAAoyb4AgAAMGqCLwAAAKMm+AIAADBqgi8AAACjJvgCAAAwaoIvAAAAoyb4\nAgAAMGqCLwAAAKMm+AIAADBqgi8AAACjNvfgW1VHVtVlVfXRqnreMvu8tKour6oLq+qwYd1+VfUP\nVfWRqrq4qk6cd60AAACMz1yDb1XtluSMJE9MckiS46rqIYv2OSrJg7v7oCTHJ3n5sOnmJL/e3Yck\neUySZy8+FgAAAHZk05zPf3iSy7v7iiSpqnOSHJvksql9jk3y6iTp7vdU1T2qap/u/kySzwzrv1JV\nlybZd9GxMBpbFk6/zbrTFk5eg0oAAGBc5j3Ued8kV00tXz2sW2mfaxbvU1UPTHJYkveseoUAAACM\n2rqf3Kqq9kzypiQndfdX1roeAAAANpZ5D3W+JskDppb3G9Yt3uf+S+1TVZsyCb2v6e6/XulCCwsL\n33y/efPmbN68+Y7WDAAAwBrZunVrtm7duqrnnHfwfV+SA6tq/yTXJnlykuMW7XNekmcneUNVHZHk\n+u6+btj2qiSXdPdLdnSh6eALAADAxrS4I/PUU0/d6XPONfh29y1VdUKSCzIZVn1Wd19aVcdPNveZ\n3X1+VR1dVR9L8tUkT0+SqnpckqcmubiqPpikk2zp7rfNs2YAAADGZd49vhmC6sGL1r1i0fIJSxz3\nriS7z7c6AAAAxm7dT24FAAAAO0PwBQAAYNQEXwAAAEZN8AUAAGDUBF8AAABGbe6zOjNeWxZOv826\n0xZOXoNKAAAAlqfHFwAAgFETfAEAABg1wRcAAIBRE3wBAAAYNZNbwTq3eBIxE4gBAMDto8cXAACA\nURN8AQAAGDXBFwAAgFETfAEAABg1wRcAAIBRE3wBAAAYNcEXAACAURN8AQAAGDXBFwAAgFETfAEA\nABg1wRcAAIBRE3wBAAAYNcEXAACAUdu01gUAt9+WhdNvs+60hZPXoBIAAFj/9PgCAAAwaoIvAAAA\noyb4AgAAMGqCLwAAAKMm+AIAADBqgi8AAACjJvgCAAAwaoIvAAAAoyb4AgAAMGqb1roAYPVsWTh9\nu+XTFk5eo0oAAGD90OMLAADAqAm+AAAAjJrgCwAAwKgJvgAAAIyaya2YiUmTAACAjUqPLwAAAKMm\n+AIAADBqhjrDyC0epp4Yqg4AwJ2LHl8AAABGTfAFAABg1ARfAAAARk3wBQAAYNQEXwAAAEbNrM5w\nJ7V4tmczPQMAMFZ6fAEAABg1wRcAAIBRE3wBAAAYNff4Attx7y8AAGOjxxcAAIBRE3wBAAAYNUOd\n2c7iYa6Joa747wIAgI1Njy8AAACjJvgCAAAwanMf6lxVRyb540xC9lnd/cIl9nlpkqOSfDXJ07v7\nwlmPBdaOGaABANgI5trjW1W7JTkjyROTHJLkuKp6yKJ9jkry4O4+KMnxSV4+67FsfFd86uNrXQI7\nYan227Jw+m1erD9bt25d6xLYCdpvY9N+G5e229i0353bvIc6H57k8u6+ortvSnJOkmMX7XNsklcn\nSXe/J8k9qmqfGY9lg7tS8N3Qbk/7CcPrix/+G5v229i038al7TY27XfnNu+hzvsmuWpq+epMAu2O\n9tl3xmPZCYapsh747xAAgHlbj48zqrUuYGw8ioaNZrn/ZpcKybdnXwAA7pyqu+d38qojkix095HD\n8m8l6elJqqrq5Une0d1vGJYvS/L4JAfs6Nipc8zvQwAAALCmununOkjn3eP7viQHVtX+Sa5N8uQk\nxy3a57wkz07yhiEoX9/d11XV52Y4NsnOfwkAAACM11yDb3ffUlUnJLkg//VIokur6vjJ5j6zu8+v\nqqOr6mOZPM7oGSsdO896AQAAGJ+5DnUGAACAtTbvxxnNVVUdWVWXVdVHq+p5a10PK6uq/arqH6rq\nI1V1cVWhp5zJAAAJKElEQVSdOKy/V1VdUFX/XlV/V1X3WOtaWVpV7VZV/1ZV5w3L2m6DqKp7VNW5\nVXXp8Hfw0dpvY6iqX6uqD1fVRVX12qraQ9utX1V1VlVdV1UXTa1btr2q6vlVdfnwd/OH16Zqtlmm\n/f5gaJ8Lq+rNVbX31Dbtt04s1XZT236jqm6tqntPrdN268hy7VdVzxna6OKqesHU+tvdfhs2+FbV\nbknOSPLEJIckOa6qHrK2VbEDNyf59e4+JMljkjx7aLPfSvL27j44yT8kef4a1sjKTkpyydSytts4\nXpLk/O5+aJJHJLks2m/dq6r7JXlOkkd196GZ3KJ0XLTdenZ2Jv82mbZke1XVw5L8TJKHJjkqycuq\nyrwla2up9rsgySHdfViSy6P91qul2i5VtV+SH0pyxdS6h0bbrTe3ab+q2pzkSUke3t0PT3L6sP4O\ntd+GDb6ZPNP38u6+ortvSnJOkmPXuCZW0N2f6e4Lh/dfSXJpkv0yabe/GHb7iyQ/tjYVspLhB8fR\nSV45tVrbbQBD78T3dvfZSdLdN3f3DdF+G8XuSe5eVZuS3C3JNdF261Z3/3OSLy5avVx7HZPknOHv\n5KcyCVWH74o6WdpS7dfdb+/uW4fFf83k3y6J9ltXlvm7lyR/lOS5i9YdG223rizTfr+S5AXdffOw\nz+eG9Xeo/TZy8N03yVVTy1cP69gAquqBSQ7L5AfIPt19XTIJx0nuu3aVsYJtPzimJwbQdhvDAUk+\nV1VnD0PVz6yqb432W/e6+9NJXpzkykwC7w3d/fZou43mvsu01+J/y1wT/5ZZ734+yfnDe+23zlXV\nMUmu6u6LF23SdhvDdyT5vqr616p6R1V957D+DrXfRg6+bFBVtWeSNyU5aej5XTzDmhnX1pmq+pEk\n1w099isNJdF269OmJI9K8ifd/ahMZtD/rfi7t+5V1T0z+c32/knul0nP71Oj7TY67bUBVdVvJ7mp\nu1+/1rWwY1V1tyRbkpyy1rVwh21Kcq/uPiLJbyY5d2dOtpGD7zVJHjC1vN+wjnVsGKr3piSv6e6/\nHlZfV1X7DNu/Pcln16o+lvW4JMdU1SeSvD7JE6rqNUk+o+02hKsz+Y33+4flN2cShP3dW/9+MMkn\nuvsL3X1Lkr9K8thou41mufa6Jsn9p/bzb5l1qqqensntPk+ZWq391rcHJ3lgkg9V1SczaZ9/q6r7\nRo7YKK5K8pYk6e73Jbmlqu6TO9h+Gzn4vi/JgVW1f1XtkeTJSc5b45rYsVcluaS7XzK17rwkTx/e\nPy3JXy8+iLXV3Vu6+wHd/aBM/q79Q3f/P0neGm237g1DLK+qqu8YVv1Ako/E372N4MokR1TVXYeJ\nO34gkwnmtN36Vtl+dMxy7XVekicPM3UfkOTAJO/dVUWyrO3ar6qOzORWn2O6+8ap/bTf+vPNtuvu\nD3f3t3f3g7r7gEx+CfzI7v5sJm33s9pu3Vn8/87/neQJSTL8G2aP7v587mD7bVr9eneN7r6lqk7I\nZKa93ZKc1d2XrnFZrKCqHpfkqUkurqoPZjLUa0uSFyZ5Y1X9fCYz7v3M2lXJ7fSCaLuN4sQkr62q\nuyT5RJJnZDJpkvZbx7r7vVX1piQfTHLT8OeZSfaKtluXqup1STYnuU9VXZnJMMsXJDl3cXt19yVV\n9cZMfplxU5Jndbdh0GtomfbbkmSPJH8/TBz7r939LO23vizVdtsmdRx0/isUa7t1Zpm/e69KcnZV\nXZzkxiQ/l9zx9ittDAAAwJht5KHOAAAAsEOCLwAAAKMm+AIAADBqgi8AAACjJvgCAAAwaoIvAAAA\noyb4AsCUqvrynM//tKr69qnlT1bVvXfifK+vqgur6qQlrvPZqvq34fXzO1M3AGxkm9a6AABYZ+b9\ngPunJ/lwks/s7PWGAP1d3X3QMruc090n3sFz79bdt97R2gBgPdHjCwA7UFXfVlVvqqr3DK/HDOtP\nqaqzquodVfWxqnrO1DG/W1WXVdU7q+p1VfXrVfWTSb4ryV8OvbB3TVJJTqyqD1TVh6rqO5a4/rdU\n1auq6qJhv8cPm/4uyf2Gcz1uqdKXOFdV1cuq6pKq+ruq+puq+olh2yer6gVV9f4kP1VVv1hV762q\nD1bVuUO9qaqzh3O8e/jcjx++h0uq6lU7920DwOoTfAFgx16S5A+7+9FJfirJWVPbDk7yQ0keneSU\nqtq9qr47yY8neXiSozMJu+nuNyd5f5KndPejuvsbwzk+293fmeTlSZ67xPWfneTW7j40yVOSvLqq\n9khyTJKPD+d61xLH/cQQpt9YVftuW5fkAd39sCQ/l+Qxi475XHd/V3e/Mcmbu/vw7n5kksuS/MLU\nfvfs7sck+fUk5yV58XDOQ6vq0OW+SABYC4IvAOzYDyY5o6o+mEnI27OqvnXY9jfdfXN3fz7JdUn2\nSfLYJH/d3Td191eSvHXR+Rb3xP7V8OcHkuy/xPW/J8lfJkl3/3uSTyW5Tc/wIucleWB3PyLJ25O8\neupc5w7nui7JOxYd94ap9w8feqwvyiRwHzK1bdtnujjJZ7r7kmH5I0keuIPaAGCXco8vAOxYJXl0\nd9+03cqqJLlxatUtuWM/W7edY9bjbzOEebHu/uLU4iuTvHDGWr469f7PkxzT3R+uqqclefzUtm01\n35rtv4Nb498XAKwzenwBYHtLhcoLknxz1uSqesQOjn1XkicN9+bumeRHp/b5cpK9b2dN/5TkqcO1\nvyPJ/ZP8+wr1bpv4aptjk1w6VdtPDvf67pNk8wrX3TPJZ6rqLtuuv4wdBnEAWEt+IwsA27tbVV2Z\nSZjrJH+Y5MQkL6uqDyXZPck7kzxriWM7Sbr7/VV1XpIPZTL8+aIkNwz7/HmSl1fV1zIZEj3LrM4v\nS/Knw5Djm5I8rbtvGnqclzv+xKo6Ztj/C5nMJp0kb07yhEyGJF+VyfDqbbUtPtfvJnlvks8meU+S\nvZbZr5d5DwDrQnX7+QQAq62q7t7dX62qu2USlJ/Z3ReudV3JdrXdO5NA+7ju/uxa1wUA86LHFwDm\n48yqeliSb0ny5+sl9A7+T1XdM8ldkvye0AvA2OnxBQAAYNRMbgUAAMCoCb4AAACMmuALAADAqAm+\nAAAAjJrgCwAAwKgJvgAAAIza/w84mfUOOhu1IAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcbd3ad7550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_hist(output):\n",
    "    frequencies = np.array(output)\n",
    "    density = np.column_stack((frequencies[:,1], frequencies[:,0] / sum(frequencies[:,0])))\n",
    "\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    plt.bar(density[:,0], density[:,1], color='#797f8b', edgecolor=\"none\")\n",
    "    plt.xlabel(\"Length of 5gram\")\n",
    "    plt.ylabel(\"Relative frequency\")\n",
    "    plt.title(\"Histogram of 5gram lengths\")\n",
    "    \n",
    "plot_hist(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW5.4: Synonym detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build stripes of word co-occurrence for the 9,000 words ranked 1,001 - 10,000\n",
    "\n",
    "#### First we need to extract our basis using data from a previous job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://ms-w261-hw05/hw5_3b/part-00000 to ./topWords.txt\r\n"
     ]
    }
   ],
   "source": [
    "# Copy results from S3 so we can extract our basis\n",
    "!aws s3 cp s3://ms-w261-hw05/hw5_3b/part-00000 topWords.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "basis = []\n",
    "\n",
    "with open('topWords.txt','r') as myfile:\n",
    "    count = 0\n",
    "    for line in myfile:\n",
    "        if count > 9000 and count <= 10000:\n",
    "            fields = line.strip().split('\\t')\n",
    "            word = eval(fields[1])\n",
    "            basis.append(word)\n",
    "        count += 1\n",
    "\n",
    "with open('basisWords.txt','w') as myfile:\n",
    "    for word in basis:\n",
    "        myfile.write(word+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We also need to create our unit testing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = [['A', {'X':20, 'Y':30, 'Z':5}],\n",
    "          ['B', {'X':100, 'Y':20}],\n",
    "          ['C', {'M':5, 'N':20, 'Z':5}]]\n",
    "\n",
    "with open('outputUnit.txt', 'w') as myfile:\n",
    "    for out in output:\n",
    "        myfile.write(str(out[0])+'\\t'+str(out[1])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting unitBasis.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile unitBasis.txt\n",
    "A\n",
    "B\n",
    "C\n",
    "M\n",
    "N\n",
    "X\n",
    "Y\n",
    "Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create co-occurrence matrix (only top right of matrix, matrix will be symmetric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MRJob5_4_1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MRJob5_4_1.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class stripes(MRJob):\n",
    "    \n",
    "    \"\"\"\n",
    "    Build stripes\n",
    "    - Read in basis words from basisWords.txt\n",
    "    - For each 5-gram:\n",
    "       - Deduplicate the words in the 5-gram, then sort alphabetically\n",
    "       - Extract count\n",
    "       - Build stripe: (word1, {word2: x, word3: y, ...})\n",
    "       \n",
    "    \"\"\"\n",
    "    \n",
    "    def mapper_buildStripe_init(self):\n",
    "        self.numDocuments = 0\n",
    "        self.vocab = set()\n",
    "        with open('basisWords.txt','r') as myfile:\n",
    "            for word in myfile:\n",
    "                self.vocab.add(word.strip())\n",
    "        \n",
    "    def mapper_buildStripe(self, _, line):\n",
    "        fields = line.strip().split('\\t')\n",
    "        words = fields[0].lower().split()\n",
    "        wordList = sorted(list(set(words)))\n",
    "        count = float(fields[1])\n",
    "        self.numDocuments += count\n",
    "        for index1 in range(len(wordList)-1):\n",
    "            stripe = {}\n",
    "            if wordList[index1] in self.vocab:\n",
    "                for index2 in range(index1+1,len(wordList)):\n",
    "                    if wordList[index2] in self.vocab:\n",
    "                        stripe[wordList[index2]] = count\n",
    "            if len(stripe) > 0:\n",
    "                yield wordList[index1], stripe\n",
    "                \n",
    "    def mapper_buildStripe_final(self):\n",
    "        yield '*total', {'*total':self.numDocuments}\n",
    "            \n",
    "    def combiner_buildStripe(self, key, values):\n",
    "        stripe = {}\n",
    "        for val in values:\n",
    "            for word in val:\n",
    "                if word in stripe:\n",
    "                    stripe[word] += val[word]\n",
    "                else:\n",
    "                    stripe[word] = val[word]\n",
    "        yield key, stripe\n",
    "        \n",
    "    def reducer_buildStripe_init(self):\n",
    "        self.numDocs = 0\n",
    "    \n",
    "    def reducer_buildStripe(self, key, values):\n",
    "        stripe = {}\n",
    "        for val in values:\n",
    "            for word in val:\n",
    "                if word in stripe:\n",
    "                    stripe[word] += val[word]\n",
    "                else:\n",
    "                    stripe[word] = val[word]\n",
    "        if key == '*total':\n",
    "            self.numDocs = stripe['*total']\n",
    "        else:\n",
    "            for word in stripe:\n",
    "                stripe[word] /= self.numDocs\n",
    "            yield key, stripe\n",
    "    \n",
    "            \n",
    "        \n",
    "    \"\"\"\n",
    "    Multi-step pipeline definitions\n",
    "    Based on user input when calling runner function\n",
    "    \"\"\"\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init=self.mapper_buildStripe_init,\n",
    "                   mapper=self.mapper_buildStripe,\n",
    "                   mapper_final=self.mapper_buildStripe_final,\n",
    "                   combiner=self.combiner_buildStripe,\n",
    "                   reducer_init=self.reducer_buildStripe_init,\n",
    "                   reducer=self.reducer_buildStripe,\n",
    "                   jobconf={'mapred.reduce.tasks': 1})\n",
    "        ]\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    stripes.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from MRJob5_4_1 import stripes\n",
    "\n",
    "def runJob5_4_1(filename, s3bucket, basis):\n",
    "\n",
    "    #mr_job = stripes(args=[filename, '--file', basis])\n",
    "    #mr_job = stripes(args=[filename, '-r', 'hadoop', '--hadoop-home', '/usr/', '--file', basis])\n",
    "    mr_job = stripes(args=[filename, '-r', 'emr', '--file', basis, '--no-output', '--output-dir', s3bucket,\n",
    "                           '--emr-job-flow-id', clusterId])\n",
    "    \n",
    "    output = []\n",
    "    \n",
    "    with mr_job.make_runner() as runner: \n",
    "        # Run MRJob\n",
    "        runner.run()\n",
    "\n",
    "        # Write stream_output to file\n",
    "        for line in runner.stream_output():\n",
    "            out = mr_job.parse_output_line(line)\n",
    "            output.append(out)\n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.compat:Detected hadoop configuration property names that do not match hadoop version 2.6.0:\n",
      "The have been translated as follows\n",
      " mapred.reduce.tasks: mapreduce.job.reduces\n"
     ]
    }
   ],
   "source": [
    "# Test our matrix creation on a small subset of data\n",
    "output = runJob5_4_1('./filtered-5Grams/googlebooks-eng-all-5gram-20090715-0-filtered.txt', 'none', 'basisWords.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('alternate', {'viewing': 1.6005761449506742e-06})\n",
      "(\"alzheimer's\", {'dementia': 3.532979051659415e-06})\n",
      "('amidst', {'tumult': 1.5615377023909016e-06, 'restless': 8.393265150351096e-07})\n",
      "('ammonium', {'hydroxide': 1.522499259831129e-06})\n",
      "('anemia', {'pernicious': 1.1321148342334037e-06})\n",
      "('annum', {'thereon': 1.3468262683121526e-06})\n",
      "('approximated', {'subcutaneous': 2.3423065535863526e-06})\n",
      "('architectural', {'decoration': 8.978841788747684e-07})\n",
      "('articular', {'cartilage': 9.954802852741998e-07})\n",
      "('authoritative', {'interpreter': 9.759610639943135e-07})\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print output[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run on EMR on full data set -- this will get us the top right of the co-occurrence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.conf:Got unexpected keyword arguments: ssh_tunnel\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    }
   ],
   "source": [
    "myfile = 's3://filtered-5grams/'\n",
    "mybasis = 'basisWords.txt'\n",
    "output_bucket = 's3://ms-w261-hw05/hw5_4_1_1000basis'\n",
    "\n",
    "!aws s3 rm --recursive {output_bucket}\n",
    "\n",
    "output = runJob5_4_1(myfile, output_bucket, mybasis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('abnormalities', {'mitral': 7.064920460541209e-08, 'lymphocytes': 5.853791238734144e-09, 'plexus': 1.1202945301715345e-08, 'lobes': 1.2515001958672997e-08, 'esophagus': 2.9672665934273075e-08, 'dysfunction': 4.541734581776491e-09, \"alzheimer's\": 6.762138155089442e-09, 'screening': 5.450081498131789e-09, 'babies': 1.2615929393823586e-08, 'congestive': 1.3524276310178884e-08, 'resembling': 6.257500979336498e-09, 'cleft': 2.1800325992527156e-08, 'uterine': 8.578831987800039e-09, 'indicators': 6.358428414487087e-09, 'cage': 1.3019639134425941e-08, 'placenta': 6.863065590240031e-09, 'correlate': 1.4129840921082417e-08, 'skeletal': 4.703218478017433e-08, 'diabetic': 2.775504466641189e-08, 'schizophrenia': 1.998363215981656e-08, 'unstable': 6.358428414487087e-09, 'cartilage': 9.285324033854159e-09, 'ovary': 1.2716856828974174e-08, 'pathological': 7.67048507144474e-09})\n"
     ]
    }
   ],
   "source": [
    "# Check the first record\n",
    "print output[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using two (symmetric) comparison methods of your choice, pairwise compare all stripes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MRJob5_4_2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MRJob5_4_2.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import math\n",
    "from itertools import combinations, product, chain\n",
    "\n",
    "class similarity(MRJob):\n",
    "    \n",
    "    \"\"\"\n",
    "    Specify some custom options so we only have to write one MRJob class for each part\n",
    "    \"\"\"\n",
    "    def configure_options(self):\n",
    "        super(similarity, self).configure_options()\n",
    "        self.add_passthrough_option('--method', default='jaccard')\n",
    "    \n",
    "    \"\"\"\n",
    "    Build the full co-occurrence matrix\n",
    "    - For each partial stripe, emit the stripe and the inverse stripe\n",
    "    - Example: (dog, {cat:2, bird:3})\n",
    "       - Emit original (dog, {cat:2, bird:3})\n",
    "       - Emit inverse (cat, {dog:2}), (bird, {dog:3})\n",
    "    \"\"\"\n",
    "    \n",
    "    def mapper_buildFullMatrix(self, _, line):\n",
    "        fields = line.strip().split('\\t')\n",
    "        word1 = eval(fields[0])\n",
    "        stripe = eval(fields[1])\n",
    "        yield word1, stripe\n",
    "        for word2 in stripe:\n",
    "            yield word2, {word1: stripe[word2]}\n",
    "        \n",
    "    def reducer_buildFullMatrix(self, key, values):\n",
    "        stripe = {}\n",
    "        for val in values:\n",
    "            for word in val:\n",
    "                if word in stripe:\n",
    "                    stripe[word] += val[word]\n",
    "                else:\n",
    "                    stripe[word] = val[word]\n",
    "        yield key, stripe\n",
    "    \n",
    "    \"\"\"\n",
    "    Jaccard similarity\n",
    "    - We need to only create the pairs we know we want to emit\n",
    "    - We only emit anything if one item in the pair has a value\n",
    "    - We binarize on whether the co-occurrence support is in the list of values\n",
    "    - There will be no values of zero in the list of values\n",
    "    - bin1 = true/false if word1 is > 0 (if word1 is in set, bin1=1)\n",
    "    - Emit (word1, word2), (bin1 and bin2, bin1, bin2)\n",
    "    \"\"\"\n",
    "    \n",
    "    def mapper_Jaccard_init(self):\n",
    "        self.vocab = set()\n",
    "        with open('basisWords.txt','r') as myfile:\n",
    "            for word in myfile:\n",
    "                self.vocab.add(word.strip())\n",
    "        \n",
    "    def mapper_Jaccard(self, key, value):\n",
    "        valueSet = set(value.keys())\n",
    "        allPairs = chain(product(valueSet, self.vocab.difference(valueSet)),\n",
    "                         combinations(self.vocab.intersection(valueSet), 2))\n",
    "\n",
    "        for pair in allPairs:\n",
    "            sortedPair = sorted(pair)\n",
    "            if sortedPair[0] in value or sortedPair[1] in value:\n",
    "                i = sortedPair[0] in value\n",
    "                j = sortedPair[1] in value\n",
    "                yield (sortedPair[0], sortedPair[1]), ((i and j)+0, i+0, j+0)\n",
    "                \n",
    "    def combiner_Jaccard(self, key, values):\n",
    "        intersect = 0\n",
    "        i = 0\n",
    "        j = 0\n",
    "        for val in values:\n",
    "            intersect += val[0]\n",
    "            i += val[1]\n",
    "            j += val[2]\n",
    "        yield key, (intersect, i, j)\n",
    "        \n",
    "    def reducer_Jaccard(self, key, values):\n",
    "        intersect = 0.0\n",
    "        i = 0.0\n",
    "        j = 0.0\n",
    "        for val in values:\n",
    "            intersect += val[0]\n",
    "            i += val[1]\n",
    "            j += val[2]\n",
    "        yield key, intersect / (i + j - intersect)       \n",
    "    \n",
    "    \"\"\"\n",
    "    Cosine similarity\n",
    "    - Similar to Jaccard, but do not binarize\n",
    "    - We can use the same mapper_init as Jaccard\n",
    "    - For each pair, we have word1 and word2\n",
    "    - count1 is the matrix entry for word1\n",
    "    - In the mapper, emit (word1, word2), (count1*count2, count1^2, count2^2)\n",
    "    - We can use the same combiner as Jaccard\n",
    "    \"\"\"\n",
    "    \n",
    "    def mapper_Cosine(self, key, value):\n",
    "        valueSet = set(value.keys())\n",
    "        allPairs = chain(product(valueSet, self.vocab.difference(valueSet)),\n",
    "                         combinations(self.vocab.intersection(valueSet), 2))\n",
    "\n",
    "        for pair in allPairs:\n",
    "            sortedPair = sorted(pair)\n",
    "            if sortedPair[0] in value or sortedPair[1] in value:\n",
    "                if sortedPair[0] in value:\n",
    "                    i = value[sortedPair[0]]\n",
    "                else: i = 0.0\n",
    "                if sortedPair[1] in value:\n",
    "                    j = value[sortedPair[1]]\n",
    "                else: j = 0.0\n",
    "                yield (sortedPair[0], sortedPair[1]), (i*j, i**2, j**2)\n",
    "                    \n",
    "    def reducer_Cosine(self, key, values):\n",
    "        dotprod = 0.0\n",
    "        i = 0.0\n",
    "        j = 0.0\n",
    "        for val in values:\n",
    "            dotprod += val[0]\n",
    "            i += val[1]\n",
    "            j += val[2]\n",
    "        if i == 0 or j == 0:\n",
    "            yield key, None\n",
    "        else:\n",
    "            yield key, dotprod / ((i ** 0.5) * (j ** 0.5))\n",
    "    \n",
    "    \"\"\"\n",
    "    Multi-step pipeline definitions\n",
    "    Based on user input when calling runner function\n",
    "    \"\"\"\n",
    "    def steps(self):\n",
    "        self.method = self.options.method\n",
    "        if self.method == 'jaccard':\n",
    "            return [\n",
    "                MRStep(mapper=self.mapper_buildFullMatrix,\n",
    "                       combiner=self.reducer_buildFullMatrix,\n",
    "                       reducer=self.reducer_buildFullMatrix),\n",
    "                MRStep(mapper_init=self.mapper_Jaccard_init,\n",
    "                       mapper=self.mapper_Jaccard,\n",
    "                       combiner=self.combiner_Jaccard,\n",
    "                       reducer=self.reducer_Jaccard)\n",
    "            ]\n",
    "        elif self.method == 'cosine':\n",
    "            return [\n",
    "                MRStep(mapper=self.mapper_buildFullMatrix,\n",
    "                       combiner=self.reducer_buildFullMatrix,\n",
    "                       reducer=self.reducer_buildFullMatrix),\n",
    "                MRStep(mapper_init=self.mapper_Jaccard_init,\n",
    "                       mapper=self.mapper_Cosine,\n",
    "                       combiner=self.combiner_Jaccard,\n",
    "                       reducer=self.reducer_Cosine)\n",
    "            ]\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    similarity.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from MRJob5_4_2 import similarity\n",
    "\n",
    "def runJob5_4_2(filename, s3bucket, basis, method):\n",
    "\n",
    "    \"\"\"\n",
    "    Here are some switches for running the job locally, in local Hadoop or on EMR\n",
    "    \"\"\"\n",
    "    #mr_job = similarity(args=[filename, '--file', basis, '--method', method])\n",
    "    #mr_job = similarity(args=[filename, '-r', 'hadoop', '--hadoop-home', '/usr/', '--file', basis, '--method', method])\n",
    "    mr_job = similarity(args=[filename, '-r', 'emr', '--file', basis, '--no-output', '--output-dir', s3bucket,\n",
    "                              '--emr-job-flow-id', clusterId, '--method', method])\n",
    "    \n",
    "    output = []\n",
    "#     outfile = open('hw5_4_2_output.txt','w')\n",
    "    \n",
    "    with mr_job.make_runner() as runner: \n",
    "        # Run MRJob\n",
    "        runner.run()\n",
    "\n",
    "        # Write stream_output to file\n",
    "        for line in runner.stream_output():\n",
    "            out = mr_job.parse_output_line(line)\n",
    "#             output.append(out)\n",
    "#             outfile.write(str(out)+'\\n')\n",
    "    \n",
    "#     outfile.close()\n",
    "            \n",
    "#     return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    }
   ],
   "source": [
    "myfile = 'outputUnit.txt'\n",
    "mybasis = 'unitBasis.txt'\n",
    "\n",
    "runJob5_4_2(myfile, 'none', mybasis, 'jaccard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['A', 'B'], 0.6666666666666666)\r\n",
      "(['A', 'C'], 0.2)\r\n",
      "(['A', 'M'], 0.0)\r\n",
      "(['A', 'N'], 0.0)\r\n",
      "(['A', 'X'], 0.0)\r\n",
      "(['A', 'Y'], 0.0)\r\n",
      "(['A', 'Z'], 0.0)\r\n",
      "(['B', 'C'], 0.0)\r\n",
      "(['B', 'M'], 0.0)\r\n",
      "(['B', 'N'], 0.0)\r\n"
     ]
    }
   ],
   "source": [
    "!head hw5_4_2_output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Check some hand calculations to be sure\n",
    "\n",
    "Full co-occurrence matrix:\n",
    "\n",
    " |A|B|C|M|N|X|Y|Z\n",
    "-|-|-|-|-|-|-|-|-\n",
    "**A**|0|0|0|0|0|20|30|5\n",
    "**B**|0|0|0|0|0|100|20|0\n",
    "**C**|0|0|0|5|20|0|0|0\n",
    "**M**|0|0|5|0|0|0|0|0\n",
    "**N**|0|0|20|0|0|0|0|0\n",
    "**X**|20|100|0|0|0|0|0|0\n",
    "**Y**|30|20|0|0|0|0|0|0\n",
    "**Z**|5|0|5|0|0|0|0|0\n",
    "\n",
    "Let's check the Jaccard similarity between A and B:\n",
    "\n",
    "$\\frac{|A \\cap B|}{|A|+|B|-|A \\cap B|} = \\frac{2}{3+2-2}=\\frac{2}{3}$\n",
    "\n",
    "Let's check the Jaccard similarity between A and C:\n",
    "\n",
    "$\\frac{|A \\cap C|}{|A|+|C|-|A \\cap C|} = \\frac{1}{3+3-1}=\\frac{1}{5}$\n",
    "\n",
    "Both of these calculations check out!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    }
   ],
   "source": [
    "myfile = 'outputUnit.txt'\n",
    "mybasis = 'unitBasis.txt'\n",
    "\n",
    "runJob5_4_2(myfile, 'none', mybasis, 'cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['A', 'B'], 0.7004041959724749)\r\n",
      "(['A', 'C'], 0.0323761954119088)\r\n",
      "(['A', 'M'], 0.0)\r\n",
      "(['A', 'N'], 0.0)\r\n",
      "(['A', 'X'], 0.0)\r\n",
      "(['A', 'Y'], 0.0)\r\n",
      "(['A', 'Z'], 0.0)\r\n",
      "(['B', 'C'], 0.0)\r\n",
      "(['B', 'M'], 0.0)\r\n",
      "(['B', 'N'], 0.0)\r\n"
     ]
    }
   ],
   "source": [
    "!head hw5_4_2_output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run on the whole dataset in EMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://ms-w261-hw05/hw5_4_2jaccard/_SUCCESS\n",
      "delete: s3://ms-w261-hw05/hw5_4_2jaccard/part-00001\n",
      "delete: s3://ms-w261-hw05/hw5_4_2jaccard/part-00002\n",
      "delete: s3://ms-w261-hw05/hw5_4_2jaccard/part-00000\n",
      "delete: s3://ms-w261-hw05/hw5_4_2jaccard/part-00004\n",
      "delete: s3://ms-w261-hw05/hw5_4_2jaccard/part-00003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.conf:Got unexpected keyword arguments: ssh_tunnel\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    }
   ],
   "source": [
    "myfile = 's3://ms-w261-hw05/hw5_4_1_1000basis/part-00000'\n",
    "mybasis = 'basisWords.txt'\n",
    "output_bucket = 's3://ms-w261-hw05/hw5_4_2jaccard'\n",
    "\n",
    "!aws s3 rm --recursive {output_bucket}\n",
    "\n",
    "runJob5_4_2(myfile, output_bucket, mybasis, 'jaccard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://ms-w261-hw05/hw5_4_2cosine/_SUCCESS\n",
      "delete: s3://ms-w261-hw05/hw5_4_2cosine/part-00003\n",
      "delete: s3://ms-w261-hw05/hw5_4_2cosine/part-00000\n",
      "delete: s3://ms-w261-hw05/hw5_4_2cosine/part-00001\n",
      "delete: s3://ms-w261-hw05/hw5_4_2cosine/part-00004\n",
      "delete: s3://ms-w261-hw05/hw5_4_2cosine/part-00002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.conf:Got unexpected keyword arguments: ssh_tunnel\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    }
   ],
   "source": [
    "myfile = 's3://ms-w261-hw05/hw5_4_1_1000basis/part-00000'\n",
    "mybasis = 'basisWords.txt'\n",
    "output_bucket = 's3://ms-w261-hw05/hw5_4_2cosine'\n",
    "\n",
    "!aws s3 rm --recursive {output_bucket}\n",
    "\n",
    "runJob5_4_2(myfile, output_bucket, mybasis, 'cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using configs in /etc/mrjob.conf\n",
      "using existing scratch bucket mrjob-ac40f1afcc0b86ce\n",
      "using s3://mrjob-ac40f1afcc0b86ce/tmp/ as our scratch dir on S3\n",
      "Terminated job flow j-2GY8DK2A59MD0\n"
     ]
    }
   ],
   "source": [
    "# Terminate job flow so we don't rack up AWS expenses\n",
    "!python -m mrjob.tools.emr.terminate_job_flow {clusterId}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW5.5\n",
    "In this part of the assignment you will evaluate the success of you synonym detector.\n",
    "Take the top 1,000 closest/most similar/correlative pairs of words as determined by your measure in (2), and use the synonyms function in the accompanying python code (nltk_synonyms.py).\n",
    "\n",
    "For each (word1,word2) pair, check to see if word1 is in the list, synonyms(word2), and vice-versa. If one of the two is a synonym of the other, then consider this pair a 'hit', and then report the precision, recall, and F1 measure  of your detector across your 1,000 best guesses. Report the macro averages of these measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# From nltk_synonyms.py\n",
    "from nltk.corpus import wordnet as wn\n",
    "import sys\n",
    "#print all the synset element of an element\n",
    "def synonyms(string):\n",
    "    syndict = {}\n",
    "    for i,j in enumerate(wn.synsets(string)):\n",
    "        syns = j.lemma_names()\n",
    "        for syn in syns:\n",
    "            syndict.setdefault(syn,1)\n",
    "    return syndict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://ms-w261-hw05/hw5_4_2jaccard/_SUCCESS to pairs_jaccard/_SUCCESS\n",
      "download: s3://ms-w261-hw05/hw5_4_2jaccard/part-00000 to pairs_jaccard/part-00000\n",
      "download: s3://ms-w261-hw05/hw5_4_2jaccard/part-00003 to pairs_jaccard/part-00003\n",
      "download: s3://ms-w261-hw05/hw5_4_2jaccard/part-00004 to pairs_jaccard/part-00004\n",
      "download: s3://ms-w261-hw05/hw5_4_2jaccard/part-00001 to pairs_jaccard/part-00001\n",
      "download: s3://ms-w261-hw05/hw5_4_2jaccard/part-00002 to pairs_jaccard/part-00002\n"
     ]
    }
   ],
   "source": [
    "# Let's download the data, since we can now work locally with only 1000 term pairs\n",
    "!aws s3 cp --recursive s3://ms-w261-hw05/hw5_4_2jaccard/ ./pairs_jaccard/\n",
    "!cat ./pairs_jaccard/part-* >> jaccard.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"abnormalities\", \"abyss\"]\t0.0\r\n",
      "[\"abnormalities\", \"actuated\"]\t0.0\r\n",
      "[\"abnormalities\", \"acutely\"]\t0.029411764705882353\r\n",
      "[\"abnormalities\", \"adopting\"]\t0.0\r\n",
      "[\"abnormalities\", \"advertisement\"]\t0.0\r\n",
      "[\"abnormalities\", \"affiliated\"]\t0.0\r\n",
      "[\"abnormalities\", \"amazement\"]\t0.0\r\n",
      "[\"abnormalities\", \"analysed\"]\t0.0\r\n",
      "[\"abnormalities\", \"antiquities\"]\t0.0\r\n",
      "[\"abnormalities\", \"armistice\"]\t0.0\r\n"
     ]
    }
   ],
   "source": [
    "!head jaccard.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://ms-w261-hw05/hw5_4_2cosine/_SUCCESS to pairs_cosine/_SUCCESS\n",
      "download: s3://ms-w261-hw05/hw5_4_2cosine/part-00003 to pairs_cosine/part-00003\n",
      "download: s3://ms-w261-hw05/hw5_4_2cosine/part-00002 to pairs_cosine/part-00002\n",
      "download: s3://ms-w261-hw05/hw5_4_2cosine/part-00001 to pairs_cosine/part-00001\n",
      "download: s3://ms-w261-hw05/hw5_4_2cosine/part-00004 to pairs_cosine/part-00004\n",
      "download: s3://ms-w261-hw05/hw5_4_2cosine/part-00000 to pairs_cosine/part-00000\n"
     ]
    }
   ],
   "source": [
    "# Let's download the data, since we can now work locally with only 1000 term pairs\n",
    "!aws s3 cp --recursive s3://ms-w261-hw05/hw5_4_2cosine/ ./pairs_cosine/\n",
    "!cat ./pairs_cosine/part-* >> cosine.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"abnormalities\", \"abyss\"]\t0.0\r\n",
      "[\"abnormalities\", \"actuated\"]\t0.0\r\n",
      "[\"abnormalities\", \"acutely\"]\t0.070793526557697245\r\n",
      "[\"abnormalities\", \"adopting\"]\t0.0\r\n",
      "[\"abnormalities\", \"advertisement\"]\t0.0\r\n",
      "[\"abnormalities\", \"affiliated\"]\t0.0\r\n",
      "[\"abnormalities\", \"amazement\"]\t0.0\r\n",
      "[\"abnormalities\", \"analysed\"]\t0.0\r\n",
      "[\"abnormalities\", \"antiquities\"]\t0.0\r\n",
      "[\"abnormalities\", \"armistice\"]\t0.0\r\n"
     ]
    }
   ],
   "source": [
    "!head cosine.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499497\n"
     ]
    }
   ],
   "source": [
    "jaccard1000 = []\n",
    "count = 0\n",
    "with open('jaccard.txt','r') as myfile:\n",
    "    for line in myfile:\n",
    "        count += 1\n",
    "        fields = line.strip().split('\\t')\n",
    "        pair = eval(fields[0])\n",
    "        sim = float(fields[1])\n",
    "        jaccard1000.append([pair,sim])\n",
    "        \n",
    "jaccard1000 = sorted(jaccard1000, key=lambda x: x[1], reverse=True)\n",
    "jaccard1000 = jaccard1000[:1000]\n",
    "\n",
    "print count\n",
    "\n",
    "with open('top1000Jaccard.txt','w') as myfile:\n",
    "    for item in jaccard1000:\n",
    "        myfile.write(str(item)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499497\n"
     ]
    }
   ],
   "source": [
    "cosine1000 = []\n",
    "count = 0\n",
    "with open('cosine.txt','r') as myfile:\n",
    "    for line in myfile:\n",
    "        count += 1\n",
    "        fields = line.strip().split('\\t')\n",
    "        pair = eval(fields[0])\n",
    "        sim = fields[1]\n",
    "        if sim != 'null':\n",
    "            cosine1000.append([pair,float(sim)])\n",
    "        \n",
    "cosine1000 = sorted(cosine1000, key=lambda x: x[1], reverse=True)\n",
    "cosine1000 = cosine1000[:1000]\n",
    "\n",
    "print count\n",
    "\n",
    "with open('top1000Cosine.txt','w') as myfile:\n",
    "    for item in cosine1000:\n",
    "        myfile.write(str(item)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def summary_stats(jaccard1000):\n",
    "    synList = {}\n",
    "    vocab = set()\n",
    "\n",
    "    for item in jaccard1000:\n",
    "        word1 = item[0][0]\n",
    "        word2 = item[0][1]\n",
    "        if word1 not in synList:\n",
    "            synList[word1] = [word2]\n",
    "        else:\n",
    "            synList[word1].append(word2)\n",
    "        if word2 not in synList:\n",
    "            synList[word2] = [word1]\n",
    "        else:\n",
    "            synList[word2].append(word1)\n",
    "\n",
    "    with open('basisWords.txt','r') as myfile:\n",
    "        for word in myfile:\n",
    "            vocab.add(word.strip())\n",
    "\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "\n",
    "    print 'Words where there are >0 true positives:\\n'\n",
    "    print '{:<15s}{:>10s}{:>10s}{:>10s}{:>10s}{:>10s}{:>10s}{:>10s}'.format('word', \n",
    "                                                                            'truePos', \n",
    "                                                                            'falsePos', \n",
    "                                                                            'falseNeg', \n",
    "                                                                            'trueNeg',\n",
    "                                                                            'precision', \n",
    "                                                                            'recall', \n",
    "                                                                            'f1')  \n",
    "    print '-------------------------------------------------------------------------------------'\n",
    "\n",
    "    for i in synList:\n",
    "        trueSyn = synonyms(i)\n",
    "        truePos = len(set(trueSyn).intersection(synList[i]))\n",
    "        falsePos = len(set(synList[i]).difference(trueSyn))\n",
    "        falseNeg = len(set(trueSyn).difference(synList[i]))\n",
    "        trueNeg = len(vocab.difference(trueSyn, synList[i]))\n",
    "\n",
    "        try:\n",
    "            precision = truePos / (truePos + falsePos)\n",
    "            precisions.append(precision)\n",
    "            precisionStr = '{:6.3f}'.format(precision)\n",
    "        except:\n",
    "            precisionStr = None\n",
    "\n",
    "        try:\n",
    "            recall = truePos / (truePos + falseNeg)\n",
    "            recalls.append(recall)\n",
    "            recallStr = '{:6.3f}'.format(recall)\n",
    "        except: \n",
    "            recallStr = None\n",
    "\n",
    "        try:\n",
    "            f1 = 2 * precision * recall / (precision + recall)\n",
    "            f1Str = '{:6.3f}'.format(f1)\n",
    "        except:\n",
    "            f1Str = None\n",
    "\n",
    "        if truePos != 0:\n",
    "            print '{:<15s}{:10,d}{:10,d}{:10,d}{:10,d}{:>10s}{:>10s}{:>10s}'.format(i,\n",
    "                                                                                    truePos,\n",
    "                                                                                    falsePos, \n",
    "                                                                                    falseNeg,\n",
    "                                                                                    trueNeg,\n",
    "                                                                                    precisionStr,\n",
    "                                                                                    recallStr, \n",
    "                                                                                    f1Str)  \n",
    "\n",
    "    recallMacro = sum(recalls) / len(recalls)\n",
    "    precisionMacro = sum(precisions) / len(precisions)\n",
    "\n",
    "    print '\\n\\nSummary stats'\n",
    "    print '--------------------------'\n",
    "    print '{:<17s} {:2.4f}'.format('Precision:', precisionMacro)\n",
    "    print '{:<17s} {:2.4f}'.format('Recall:', recallMacro)\n",
    "    if precisionMacro + recallMacro != 0:\n",
    "        f1Macro = 2 * 2 * precisionMacro * recallMacro / (precisionMacro + recallMacro)\n",
    "        print '{:<17s} {:2.4f}'.format('F1:', f1)\n",
    "    else:\n",
    "        f1 = None\n",
    "        print '{:<17s} {:7s}'.format('F1:', f1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words where there are >0 true positives:\n",
      "\n",
      "word              truePos  falsePos  falseNeg   trueNeg precision    recall        f1\n",
      "-------------------------------------------------------------------------------------\n",
      "benevolent              1         1        10       997     0.000     0.000      None\n",
      "provoked                1         2        22       994     0.000     0.000      None\n",
      "charitable              1         1         6       997     0.000     0.000      None\n",
      "wept                    1         2         1       997     0.000     0.000      None\n",
      "sinners                 1         1         1       998     0.000     0.000      None\n",
      "\n",
      "\n",
      "Summary stats\n",
      "--------------------------\n",
      "Precision:        0.0000\n",
      "Recall:           0.0000\n",
      "F1:               None   \n"
     ]
    }
   ],
   "source": [
    "summary_stats(jaccard1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words where there are >0 true positives:\n",
      "\n",
      "word              truePos  falsePos  falseNeg   trueNeg precision    recall        f1\n",
      "-------------------------------------------------------------------------------------\n",
      "transparent             1         0        16       998     1.000     0.000     0.000\n",
      "sinners                 1         3         1       996     0.000     0.000      None\n",
      "aggravated              1         2         5       996     0.000     0.000      None\n",
      "tumult                  1         5         9       993     0.000     0.000      None\n",
      "turmoil                 1         5         6       993     0.000     0.000      None\n",
      "crystalline             1         2         5       996     0.000     0.000      None\n",
      "provoked                2         3        21       993     0.000     0.000      None\n",
      "wept                    1         1         1       998     0.000     0.000      None\n",
      "spoils                  1         2        57       996     0.000     0.000      None\n",
      "\n",
      "\n",
      "Summary stats\n",
      "--------------------------\n",
      "Precision:        0.0000\n",
      "Recall:           0.0000\n",
      "F1:               None   \n"
     ]
    }
   ],
   "source": [
    "summary_stats(cosine1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
