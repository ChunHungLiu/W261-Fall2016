{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASCI W261: Machine Learning at Scale\n",
    "## Assignment Week 5\n",
    "Miki Seltzer (miki.seltzer@berkeley.edu)<br>\n",
    "W261-2, Spring 2016<br>\n",
    "Submission: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## HW5.0:\n",
    "### What is a data warehouse?\n",
    "A data warehouse is a repository for one or multiple data sources. Data warehouses can contain relational databases.\n",
    "\n",
    "### What is a star schema?\n",
    "A star schema relates multiple fact and dimension tables, and is similar to the snowflake schema. In both schemas, fact tables are referenced by dimension tables (one or multiple). However, star schemas are denormalized, whereas snowflake schemas are normalized.\n",
    "\n",
    "### When is it used?\n",
    "A star schema is used to organize the meta data of a relational database, such as which tables can be joined, and the keys on which they can be joined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW5.1:\n",
    "### In the database world, what is 3NF?\n",
    "3NF is shorthand for third normal form. A table is in third normal form if the following conditions hold:\n",
    "- The table is already in second normal form\n",
    "- Non-prime attributes of the table are non-transitively dependent on every key in the table\n",
    "\n",
    "### Does machine learning use data in 3NF?\n",
    "ML can, but does not always use data in 3NF.\n",
    "\n",
    "### If so, why?\n",
    "3NF can save a significant amount of disk space because data duplication is avoided. Additionally, if data is denormalized, then fields in the data set might be related to each other and create dependencies. This may be problematic if we are using algorithms that require independent features.\n",
    "\n",
    "### In what form does ML consume data?\n",
    "Typically, ML requires all data to be fed into an algorithm to be collected into a single source. Thus, the easiest way for ML to ingest data is for it to be denormalized.\n",
    "\n",
    "### Why would one use log files that are denormalized?\n",
    "If one needs to perform real-time analysis on log files, it may be too time consuming to join normalized log files with other tables. If log files are denormalized, they may not need any further processing (joins) to be fed into other steps of a pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.2: Using MRJob, implement a hashside join (memory-backed map-side) for left, right and inner joins. Run your code on the  data used in HW 4.4.\n",
    "\n",
    "### Justify which table you chose as left table in this hashside join\n",
    "The two tables used were:\n",
    "- **anonymous-msweb-preprocess.data:** The log file of visitors and each page that they visited (processed rows prefixed by 'C' or 'V')\n",
    "- **attributes.csv:** The page ID, page name and URL of each page (prefixed by 'A' in the original data)\n",
    "\n",
    "The attributes.csv file was very small (only 294 lines), so this is the file that I chose to store in memory. This became my **right** table.\n",
    "\n",
    "The log file was much larger, so I streamed through this file, and used it as the **left** table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We will need these so we can reload modules as we modify them\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapSideJoin.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapSideJoin.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    " \n",
    "class join(MRJob):\n",
    "    \n",
    "    # Specify some custom options so we only have to write one MRJob class for each join\n",
    "    def configure_options(self):\n",
    "        super(join, self).configure_options()\n",
    "        self.add_passthrough_option('--joinType', default='inner', )\n",
    "    \n",
    "    # Store attributes.csv into memory\n",
    "    #  - account for multiple occurrences of keys\n",
    "    #  - self.pages is dict with a list of [pageName, pageURL] pairs\n",
    "    # Set joinType variable\n",
    "    def mapper_init(self):\n",
    "        self.pages = {}\n",
    "        with open('attributes.csv','r') as myfile:\n",
    "            for line in myfile:\n",
    "                fields = line.strip().split(',')\n",
    "                if fields[0] not in self.pages:\n",
    "                    self.pages[fields[0]]=[]\n",
    "                self.pages[fields[0]].append([fields[1], fields[2]])\n",
    "        self.joinType = self.options.joinType\n",
    "        self.seenRight = set()\n",
    "    \n",
    "    # RIGHT table is stored in memory (self.pages)\n",
    "    # LEFT table is streamed\n",
    "    # We need so keep track of which RIGHT keys we have seen\n",
    "    def mapper(self, _, line):\n",
    "        fields = line.split(',')\n",
    "        key = fields[0]\n",
    "        self.seenRight.add(key)\n",
    "        if key in self.pages:\n",
    "            for i in self.pages[key]:\n",
    "                yield key, (fields[1], fields[2], i[0], i[1])\n",
    "        elif self.joinType == 'left':\n",
    "            yield key, (fields[1], fields[2], None, None)\n",
    "\n",
    "    # We need to emit all of the RIGHT keys that we never saw while streaming through LEFT\n",
    "    # We will need to deduplicate these in the reducer in case we have multiple mappers\n",
    "    def mapper_final(self):\n",
    "        if self.joinType == 'right':\n",
    "            for key in self.pages:\n",
    "                if key not in self.seenRight:\n",
    "                    for value in self.pages[key]:\n",
    "                        yield key, (None, None, value[0], value[1])\n",
    "    \n",
    "    # Need to persist variables\n",
    "    def reducer_init(self):\n",
    "        self.joinType = self.options.joinType\n",
    "    \n",
    "    # We need to unpack and emit each record\n",
    "    # We also need to do some work emitting records for the right join\n",
    "    def reducer(self, key, values):\n",
    "        emptyRight = True\n",
    "        for val in values:\n",
    "            if self.joinType == 'inner' or self.joinType == 'left':\n",
    "                yield key, val\n",
    "            elif self.joinType == 'right':\n",
    "                if val[:2] != [None]*2:\n",
    "                    emptyRight = False\n",
    "                    yield key, val\n",
    "                else: emptyRecord = val\n",
    "        if emptyRight and self.joinType == 'right':\n",
    "            yield key, emptyRecord\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    join.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    }
   ],
   "source": [
    "from mapSideJoin import join\n",
    "\n",
    "def runJoin(joinType):\n",
    "\n",
    "    mr_job = join(args=['TopVisitors.txt', '--file', 'attributes.csv', '--joinType', joinType])\n",
    "    output = []\n",
    "\n",
    "    with mr_job.make_runner() as runner: \n",
    "        # Run MRJob\n",
    "        runner.run()\n",
    "\n",
    "        # Write stream_output to file\n",
    "        for line in runner.stream_output():\n",
    "            output.append(mr_job.parse_output_line(line))\n",
    "    \n",
    "    return output\n",
    "            \n",
    "outInner = runJoin('inner')\n",
    "outLeft = runJoin('left')\n",
    "outRight = runJoin('right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows resulting from join type:\n",
      "\n",
      "inner   285\n",
      "left    285\n",
      "right   294\n"
     ]
    }
   ],
   "source": [
    "print \"Rows resulting from join type:\\n\"\n",
    "for joinType in ['inner', 'left', 'right']:\n",
    "    if joinType == 'inner': out = outInner\n",
    "    elif joinType == 'left': out = outLeft\n",
    "    elif joinType == 'right': out = outRight\n",
    "    \n",
    "    print \"{:7s}{:>4,d}\".format(joinType, len(out))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW5.3: Do some EDA on this data set using MRJob:\n",
    "1. Longest 5-gram (number of characters)\n",
    "2. Top 10 most frequent words (count), i.e., unigrams\n",
    "3. Top 20 most/least densely appearing words (count/pages_count) sorted in decreasing order\n",
    "4. Distribution of 5-gram sizes (counts) sorted in decreasing order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MRJob5_3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MRJob5_3.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class job(MRJob):\n",
    "    \n",
    "    # Specify some custom options so we only have to write one MRJob class for each part\n",
    "    def configure_options(self):\n",
    "        super(job, self).configure_options()\n",
    "        self.add_passthrough_option('--part', default='1')\n",
    "    \n",
    "    \"\"\"\n",
    "    Find the longest 5-gram\n",
    "    - In this case, in each mapper, we only need to store the length of the longest 5-gram we have seen\n",
    "    - After the mapper has run, we emit the longest 5-gram from this mapper\n",
    "    - All results will be sent to the same reducer (we specify this)\n",
    "    - Then we loop through the records in the reducer and emit the remaining longest 5-gram\n",
    "    \"\"\"\n",
    "    \n",
    "    def mapper_longest5Gram_init(self):\n",
    "        self.maxLength = 0\n",
    "        self.longest5Gram = None\n",
    "    \n",
    "    def mapper_longest5Gram(self, _, line):\n",
    "        fields = line.strip().split('\\t')\n",
    "        if len(fields[0]) > self.maxLength: \n",
    "            self.maxLength = len(fields[0])\n",
    "            self.longest5Gram = fields[0]\n",
    "            \n",
    "    def mapper_longest5Gram_final(self):\n",
    "        yield self.longest5Gram, self.maxLength\n",
    "     \n",
    "    def reducer_longest5Gram_init(self):\n",
    "        self.maxLength = 0\n",
    "        self.longest5Gram = None\n",
    "    \n",
    "    def reducer_longest5Gram(self, key, values):\n",
    "        for val in values:\n",
    "            if val > self.maxLength:\n",
    "                self.maxLength = val\n",
    "                self.longest5Gram = key\n",
    "        \n",
    "    def reducer_longest5Gram_final(self):\n",
    "        yield self.maxLength, self.longest5Gram\n",
    "    \n",
    "    \"\"\"\n",
    "    Top 10 most frequent words\n",
    "    - This is our standard word count\n",
    "    - Loop through each word in the 5-gram and emit (word, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    def mapper_topWords(self, _, line):\n",
    "        fields = line.strip().split('\\t')\n",
    "        words = fields[0].lower().split()\n",
    "        count, pages_count, books_count = int(fields[1]), int(fields[2]), int(fields[3])\n",
    "        for word in words:\n",
    "            self.increment_counter('total', 'words', count)\n",
    "            yield word, count\n",
    "        \n",
    "    def combiner_topWords(self, key, values):\n",
    "        yield key, sum(values)\n",
    "        \n",
    "    def reducer_topWords(self, key, values):\n",
    "        yield key, sum(values)\n",
    " \n",
    "    \"\"\"\n",
    "    Densely appearing words\n",
    "    - For each word, emit count and pages_count\n",
    "    - Combiner sums count and pages_count\n",
    "    - Reducer sums count and pages_count, then emits count/pages_count\n",
    "    \"\"\"\n",
    "    \n",
    "    def mapper_denseWords(self, _, line):\n",
    "        fields = line.strip().split('\\t')\n",
    "        words = fields[0].lower().split()\n",
    "        count, pages_count, books_count = int(fields[1]), int(fields[2]), int(fields[3])\n",
    "        for word in words:\n",
    "            yield word, (count, pages_count)\n",
    "        \n",
    "    def combiner_denseWords(self, key, values):\n",
    "        count, pages_count = 0.0, 0.0\n",
    "        for val in values:\n",
    "            count += val[0]\n",
    "            pages_count += val[1]\n",
    "        yield key, (count, pages_count)\n",
    "        \n",
    "    def reducer_denseWords(self, key, values):\n",
    "        count, pages_count = 0.0, 0.0\n",
    "        for val in values:\n",
    "            count += val[0]\n",
    "            pages_count += val[1]\n",
    "        yield key, count/pages_count\n",
    "\n",
    "    \"\"\"\n",
    "    Frequent5-grams\n",
    "    - Use count to determine the most frequent 5-gram\n",
    "    - Sum counts in combiner and reducer\n",
    "    \"\"\"\n",
    "        \n",
    "    def mapper_frequent5Gram(self, _, line):\n",
    "        fields = line.strip().split('\\t')\n",
    "        yield len(fields[0]), float(fields[1])\n",
    "        \n",
    "    def combiner_frequent5Gram(self, key, values):\n",
    "        yield key, sum(values)\n",
    "        \n",
    "    def reducer_frequent5Gram(self, key, values):\n",
    "        yield key, sum(values)\n",
    "        \n",
    "    \"\"\"\n",
    "    Sorting functions\n",
    "    - We need these to get the top and bottom values\n",
    "    - Utilize only one reducer instead of writing a custom partitioner\n",
    "    \"\"\"\n",
    "    def mapper_sort(self, key, value):\n",
    "        yield float(value), key\n",
    "        \n",
    "    def reducer_sort_init(self):\n",
    "        self.count = 0\n",
    "    \n",
    "    def reducer_top10(self, key, values):\n",
    "        for val in values:\n",
    "            if self.count < 10:\n",
    "                yield key, val\n",
    "                self.count += 1\n",
    "                \n",
    "    def reducer_top100(self, key, values):\n",
    "        for val in values:\n",
    "            if self.count < 100:\n",
    "                yield key, val\n",
    "                self.count += 1\n",
    "                \n",
    "    def reducer_top10000(self, key, values):\n",
    "        for val in values:\n",
    "            if self.count < 10000:\n",
    "                yield key, val\n",
    "                self.count += 1\n",
    "                \n",
    "    def reducer_all(self, key, values):\n",
    "        for val in values:\n",
    "            yield key, val\n",
    "\n",
    "    \"\"\"\n",
    "    Multi-step pipeline definitions\n",
    "    Based on user input when calling runner function\n",
    "    \"\"\"\n",
    "    def steps(self):\n",
    "        self.part = self.options.part\n",
    "        if self.part == '1':\n",
    "            return [\n",
    "                MRStep(mapper_init=self.mapper_longest5Gram_init,\n",
    "                       mapper=self.mapper_longest5Gram,\n",
    "                       mapper_final=self.mapper_longest5Gram_final,\n",
    "                       reducer_init=self.reducer_longest5Gram_init,\n",
    "                       reducer=self.reducer_longest5Gram,\n",
    "                       reducer_final=self.reducer_longest5Gram_final,\n",
    "                       jobconf={'mapred.reduce.tasks': 1})\n",
    "            ]\n",
    "        elif self.part == '2':\n",
    "            return [\n",
    "                MRStep(mapper=self.mapper_topWords,\n",
    "                       combiner=self.combiner_topWords,\n",
    "                       reducer=self.reducer_topWords),\n",
    "                MRStep(mapper=self.mapper_sort,\n",
    "                       reducer_init=self.reducer_sort_init,\n",
    "                       reducer=self.reducer_all,\n",
    "                       jobconf={'mapred.output.key.comparator.class':'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                                'mapred.text.key.partitioner.options':'-k1,1',\n",
    "                                'stream.num.map.output.key.fields':1,\n",
    "                                'mapred.text.key.comparator.options':'-k1,1nr',\n",
    "                                'mapred.reduce.tasks': 1})\n",
    "            ]\n",
    "        elif self.part == '3':\n",
    "            return [\n",
    "                MRStep(mapper=self.mapper_denseWords,\n",
    "                       combiner=self.combiner_denseWords,\n",
    "                       reducer=self.reducer_denseWords),\n",
    "                MRStep(mapper=self.mapper_sort,\n",
    "                       reducer_init=self.reducer_sort_init,\n",
    "                       reducer=self.reducer_all,\n",
    "                       jobconf={'mapred.output.key.comparator.class':'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                                'mapred.text.key.partitioner.options':'-k1,1',\n",
    "                                'stream.num.map.output.key.fields':1,\n",
    "                                'mapred.text.key.comparator.options':'-k1,1nr',\n",
    "                                'mapred.reduce.tasks': 1})\n",
    "            ]\n",
    "        elif self.part == '4':\n",
    "            return [\n",
    "                MRStep(mapper=self.mapper_frequent5Gram,\n",
    "                       combiner=self.combiner_frequent5Gram,\n",
    "                       reducer=self.reducer_frequent5Gram),\n",
    "                MRStep(mapper=self.mapper_sort,\n",
    "                       reducer_init=self.reducer_sort_init,\n",
    "                       reducer=self.reducer_all,\n",
    "                       jobconf={'mapred.output.key.comparator.class':'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                                'mapred.text.key.partitioner.options':'-k1,1',\n",
    "                                'stream.num.map.output.key.fields':1,\n",
    "                                'mapred.text.key.comparator.options':'-k1,1nr',\n",
    "                                'mapred.reduce.tasks': 1})\n",
    "            ]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using configs in /etc/mrjob.conf\n",
      "using existing scratch bucket mrjob-ac40f1afcc0b86ce\n",
      "using s3://mrjob-ac40f1afcc0b86ce/tmp/ as our scratch dir on S3\n",
      "Creating persistent job flow to run several jobs in...\n",
      "creating tmp directory /tmp/no_script.cloudera.20160214.231258.206908\n",
      "writing master bootstrap script to /tmp/no_script.cloudera.20160214.231258.206908/b.py\n",
      "Copying non-input files into s3://mrjob-ac40f1afcc0b86ce/tmp/no_script.cloudera.20160214.231258.206908/files/\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Creating Elastic MapReduce job flow\n",
      "Job flow created with ID: j-98XEIC78B7U2\n",
      "j-98XEIC78B7U2\n"
     ]
    }
   ],
   "source": [
    "# Create job flow so that we don't need to keep spinning up clusters\n",
    "!python -m mrjob.tools.emr.create_job_flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create our runner and supporting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from MRJob5_3 import job\n",
    "\n",
    "def runJob5_3(filename, part, s3bucket):\n",
    "\n",
    "    #mr_job = job(args=[filename, '--part', str(part)])\n",
    "    #mr_job = job(args=[filename, '--part', str(part), '-r', 'hadoop', '--hadoop-home', '/usr/'])\n",
    "    mr_job = job(args=[filename, '--part', str(part), '--no-output', '--output-dir', s3bucket,\n",
    "                       '-r', 'emr', '--emr-job-flow-id', 'j-98XEIC78B7U2'])\n",
    "    \n",
    "    output = []\n",
    "\n",
    "    with mr_job.make_runner() as runner: \n",
    "        # Run MRJob\n",
    "        runner.run()\n",
    "\n",
    "        # Write stream_output to file\n",
    "        for line in runner.stream_output():\n",
    "            output.append(mr_job.parse_output_line(line))\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_output(output, part):\n",
    "    if part == 1:\n",
    "        print \"Longest 5-gram (number of characters)\"\n",
    "        print '{:>21s}   {:<10s}'.format('length', '5gram')\n",
    "    elif part == 2:\n",
    "        print \"Top 10 words (using count information)\"\n",
    "        print '{:>21s}   {:<10s}'.format('count', 'word')\n",
    "    elif part == 3:\n",
    "        print \"Top 20 most/least dense words\"\n",
    "        print '{:>21s}   {:<10s}'.format('density', 'word')\n",
    "    elif part == 4:\n",
    "        print \"Distribution of 5gram sizes\"\n",
    "        print '{:>21s}   {:<10s}'.format('count', 'length')\n",
    "    \n",
    "    print '-------------------------------------------------------'\n",
    "    \n",
    "    for item in output:\n",
    "        if part == 3:\n",
    "            print '{:21.5f}   {:<50s}'.format(item[0], item[1])\n",
    "        elif part == 4:\n",
    "            print '{:21,d}   {:<50d}'.format(int(item[0]), item[1])\n",
    "        else:\n",
    "            print '{:21,d}   {:<50s}'.format(int(item[0]), item[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run each part of the script on the full 5gram dataset on EMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myfile = 's3://filtered-5grams/'\n",
    "#myfile = './filtered-5Grams/short-5gram.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Longest 5-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest 5-gram (number of characters)\n",
      "               length   5gram     \n",
      "-------------------------------------------------------\n",
      "                  159   AIOPJUMRXUYVASLYHYPSIBEMAPODIKR UFRYDIUUOLBIGASUAURUSREXLISNAYE RNOONDQSRUNSUBUNOUGRABBERYAIRTC UTAHRAPTOREDILEIPMILBDUMMYUVERI SYEVRAHVELOCYALLOSAURUSLINROTSR\n"
     ]
    }
   ],
   "source": [
    "output_bucket = 's3://ms-w261-hw05/hw5_3a'\n",
    "\n",
    "!aws s3 rm --recursive {output_bucket}\n",
    "\n",
    "part = 1\n",
    "output = runJob5_3(myfile, part, output_bucket)\n",
    "format_output(output, part)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words (using count information)\n",
      "                count   word      \n",
      "-------------------------------------------------------\n",
      "        5,490,815,394   the                                               \n",
      "        3,698,583,299   of                                                \n",
      "        2,227,866,570   to                                                \n",
      "        1,421,312,776   in                                                \n",
      "        1,361,123,022   a                                                 \n",
      "        1,149,577,477   and                                               \n",
      "          802,921,147   that                                              \n",
      "          758,328,796   is                                                \n",
      "          688,707,130   be                                                \n",
      "          492,170,314   as                                                \n"
     ]
    }
   ],
   "source": [
    "output_bucket = 's3://ms-w261-hw05/hw5_3b'\n",
    "\n",
    "!aws s3 rm --recursive {output_bucket}\n",
    "\n",
    "part = 2\n",
    "output = runJob5_3(myfile, part, output_bucket)\n",
    "format_output(output[:10], part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 20 most/least dense words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 most/least dense words\n",
      "              density   word      \n",
      "-------------------------------------------------------\n",
      "             11.55729   xxxx                                              \n",
      "              8.07416   blah                                              \n",
      "              7.53333   nnn                                               \n",
      "              6.20175   na                                                \n",
      "              4.92188   oooooooooooooooo                                  \n",
      "              4.85431   nd                                                \n",
      "              4.51163   llll                                              \n",
      "              4.16965   oooooo                                            \n",
      "              3.85864   ooooo                                             \n",
      "              3.76245   lillelu                                           \n",
      "              3.57692   pfeffermann                                       \n",
      "              3.57692   madarassy                                         \n",
      "              3.56000   meteoritical                                      \n",
      "              3.50000   xxxxxxxx                                          \n",
      "              3.22904   beep                                              \n",
      "              3.18868   latha                                             \n",
      "              2.91912   iyengar                                           \n",
      "              2.82500   counterfeiteth                                    \n",
      "              2.81982   nonmorular                                        \n",
      "              2.81982   nonsquamous                                       \n",
      "\n",
      "\n",
      "Top 20 most/least dense words\n",
      "              density   word      \n",
      "-------------------------------------------------------\n",
      "              1.00000   hosier's                                          \n",
      "              1.00000   hosp                                              \n",
      "              1.00000   hospites                                          \n",
      "              1.00000   hostein                                           \n",
      "              1.00000   hostelrie                                         \n",
      "              1.00000   hostelries                                        \n",
      "              1.00000   hostlers                                          \n",
      "              1.00000   hostus                                            \n",
      "              1.00000   hote                                              \n",
      "              1.00000   hotest                                            \n",
      "              1.00000   hotheaded                                         \n",
      "              1.00000   hothouse                                          \n",
      "              1.00000   hothoused                                         \n",
      "              1.00000   hothouses                                         \n",
      "              1.00000   hotsumi                                           \n",
      "              1.00000   hott                                              \n",
      "              1.00000   hottse                                            \n",
      "              1.00000   houard's                                          \n",
      "              1.00000   houbraken                                         \n",
      "              1.00000   houbraken's                                       \n"
     ]
    }
   ],
   "source": [
    "output_bucket = 's3://ms-w261-hw05/hw5_3c'\n",
    "\n",
    "!aws s3 rm --recursive {output_bucket}\n",
    "\n",
    "part = 3\n",
    "output = runJob5_3(myfile, part, output_bucket)\n",
    "format_output(output[:20], part)\n",
    "print '\\n'\n",
    "format_output(output[-20:], part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogram of 5-gram lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.conf:Got unexpected keyword arguments: ssh_tunnel\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    }
   ],
   "source": [
    "output_bucket = 's3://ms-w261-hw05/hw5_3d'\n",
    "\n",
    "!aws s3 rm --recursive {output_bucket}\n",
    "\n",
    "part = 4\n",
    "output = runJob5_3(myfile, part, output_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA74AAAGJCAYAAACtnu89AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAjJrgCwAAwKgJvgAAAIza/w84mfUOOhu1IAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcbd3ad7550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_hist(output):\n",
    "    frequencies = np.array(output)\n",
    "    density = np.column_stack((frequencies[:,1], frequencies[:,0] / sum(frequencies[:,0])))\n",
    "\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    plt.bar(density[:,0], density[:,1], color='#797f8b', edgecolor=\"none\")\n",
    "    plt.xlabel(\"Length of 5gram\")\n",
    "    plt.ylabel(\"Relative frequency\")\n",
    "    plt.title(\"Histogram of 5gram lengths\")\n",
    "    \n",
    "plot_hist(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW5.4: Synonym detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build stripes of word co-occurrence for the 9,000 words ranked 1,001 - 10,000\n",
    "\n",
    "#### First we need to extract our basis using data from a previous job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://ms-w261-hw05/hw5_3b/part-00000 to ./topWords.txt\r\n"
     ]
    }
   ],
   "source": [
    "# Copy results from S3 so we can extract our basis\n",
    "!aws s3 cp s3://ms-w261-hw05/hw5_3b/part-00000 topWords.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "basis = []\n",
    "\n",
    "with open('topWords.txt','r') as myfile:\n",
    "    count = 0\n",
    "    for line in myfile:\n",
    "        if count > 1000 and count <= 10000:\n",
    "            fields = line.strip().split('\\t')\n",
    "            word = eval(fields[1])\n",
    "            basis.append(word)\n",
    "        count += 1\n",
    "\n",
    "with open('basisWords.txt','w') as myfile:\n",
    "    for word in basis:\n",
    "        myfile.write(word+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We also need to create our unit testing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = [['A', {'X':20, 'Y':30, 'Z':5}],\n",
    "          ['B', {'X':100, 'Y':20}],\n",
    "          ['C', {'M':5, 'N':20, 'Z':5}]]\n",
    "\n",
    "with open('outputUnit.txt', 'w') as myfile:\n",
    "    for out in output:\n",
    "        myfile.write(str(out[0])+'\\t'+str(out[1])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting unitBasis.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile unitBasis.txt\n",
    "A\n",
    "B\n",
    "C\n",
    "M\n",
    "N\n",
    "X\n",
    "Y\n",
    "Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create co-occurrence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MRJob5_4_1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MRJob5_4_1.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class stripes(MRJob):\n",
    "    \n",
    "    \"\"\"\n",
    "    Build stripes\n",
    "    - Read in basis words from basisWords.txt\n",
    "    - For each 5-gram:\n",
    "       - Deduplicate the words in the 5-gram, then sort alphabetically\n",
    "       - Extract count\n",
    "       - Build stripe: (word1, {word2: x, word3: y, ...})\n",
    "       \n",
    "    \"\"\"\n",
    "    \n",
    "    def mapper_buildStripe_init(self):\n",
    "        self.numDocuments = 0\n",
    "        self.vocab = set()\n",
    "        with open('basisWords.txt','r') as myfile:\n",
    "            for word in myfile:\n",
    "                self.vocab.add(word.strip())\n",
    "        \n",
    "    def mapper_buildStripe(self, _, line):\n",
    "        fields = line.strip().split('\\t')\n",
    "        words = fields[0].lower().split()\n",
    "        wordList = sorted(list(set(words)))\n",
    "        count = float(fields[1])\n",
    "        self.numDocuments += count\n",
    "        for index1 in range(len(wordList)-1):\n",
    "            stripe = {}\n",
    "            if wordList[index1] in self.vocab:\n",
    "                for index2 in range(index1+1,len(wordList)):\n",
    "                    if wordList[index2] in self.vocab:\n",
    "                        stripe[wordList[index2]] = count\n",
    "            if len(stripe) > 0:\n",
    "                yield wordList[index1], stripe\n",
    "                \n",
    "    def mapper_buildStripe_final(self):\n",
    "        yield '*total', {'*total':self.numDocuments}\n",
    "            \n",
    "    def combiner_buildStripe(self, key, values):\n",
    "        stripe = {}\n",
    "        for val in values:\n",
    "            for word in val:\n",
    "                if word in stripe:\n",
    "                    stripe[word] += val[word]\n",
    "                else:\n",
    "                    stripe[word] = val[word]\n",
    "        yield key, stripe\n",
    "        \n",
    "    def reducer_buildStripe_init(self):\n",
    "        self.numDocs = 0\n",
    "    \n",
    "    def reducer_buildStripe(self, key, values):\n",
    "        stripe = {}\n",
    "        for val in values:\n",
    "            for word in val:\n",
    "                if word in stripe:\n",
    "                    stripe[word] += val[word]\n",
    "                else:\n",
    "                    stripe[word] = val[word]\n",
    "        if key == '*total':\n",
    "            self.numDocs = stripe['*total']\n",
    "        else:\n",
    "            for word in stripe:\n",
    "                stripe[word] /= self.numDocs\n",
    "            yield key, stripe\n",
    "    \n",
    "            \n",
    "        \n",
    "    \"\"\"\n",
    "    Multi-step pipeline definitions\n",
    "    Based on user input when calling runner function\n",
    "    \"\"\"\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init=self.mapper_buildStripe_init,\n",
    "                   mapper=self.mapper_buildStripe,\n",
    "                   mapper_final=self.mapper_buildStripe_final,\n",
    "                   combiner=self.combiner_buildStripe,\n",
    "                   reducer_init=self.reducer_buildStripe_init,\n",
    "                   reducer=self.reducer_buildStripe,\n",
    "                   jobconf={'mapred.reduce.tasks': 1})\n",
    "        ]\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    stripes.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from MRJob5_4_1 import stripes\n",
    "\n",
    "def runJob5_4_1(filename, s3bucket, basis):\n",
    "\n",
    "    #mr_job = stripes(args=[filename, '--file', basis])\n",
    "    #mr_job = stripes(args=[filename, '-r', 'hadoop', '--hadoop-home', '/usr/', '--file', basis])\n",
    "    mr_job = stripes(args=[filename, '-r', 'emr', '--file', basis, '--no-output', '--output-dir', s3bucket,\n",
    "                           '--emr-job-flow-id', 'j-98XEIC78B7U2'])\n",
    "    \n",
    "    output = []\n",
    "    \n",
    "    with mr_job.make_runner() as runner: \n",
    "        # Run MRJob\n",
    "        runner.run()\n",
    "\n",
    "        # Write stream_output to file\n",
    "        for line in runner.stream_output():\n",
    "            out = mr_job.parse_output_line(line)\n",
    "            output.append(out)\n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.compat:Detected hadoop configuration property names that do not match hadoop version 2.6.0:\n",
      "The have been translated as follows\n",
      " mapred.reduce.tasks: mapreduce.job.reduces\n"
     ]
    }
   ],
   "source": [
    "# Test our matrix creation on a small subset of data\n",
    "output = runJob5_4_1('./filtered-5Grams/googlebooks-eng-all-5gram-20090715-0-filtered.txt', 'none', 'basisWords.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('alternate', {'viewing': 1.6005761449506742e-06})\n",
      "(\"alzheimer's\", {'dementia': 3.532979051659415e-06})\n",
      "('amidst', {'tumult': 1.5615377023909016e-06, 'restless': 8.393265150351096e-07})\n",
      "('ammonium', {'hydroxide': 1.522499259831129e-06})\n",
      "('anemia', {'pernicious': 1.1321148342334037e-06})\n",
      "('annum', {'thereon': 1.3468262683121526e-06})\n",
      "('approximated', {'subcutaneous': 2.3423065535863526e-06})\n",
      "('architectural', {'decoration': 8.978841788747684e-07})\n",
      "('articular', {'cartilage': 9.954802852741998e-07})\n",
      "('authoritative', {'interpreter': 9.759610639943135e-07})\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print output[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run on EMR on full data set -- this will get us the top right of the co-occurrence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.conf:Got unexpected keyword arguments: ssh_tunnel\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    }
   ],
   "source": [
    "myfile = 's3://filtered-5grams/'\n",
    "mybasis = 'basisWords.txt'\n",
    "output_bucket = 's3://ms-w261-hw05/hw5_4_1'\n",
    "\n",
    "!aws s3 rm --recursive {output_bucket}\n",
    "\n",
    "output = runJob5_4_1(myfile, output_bucket, mybasis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ab', {'essay': 4.238952276324725e-09, 'represent': 1.5007909606892538e-07, 'consul': 4.844516887228257e-09, 'personally': 7.165847895691797e-09, 'rod': 3.169121463728485e-08, 'month': 1.9781777289515385e-08, 'hath': 9.719312005001691e-08, 'consists': 1.3019639134425941e-08, 'centers': 6.358428414487087e-09, 'segments': 3.239770668333897e-08, 'division': 1.7359518845901255e-08, 'amplifier': 9.18439659870357e-09, 'void': 1.857064806770832e-07, 'squares': 6.963993025390619e-08, 'cable': 9.588106339305926e-09, 'die': 9.285324033854159e-09, 'p': 1.5845607318642425e-08, 'vector': 4.6426620169270795e-09, 'yale': 1.392798605078124e-08, 'lever': 7.771412506595329e-09, 'round': 4.4408071466259026e-09, 'estimates': 1.3625203745329473e-08, 'prediction': 1.78641560216542e-08, 'dynamics': 6.15657354418591e-09, 'honors': 4.339879711475314e-09, 'index': 4.037097406023548e-09, 'desirous': 1.8066010891955377e-08, 'supplied': 1.2615929393823586e-08, 'uniform': 3.0278230545176605e-08, 'journal': 8.47790455264945e-09, 'edited': 2.0488269335569504e-08, 'respectively': 3.088379515608014e-08, 'fifteenth': 1.7056736540449488e-08, 'tangent': 6.893343820785208e-08, 'directions': 4.582105555836727e-08, 'path': 6.863065590240031e-09, 'extreme': 1.0698308125962402e-08, 'bible': 3.693944126511546e-08, 'receive': 7.448444714113446e-08, 'k': 8.982541728402394e-09, 'title': 1.4028913485931828e-08, 'diameter': 1.356464728423912e-07, 'named': 6.257500979336498e-09, 'tasks': 9.386251469004748e-09, 'divide': 6.721767181029207e-08, 'extra': 4.7536821955927275e-08, 'cylinder': 7.569557636294151e-09, 'graduated': 2.5655754015279647e-07, 'fee': 1.2212219653221232e-08, 'remains': 1.312056656957653e-08, 'illegal': 8.881614293251805e-09, 'inclined': 2.0488269335569504e-08, 'hereby': 8.649481192405451e-08, 'adult': 7.2667753308423855e-09, 'f': 4.4408071466259026e-09, 'equation': 5.7528638035835556e-09, 'modified': 5.853791238734144e-09, 'den': 5.551008933282378e-09, 'inscription': 6.257500979336498e-09, 'bent': 4.037097406023548e-09, 'indicated': 7.165847895691797e-09, 'stated': 4.743589452077668e-09, 'calculation': 3.6636658959663695e-08, 'indicates': 5.651936368432967e-09, 'axis': 8.56873924428498e-08, 'sir': 1.3726131180480062e-08, 'rays': 4.6426620169270795e-09, 'hoc': 9.487178904155337e-09, 'declared': 1.0597380690811813e-08, 'carriers': 5.349154062981201e-09, 'travel': 4.4408071466259026e-09, 'holds': 8.175122247197685e-09, 'tension': 9.588106339305926e-09, 'levy': 1.463447809683536e-08, 'derived': 1.3322421439877708e-08, 'plane': 2.437397558886717e-07, 'horizontal': 4.662847503957197e-08, 'parallel': 6.775258721659019e-07, 'stress': 6.661210719938854e-09, 'counter': 1.0193670950209457e-08, 'height': 1.78641560216542e-08, 'greek': 4.238952276324725e-09, 'succession': 5.853791238734144e-09, 'se': 5.651936368432967e-09, 'scott': 2.5030003917345993e-08, 'cuts': 4.6426620169270795e-09, 'fit': 4.339879711475314e-09, 'acknowledge': 5.147299192680023e-09, 'll': 1.6451171929545955e-08, 'whereof': 3.946262714388018e-08, 'degrees': 2.4999725686800817e-07, 'fig': 1.6451171929545957e-07, 'petition': 1.5038187837437714e-08, 'conveying': 2.7553189796110712e-08, 'non': 6.106109826610615e-08, 'cutting': 2.7250407490658945e-08, 'rotation': 7.771412506595329e-09, 'structural': 4.743589452077668e-09, 'represents': 7.801690737140506e-08, 'imprisoned': 2.0589196770720092e-08, 'magnitude': 3.310419872939309e-08, 'arrow': 4.844516887228257e-09, 'accompanied': 5.248226627830612e-09, 'friendship': 4.844516887228257e-09, 'sterling': 6.257500979336498e-09, 'domain': 6.15657354418591e-09, 'square': 7.488815688173681e-08, 'meets': 6.358428414487087e-09, 'owing': 1.392798605078124e-08, 'ex': 6.96399302539062e-09, 'quantum': 3.03791579803272e-08, 'et': 1.6854881670148312e-07, 'matrix': 1.826786576225655e-08, 'wholly': 1.0496453255661224e-08, 'acting': 1.8066010891955377e-08, 'oriental': 1.3423348875028295e-08, 'receiving': 2.624113313915306e-08, 'occurs': 5.853791238734144e-09, 'red': 5.248226627830612e-09, 'shows': 1.998363215981656e-08, 'disputed': 4.4408071466259026e-09, 'team': 1.3726131180480062e-08, 'mechanisms': 5.349154062981201e-09, 'diagonal': 5.853791238734144e-09, 'terminals': 1.4230768356233004e-08, 'qui': 1.4584014379260065e-07, 'ore': 4.743589452077668e-09, 'remainder': 4.844516887228257e-09, 'hostile': 4.138024841174136e-09, 'phrases': 4.4408071466259026e-09, 'w': 6.863065590240031e-09, 'ltd': 6.257500979336498e-09, 'assign': 1.4331695791383593e-08, 'striking': 4.541734581776491e-09, 'reflected': 4.238952276324725e-09, 'hearing': 6.055646109035321e-09, 'philosophical': 4.541734581776491e-09, 'electronic': 3.11865774615319e-08, 'mercury': 4.945444322378846e-09, 'passes': 4.945444322378846e-09, 'calculations': 3.2962900320182265e-07, 'john': 1.988270472466597e-08, 'speaking': 4.6426620169270795e-09, 'chemicals': 7.771412506595329e-09, 'b': 1.7288869641295844e-07, 'radius': 6.752045411574383e-08, 'russia': 2.8764319017917775e-08, 'creditors': 3.017730311002602e-08, 'pro': 2.2910527779183633e-08, 'curves': 8.578831987800039e-09, 'medical': 8.982541728402394e-09, 'recover': 5.248226627830612e-09, 'ray': 8.780686858101216e-09, 'lie': 4.945444322378846e-09, 'angle': 1.294898992982053e-07, 'convenient': 1.78641560216542e-08, 'caesar': 8.37697711749886e-09, 'interior': 7.569557636294151e-09, 'performance': 5.349154062981201e-09, 'coincide': 4.4408071466259026e-09, 'trace': 6.055646109035321e-09, 'extending': 8.578831987800039e-09, 'eight': 1.1001090431414167e-08, 'segment': 1.3695852949934886e-07, 'camps': 8.074194812047096e-09, 'episode': 1.3726131180480062e-08, 'professor': 6.055646109035321e-09, 'm': 6.96399302539062e-09, 'mechanical': 2.4222584436141286e-08, 'velocity': 1.241407452352241e-08, 'joining': 4.138024841174136e-09, 'slope': 4.915166091833669e-08, 'fine': 7.771412506595329e-09, 'agrees': 9.588106339305926e-09, 'ratio': 7.14566240866168e-08, 'earned': 3.1792142072435435e-08, 'circle': 6.06573885255038e-08, 'triangle': 6.540097797758148e-08, 'beam': 3.169121463728485e-08, 'earn': 4.037097406023548e-09, 'bar': 2.1497543687075392e-08, 'fields': 2.8764319017917775e-08, 'twice': 1.5542825013190658e-08, 'liable': 1.3423348875028295e-08, 'bars': 1.4937260402287125e-08, 'intelligence': 6.863065590240031e-09, 'elected': 7.2667753308423855e-09, 'arc': 2.356655610766246e-07, 'college': 5.97490416091485e-08, 'arm': 6.560283284788265e-09, 'techniques': 7.165847895691797e-09, 'nuclear': 5.349154062981201e-09, 'publishers': 6.257500979336498e-09, 'latin': 5.046371757529434e-09, 'parallels': 8.982541728402394e-09, 'vertical': 3.9664482014181355e-08, 'deliver': 1.231314708837182e-08, 'incident': 9.083469163552981e-09, 'harvard': 6.822694616179795e-08, 'inertia': 6.661210719938854e-09, 'c': 2.051854756611468e-07, 'contract': 2.3011455214334222e-08, 'und': 1.6552099364696544e-08, 'faces': 1.0092743515058868e-08, 'co': 6.762138155089442e-09, 'acted': 7.569557636294151e-09, 'corresponds': 8.679759422950627e-09, 'ce': 5.853791238734144e-09, 'cd': 1.6332077556068262e-06, 'represented': 3.522367486755545e-08, 'pole': 1.0597380690811813e-08, 'grants': 6.15657354418591e-09, 'learning': 7.2667753308423855e-09, 'proved': 8.37697711749886e-09, 'connexion': 1.5643752448341247e-08, 'custody': 7.367702765992974e-09, 'subscribed': 4.4408071466259026e-09, 'erect': 5.046371757529434e-09, 'firm': 7.0649204605412086e-09, 'concerning': 1.8166938327105963e-08, 'wires': 5.147299192680023e-09, 'angles': 4.360065198505431e-08, 'bull': 1.312056656957653e-08, 'doth': 1.55327322696756e-07, 'arisen': 6.257500979336498e-09, 'atmospheric': 1.0294598385360046e-08, 'sit': 1.392798605078124e-08, 'amend': 1.917621267861185e-08, 'plain': 5.551008933282378e-09, 'straight': 6.495689726291888e-07, 'bill': 1.8570648067708318e-08, 'empirical': 5.651936368432967e-09, 'suppose': 2.2910527779183633e-08, 'fleet': 5.046371757529434e-09, 'anchor': 3.693944126511546e-08, 'seven': 1.1303872736865933e-08, 'molecular': 4.8041459131680216e-08, 'von': 8.47790455264945e-09, 'protein': 1.2817784264124763e-08, 'containing': 1.1404800172016522e-08, 'null': 3.83524253572237e-08, 'grant': 8.578831987800039e-09, 'lengths': 5.783142034128732e-08, 'parish': 1.1404800172016522e-08, 'heirs': 1.3221494004727118e-07, 'finite': 1.1001090431414167e-08, 'entitled': 1.5542825013190658e-08, 'covenant': 3.65357315245131e-08, 'independent': 6.661210719938854e-09, 'oil': 2.8764319017917775e-08, 'columbia': 5.651936368432967e-09, 'edges': 4.238952276324725e-09, 'moving': 5.7528638035835556e-09, 'neural': 5.349154062981201e-09, 'possessed': 5.046371757529434e-09, 'humble': 1.5038187837437714e-08, 'spectator': 7.367702765992974e-09, 'chemistry': 2.5635568528249527e-08, 'charged': 6.459355849637676e-09, 'bending': 6.358428414487087e-09, 'distances': 3.7141296135416636e-08, 'chord': 1.0153299976149222e-07, 'bearer': 6.863065590240031e-09, 'proposed': 4.4408071466259026e-09, 'moments': 1.5038187837437714e-08, 'ninth': 1.4139933664597476e-07, 'dose': 5.450081498131789e-09, 'underlying': 5.349154062981201e-09, 'distant': 1.6653026799847133e-08, 'prince': 8.37697711749886e-09, 'unto': 2.8057826971863657e-08, 'vices': 1.0900162996263578e-08, 'residential': 7.771412506595329e-09, 'add': 4.4408071466259026e-09, 'resolved': 1.4836332967136538e-08, 'complaint': 2.4121657000990697e-08, 'offence': 3.996726431963312e-08, 'intersection': 1.0789142817597931e-07, 'd': 7.428259227083327e-08, 'precise': 8.881614293251805e-09, 'sincerely': 2.8259681842164834e-08, 'officer': 7.367702765992974e-09, 'translated': 5.651936368432967e-09, 'output': 9.588106339305926e-09, 'tower': 5.046371757529434e-09, 'pursuance': 7.468630201143562e-09, 'peter': 6.560283284788265e-09, 'happens': 8.578831987800039e-09, 'est': 2.5837423398550704e-08, 'projection': 3.704036870026605e-08, 'tube': 1.7965083456804788e-08, 'connecting': 6.863065590240031e-09, 'limitations': 8.881614293251805e-09, 'bc': 3.968466750121147e-07, 'equivalent': 6.863065590240031e-09, 'perpendicular': 6.817648244422265e-07, 'offset': 4.844516887228257e-09, 'sixty': 6.055646109035321e-09, 'post': 2.1194761381623626e-08, 'column': 4.238952276324725e-09, 'simulation': 6.15657354418591e-09, 'swear': 7.367702765992974e-09, 'equally': 1.6653026799847133e-08, 'owe': 5.147299192680023e-09, 'image': 1.1303872736865933e-08, 'lies': 1.8873430373160085e-08, 'promise': 2.8259681842164834e-08, 'doctrine': 1.917621267861185e-08, 'proportional': 6.812601872664737e-08, 'statistics': 8.982541728402394e-09, 'measured': 2.6442988009454238e-08, 'fast': 6.661210719938854e-09, 'awarded': 1.5038187837437714e-08, 'sealed': 6.358428414487087e-09, 'suit': 4.1178393541440185e-08, 'slopes': 6.459355849637676e-09, 'sections': 1.524004270773889e-08, 'vested': 1.4836332967136538e-08, 'delayed': 9.386251469004748e-09, 'administrators': 9.789961209607103e-09, 'payable': 6.863065590240031e-09, 'link': 4.844516887228257e-09, 'segregation': 7.771412506595329e-09, 'portions': 1.2817784264124763e-08, 'evident': 4.238952276324725e-09, 'extremity': 5.349154062981201e-09, 'attached': 7.468630201143562e-09, 'signed': 1.5139115272588303e-08, 'crystal': 7.0649204605412086e-09, 'planes': 3.7645933311169584e-08, 'agree': 9.689033774456515e-09, 'bearing': 4.511456351231314e-08, 'ordered': 1.0900162996263578e-08, 'ad': 1.6350244494395369e-07, 'defined': 1.1707582477468287e-08, 'describe': 6.560283284788265e-09, 'moved': 9.789961209607103e-09, 'resultant': 4.259137763354843e-08, 'promised': 1.524004270773889e-08, 'ship': 8.679759422950627e-09, 'moves': 7.468630201143562e-09, 'dotted': 3.249863411848956e-08, 'whereas': 5.157391936195082e-08, 'lately': 6.459355849637676e-09, 'draw': 1.1313965480380992e-07, 'spectators': 8.982541728402394e-09, 'june': 1.7157663975600077e-08, 'william': 6.560283284788265e-09, 'scale': 4.743589452077668e-09, 'e': 4.4408071466259026e-09, 'interval': 4.844516887228257e-09, 'curve': 6.95390028187556e-08, 'adam': 5.248226627830612e-09, 'avoid': 1.2918711699275352e-08})\n"
     ]
    }
   ],
   "source": [
    "# Check the first record\n",
    "print output[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using two (symmetric) comparison methods of your choice, pairwise compare all stripes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MRJob5_4_2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MRJob5_4_2.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from itertools import combinations\n",
    "\n",
    "class similarity(MRJob):\n",
    "    \n",
    "    \"\"\"\n",
    "    Specify some custom options so we only have to write one MRJob class for each part\n",
    "    \"\"\"\n",
    "    def configure_options(self):\n",
    "        super(similarity, self).configure_options()\n",
    "        self.add_passthrough_option('--method', default='jaccard')\n",
    "    \n",
    "    \"\"\"\n",
    "    Build the full co-occurrence matrix\n",
    "    - For each partial stripe, emit the stripe and the inverse stripe\n",
    "    - Example: (dog, {cat:2, bird:3})\n",
    "       - Emit original (dog, {cat:2, bird:3})\n",
    "       - Emit inverse (cat, {dog:2}), (bird, {dog:3})\n",
    "    \"\"\"\n",
    "    \n",
    "    def mapper_buildFullMatrix(self, _, line):\n",
    "        fields = line.strip().split('\\t')\n",
    "        word1 = fields[0]\n",
    "        stripe = eval(fields[1])\n",
    "        yield word1, stripe\n",
    "        for word2 in stripe:\n",
    "            yield word2, {word1: stripe[word2]}\n",
    "        \n",
    "    def reducer_buildFullMatrix(self, key, values):\n",
    "        stripe = {}\n",
    "        for val in values:\n",
    "            for word in val:\n",
    "                if word in stripe:\n",
    "                    stripe[word] += val[word]\n",
    "                else:\n",
    "                    stripe[word] = val[word]\n",
    "        yield key, stripe\n",
    "    \n",
    "    \"\"\"\n",
    "    Jaccard similarity\n",
    "    - We need to look at all pairs in the basis\n",
    "    - We only emit anything if one item in the pair has a value\n",
    "    - We binarize on whether the co-occurrence support is > 0\n",
    "    - Emit (word1, word2), (word1 and word2, word1, word2)\n",
    "    \"\"\"\n",
    "    \n",
    "    def mapper_Jaccard_init(self):\n",
    "        self.vocab = set()\n",
    "        with open('basisWords.txt','r') as myfile:\n",
    "            for word in myfile:\n",
    "                self.vocab.add(word.strip())\n",
    "        self.allPairs = list(combinations(sorted(self.vocab),2))\n",
    "        \n",
    "    def mapper_Jaccard(self, key, values):\n",
    "        for pair in self.allPairs:\n",
    "            i = pair[0] in values\n",
    "            j = pair[1] in values\n",
    "            if i or j:\n",
    "                yield (pair[0], pair[1]), ((i and j)+0, i+0, j+0)\n",
    "                \n",
    "    def combiner_Jaccard(self, key, values):\n",
    "        intersect = 0\n",
    "        i = 0\n",
    "        j = 0\n",
    "        for val in values:\n",
    "            intersect += val[0]\n",
    "            i += val[1]\n",
    "            j += val[2]\n",
    "        yield key, (intersect, i, j)\n",
    "        \n",
    "    def reducer_Jaccard(self, key, values):\n",
    "        intersect = 0.0\n",
    "        i = 0\n",
    "        j = 0\n",
    "        for val in values:\n",
    "            intersect += val[0]\n",
    "            i += val[1]\n",
    "            j += val[2]\n",
    "        yield key, intersect / (i + j - intersect)       \n",
    "    \n",
    "    \"\"\"\n",
    "    Multi-step pipeline definitions\n",
    "    Based on user input when calling runner function\n",
    "    \"\"\"\n",
    "    def steps(self):\n",
    "        self.method = self.options.method\n",
    "        if self.method == 'jaccard':\n",
    "            return [\n",
    "                MRStep(mapper=self.mapper_buildFullMatrix,\n",
    "                       combiner=self.reducer_buildFullMatrix,\n",
    "                       reducer=self.reducer_buildFullMatrix),\n",
    "                MRStep(mapper_init=self.mapper_Jaccard_init,\n",
    "                       mapper=self.mapper_Jaccard,\n",
    "                       combiner=self.combiner_Jaccard,\n",
    "                       reducer=self.reducer_Jaccard)\n",
    "            ]\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    similarity.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from MRJob5_4_2 import similarity\n",
    "\n",
    "def runJob5_4_2(filename, s3bucket, basis, method):\n",
    "\n",
    "    #mr_job = similarity(args=[filename, '--file', basis, '--method', method])\n",
    "    #mr_job = similarity(args=[filename, '-r', 'hadoop', '--hadoop-home', '/usr/', '--file', basis, '--method', method])\n",
    "    mr_job = similarity(args=[filename, '-r', 'emr', '--file', basis, '--no-output', '--output-dir', s3bucket,\n",
    "                              '--emr-job-flow-id', 'j-98XEIC78B7U2', '--method', method])\n",
    "    \n",
    "    output = []\n",
    "    \n",
    "    with mr_job.make_runner() as runner: \n",
    "        # Run MRJob\n",
    "        runner.run()\n",
    "\n",
    "        # Write stream_output to file\n",
    "        for line in runner.stream_output():\n",
    "            out = mr_job.parse_output_line(line)\n",
    "            output.append(out)\n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['A', 'B'], 0.6666666666666666), (['A', 'C'], 0.2), (['A', 'M'], 0.0), (['A', 'N'], 0.0), (['A', 'X'], 0.0), (['A', 'Y'], 0.0), (['A', 'Z'], 0.0), (['B', 'C'], 0.0), (['B', 'M'], 0.0), (['B', 'N'], 0.0), (['B', 'X'], 0.0), (['B', 'Y'], 0.0), (['B', 'Z'], 0.0), (['C', 'M'], 0.0), (['C', 'N'], 0.0), (['C', 'X'], 0.0), (['C', 'Y'], 0.0), (['C', 'Z'], 0.0), (['M', 'N'], 1.0), (['M', 'X'], 0.0), (['M', 'Y'], 0.0), (['M', 'Z'], 0.5), (['N', 'X'], 0.0), (['N', 'Y'], 0.0), (['N', 'Z'], 0.5), (['X', 'Y'], 1.0), (['X', 'Z'], 0.3333333333333333), (['Y', 'Z'], 0.3333333333333333)]\n"
     ]
    }
   ],
   "source": [
    "myfile = 'outputUnit.txt'\n",
    "mybasis = 'unitBasis.txt'\n",
    "\n",
    "output = runJob5_4_2(myfile, 'none', mybasis, 'jaccard')\n",
    "print output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Check some hand calculations to be sure\n",
    "\n",
    "Full co-occurrence matrix:\n",
    "\n",
    " |A|B|C|M|N|X|Y|Z\n",
    "-|-|-|-|-|-|-|-|-\n",
    "**A**|0|0|0|0|0|20|30|5\n",
    "**B**|0|0|0|0|0|100|20|0\n",
    "**C**|0|0|0|5|20|0|0|0\n",
    "**M**|0|0|5|0|0|0|0|0\n",
    "**N**|0|0|20|0|0|0|0|0\n",
    "**X**|20|100|0|0|0|0|0|0\n",
    "**Y**|30|20|0|0|0|0|0|0\n",
    "**Z**|5|0|5|0|0|0|0|0\n",
    "\n",
    "Let's check the Jaccard similarity between A and B:\n",
    "\n",
    "$\\frac{|A \\cap B|}{|A|+|B|-|A \\cap B|} = \\frac{2}{3+2-2}=\\frac{2}{3}$\n",
    "\n",
    "Let's check the Jaccard similarity between A and C:\n",
    "\n",
    "$\\frac{|A \\cap C|}{|A|+|C|-|A \\cap C|} = \\frac{1}{3+3-1}=\\frac{1}{5}$\n",
    "\n",
    "Both of these calculations check out!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.conf:Got unexpected keyword arguments: ssh_tunnel\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "ERROR:mrjob.emr:Job on job flow j-98XEIC78B7U2 failed with status WAITING: Cluster ready after last step failed.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-133-fb72c7f0a0f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'aws s3 rm --recursive {output_bucket}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrunJob5_4_2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmyfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_bucket\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmybasis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'jaccard'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-132-2f45e18a22f8>\u001b[0m in \u001b[0;36mrunJob5_4_2\u001b[1;34m(filename, s3bucket, basis, method)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mmr_job\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_runner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mrunner\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;31m# Run MRJob\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mrunner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;31m# Write stream_output to file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cloudera/miniconda2/lib/python2.7/site-packages/mrjob/runner.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    468\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Job already ran!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 470\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    471\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ran_job\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cloudera/miniconda2/lib/python2.7/site-packages/mrjob/emr.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    881\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_launch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 883\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wait_for_job_to_complete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    884\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    885\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_launch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cloudera/miniconda2/lib/python2.7/site-packages/mrjob/emr.pyc\u001b[0m in \u001b[0;36m_wait_for_job_to_complete\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1747\u001b[0m                 \u001b[1;31m# look for a Python traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1748\u001b[0m                 cause = self._find_probable_cause_of_failure(\n\u001b[1;32m-> 1749\u001b[1;33m                     step_nums, sorted(lg_step_num_mapping.values()))\n\u001b[0m\u001b[0;32m   1750\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mcause\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1751\u001b[0m                     \u001b[1;31m# log cause, and put it in exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cloudera/miniconda2/lib/python2.7/site-packages/mrjob/emr.pyc\u001b[0m in \u001b[0;36m_find_probable_cause_of_failure\u001b[1;34m(self, step_nums, lg_step_nums)\u001b[0m\n\u001b[0;32m   2013\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2014\u001b[0m                 return self._find_probable_cause_of_failure_ssh(\n\u001b[1;32m-> 2015\u001b[1;33m                     step_nums, lg_step_nums)\n\u001b[0m\u001b[0;32m   2016\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLogFetchError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2017\u001b[0m                 return self._find_probable_cause_of_failure_s3(\n",
      "\u001b[1;32m/home/cloudera/miniconda2/lib/python2.7/site-packages/mrjob/emr.pyc\u001b[0m in \u001b[0;36m_find_probable_cause_of_failure_ssh\u001b[1;34m(self, step_nums, lg_step_nums)\u001b[0m\n\u001b[0;32m   2038\u001b[0m         \u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Scanning SSH logs for probable cause of failure'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2039\u001b[0m         return best_error_from_logs(self, task_attempt_logs, step_logs,\n\u001b[1;32m-> 2040\u001b[1;33m                                     job_logs)\n\u001b[0m\u001b[0;32m   2041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_find_probable_cause_of_failure_s3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_nums\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlg_step_nums\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cloudera/miniconda2/lib/python2.7/site-packages/mrjob/logparsers.pyc\u001b[0m in \u001b[0;36mbest_error_from_logs\u001b[1;34m(fs, task_attempts, steps, jobs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[0mjobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_sorted_jobs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m     \u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parse_task_attempts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtask_attempts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    198\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cloudera/miniconda2/lib/python2.7/site-packages/mrjob/logparsers.pyc\u001b[0m in \u001b[0;36m_parse_task_attempts\u001b[1;34m(fs, logs)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'stderr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m             lines = (_parsed_error(fs, path, find_python_traceback) or\n\u001b[1;32m--> 159\u001b[1;33m                      _parsed_error(fs, path, find_hadoop_java_stack_trace))\n\u001b[0m\u001b[0;32m    160\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m             \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parsed_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfind_hadoop_java_stack_trace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cloudera/miniconda2/lib/python2.7/site-packages/mrjob/logparsers.pyc\u001b[0m in \u001b[0;36m_parsed_error\u001b[1;34m(fs, path, parse_func)\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mparse_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cloudera/miniconda2/lib/python2.7/site-packages/mrjob/parse.pyc\u001b[0m in \u001b[0;36mfind_hadoop_java_stack_trace\u001b[1;34m(lines)\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[0mWe\u001b[0m \u001b[0momit\u001b[0m \u001b[0mthe\u001b[0m \u001b[1;34m\"Error running child\"\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m     \"\"\"\n\u001b[1;32m--> 263\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    264\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\r\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Error running child\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m             \u001b[0mst_lines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cloudera/miniconda2/lib/python2.7/site-packages/mrjob/fs/base.pyc\u001b[0m in \u001b[0;36mcat\u001b[1;34m(self, path_glob)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_glob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;34m\"\"\"cat all files matching **path_glob**, decompressing if necessary\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_glob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cat_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cloudera/miniconda2/lib/python2.7/site-packages/mrjob/fs/ssh.pyc\u001b[0m in \u001b[0;36mls\u001b[1;34m(self, path_glob)\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_glob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mSSH_URI_RE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_glob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ssh_ls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_glob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cloudera/miniconda2/lib/python2.7/site-packages/mrjob/fs/ssh.pyc\u001b[0m in \u001b[0;36m_ssh_ls\u001b[1;34m(self, uri)\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ec2_key_pair_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'filesystem_path'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mssh_key_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m         )\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cloudera/miniconda2/lib/python2.7/site-packages/mrjob/ssh.pyc\u001b[0m in \u001b[0;36mssh_ls\u001b[1;34m(ssh_bin, address, ec2_key_pair_file, path, keyfile)\u001b[0m\n\u001b[0;32m    176\u001b[0m     out = check_output(*ssh_run_with_recursion(\n\u001b[0;32m    177\u001b[0m         \u001b[0mssh_bin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maddress\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mec2_key_pair_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeyfile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m         ['find', '-L', path, '-type', 'f']))\n\u001b[0m\u001b[0;32m    179\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m'No such file or directory'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No such file or directory: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cloudera/miniconda2/lib/python2.7/site-packages/mrjob/ssh.pyc\u001b[0m in \u001b[0;36mssh_run_with_recursion\u001b[1;34m(ssh_bin, address, ec2_key_pair_file, keyfile, cmd_args)\u001b[0m\n\u001b[0;32m    109\u001b[0m         ]\n\u001b[0;32m    110\u001b[0m         return ssh_run(ssh_bin, host1, ec2_key_pair_file,\n\u001b[1;32m--> 111\u001b[1;33m                        more_args + list(cmd_args))\n\u001b[0m\u001b[0;32m    112\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mssh_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mssh_bin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maddress\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mec2_key_pair_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmd_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cloudera/miniconda2/lib/python2.7/site-packages/mrjob/ssh.pyc\u001b[0m in \u001b[0;36mssh_run\u001b[1;34m(ssh_bin, address, ec2_key_pair_file, cmd_args, stdin)\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Run SSH command: %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstdin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstdin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cloudera/miniconda2/lib/python2.7/subprocess.pyc\u001b[0m in \u001b[0;36mcommunicate\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    797\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    798\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 799\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_communicate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    800\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    801\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cloudera/miniconda2/lib/python2.7/subprocess.pyc\u001b[0m in \u001b[0;36m_communicate\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m   1407\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1408\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0m_has_poll\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m                 \u001b[0mstdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_communicate_with_poll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1410\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m                 \u001b[0mstdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_communicate_with_select\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cloudera/miniconda2/lib/python2.7/subprocess.pyc\u001b[0m in \u001b[0;36m_communicate_with_poll\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m   1461\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mfd2file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1462\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1463\u001b[1;33m                     \u001b[0mready\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpoller\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1464\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mselect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1465\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEINTR\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "myfile = 's3://ms-w261-hw05/hw5_4_1/part-00000'\n",
    "mybasis = 'basisWords.txt'\n",
    "output_bucket = 's3://ms-w261-hw05/hw5_4_2'\n",
    "\n",
    "!aws s3 rm --recursive {output_bucket}\n",
    "\n",
    "output = runJob5_4_2(myfile, output_bucket, mybasis, 'jaccard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using configs in /etc/mrjob.conf\n",
      "using existing scratch bucket mrjob-ac40f1afcc0b86ce\n",
      "using s3://mrjob-ac40f1afcc0b86ce/tmp/ as our scratch dir on S3\n",
      "Terminated job flow j-98XEIC78B7U2\n"
     ]
    }
   ],
   "source": [
    "# Terminate job flow so we don't rack up AWS expenses\n",
    "!python -m mrjob.tools.emr.terminate_job_flow j-98XEIC78B7U2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
