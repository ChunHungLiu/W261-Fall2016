{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASCI W261: Machine Learning at Scale\n",
    "## Assignment Week 5\n",
    "Miki Seltzer (miki.seltzer@berkeley.edu)<br>\n",
    "W261-2, Spring 2016<br>\n",
    "Submission: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## HW5.0:\n",
    "### What is a data warehouse?\n",
    "A data warehouse is a repository for one or multiple data sources. Data warehouses can contain relational databases.\n",
    "\n",
    "### What is a star schema?\n",
    "A star schema relates multiple fact and dimension tables, and is similar to the snowflake schema. In both schemas, fact tables are referenced by dimension tables (one or multiple). However, star schemas are denormalized, whereas snowflake schemas are normalized.\n",
    "\n",
    "### When is it used?\n",
    "A star schema is used to organize the meta data of a relational database, such as which tables can be joined, and the keys on which they can be joined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW5.1:\n",
    "### In the database world, what is 3NF?\n",
    "3NF is shorthand for third normal form. A table is in third normal form if the following conditions hold:\n",
    "- The table is already in second normal form\n",
    "- Non-prime attributes of the table are non-transitively dependent on every key in the table\n",
    "\n",
    "### Does machine learning use data in 3NF?\n",
    "ML can, but does not always use data in 3NF.\n",
    "\n",
    "### If so, why?\n",
    "3NF can save a significant amount of disk space because data duplication is avoided. Additionally, if data is denormalized, then fields in the data set might be related to each other and create dependencies. This may be problematic if we are using algorithms that require independent features.\n",
    "\n",
    "### In what form does ML consume data?\n",
    "Typically, ML requires all data to be fed into an algorithm to be collected into a single source. Thus, the easiest way for ML to ingest data is for it to be denormalized.\n",
    "\n",
    "### Why would one use log files that are denormalized?\n",
    "If one needs to perform real-time analysis on log files, it may be too time consuming to join normalized log files with other tables. If log files are denormalized, they may not need any further processing (joins) to be fed into other steps of a pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.2: Using MRJob, implement a hashside join (memory-backed map-side) for left, right and inner joins. Run your code on the  data used in HW 4.4.\n",
    "\n",
    "### Justify which table you chose as left table in this hashside join\n",
    "The two tables used were:\n",
    "- **anonymous-msweb-preprocess.data:** The log file of visitors and each page that they visited (processed rows prefixed by 'C' or 'V')\n",
    "- **attributes.csv:** The page ID, page name and URL of each page (prefixed by 'A' in the original data)\n",
    "\n",
    "The attributes.csv file was very small (only 294 lines), so this is the file that I chose to store in memory. This became my **right** table.\n",
    "\n",
    "The log file was much larger, so I streamed through this file, and used it as the **left** table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We will need these so we can reload modules as we modify them\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapSideJoin.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapSideJoin.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    " \n",
    "class join(MRJob):\n",
    "    \n",
    "    # Specify some custom options so we only have to write one MRJob class for each join\n",
    "    def configure_options(self):\n",
    "        super(join, self).configure_options()\n",
    "        self.add_passthrough_option('--joinType', default='inner', )\n",
    "    \n",
    "    # Store attributes.csv into memory\n",
    "    #  - account for multiple occurrences of keys\n",
    "    #  - self.pages is dict with a list of [pageName, pageURL] pairs\n",
    "    # Set joinType variable\n",
    "    def mapper_init(self):\n",
    "        self.pages = {}\n",
    "        with open('attributes.csv','r') as myfile:\n",
    "            for line in myfile:\n",
    "                fields = line.strip().split(',')\n",
    "                if fields[0] not in self.pages:\n",
    "                    self.pages[fields[0]]=[]\n",
    "                self.pages[fields[0]].append([fields[1], fields[2]])\n",
    "        self.joinType = self.options.joinType\n",
    "        self.seenRight = set()\n",
    "    \n",
    "    # RIGHT table is stored in memory (self.pages)\n",
    "    # LEFT table is streamed\n",
    "    # We need so keep track of which RIGHT keys we have seen\n",
    "    def mapper(self, _, line):\n",
    "        fields = line.split(',')\n",
    "        key = fields[0]\n",
    "        self.seenRight.add(key)\n",
    "        if key in self.pages:\n",
    "            for i in self.pages[key]:\n",
    "                yield key, (fields[1], fields[2], i[0], i[1])\n",
    "        elif self.joinType == 'left':\n",
    "            yield key, (fields[1], fields[2], None, None)\n",
    "\n",
    "    # We need to emit all of the RIGHT keys that we never saw while streaming through LEFT\n",
    "    # We will need to deduplicate these in the reducer in case we have multiple mappers\n",
    "    def mapper_final(self):\n",
    "        if self.joinType == 'right':\n",
    "            for key in self.pages:\n",
    "                if key not in self.seenRight:\n",
    "                    for value in self.pages[key]:\n",
    "                        yield key, (None, None, value[0], value[1])\n",
    "    \n",
    "    # Need to persist variables\n",
    "    def reducer_init(self):\n",
    "        self.joinType = self.options.joinType\n",
    "    \n",
    "    # We need to unpack and emit each record\n",
    "    # We also need to do some work emitting records for the right join\n",
    "    def reducer(self, key, values):\n",
    "        emptyRight = True\n",
    "        for val in values:\n",
    "            if self.joinType == 'inner' or self.joinType == 'left':\n",
    "                yield key, val\n",
    "            elif self.joinType == 'right':\n",
    "                if val[:2] != [None]*2:\n",
    "                    emptyRight = False\n",
    "                    yield key, val\n",
    "                else: emptyRecord = val\n",
    "        if emptyRight and self.joinType == 'right':\n",
    "            yield key, emptyRecord\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    join.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    }
   ],
   "source": [
    "from mapSideJoin import join\n",
    "\n",
    "def runJoin(joinType):\n",
    "\n",
    "    mr_job = join(args=['TopVisitors.txt', '--file', 'attributes.csv', '--joinType', joinType])\n",
    "    output = []\n",
    "\n",
    "    with mr_job.make_runner() as runner: \n",
    "        # Run MRJob\n",
    "        runner.run()\n",
    "\n",
    "        # Write stream_output to file\n",
    "        for line in runner.stream_output():\n",
    "            output.append(mr_job.parse_output_line(line))\n",
    "    \n",
    "    return output\n",
    "            \n",
    "outInner = runJoin('inner')\n",
    "outLeft = runJoin('left')\n",
    "outRight = runJoin('right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows resulting from join type:\n",
      "\n",
      "inner   285\n",
      "left    285\n",
      "right   294\n"
     ]
    }
   ],
   "source": [
    "print \"Rows resulting from join type:\\n\"\n",
    "for joinType in ['inner', 'left', 'right']:\n",
    "    if joinType == 'inner': out = outInner\n",
    "    elif joinType == 'left': out = outLeft\n",
    "    elif joinType == 'right': out = outRight\n",
    "    \n",
    "    print \"{:7s}{:>4,d}\".format(joinType, len(out))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW5.3: Do some EDA on this data set using MRJob:\n",
    "- Longest 5-gram (number of characters)\n",
    "- Top 10 most frequent words (count), i.e., unigrams\n",
    "- Most/least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency\n",
    "- Distribution of 5-gram sizes (counts) sorted in decreasing order of relative frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "docs = {}\n",
    "docs['A'] = ['X']*20\n",
    "docs['A'].extend(['Y']*30)\n",
    "docs['A'].extend(['Z']*5)\n",
    "docs['B'] = ['X']*100\n",
    "docs['B'].extend(['Y']*20)\n",
    "docs['C'] = ['M']*5\n",
    "docs['C'].extend(['N']*20)\n",
    "docs['C'].extend(['Z']*5)\n",
    "\n",
    "\n",
    "# Create the file for unit testing\n",
    "with open('unitTest.txt', 'w') as myfile:\n",
    "    outWriter = csv.writer(myfile)\n",
    "    for doc in docs:\n",
    "        row = [doc]\n",
    "        row.extend(docs[doc])\n",
    "        outWriter.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MRJob5_3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MRJob5_3.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class job(MRJob):\n",
    "    \n",
    "    # Specify some custom options so we only have to write one MRJob class for each part\n",
    "    def configure_options(self):\n",
    "        super(job, self).configure_options()\n",
    "        self.add_passthrough_option('--part', default='1')\n",
    "    \n",
    "    \"\"\"\n",
    "    Find the longest 5-gram\n",
    "    - In this case, in each mapper, we only need to store the length of the longest 5-gram we have seen\n",
    "    - After the mapper has run, we emit the longest 5-gram from this mapper\n",
    "    - All results will be sent to the same reducer (we specify this)\n",
    "    - Then we loop through the records in the reducer and emit the remaining longest 5-gram\n",
    "    \"\"\"\n",
    "    \n",
    "    def mapper_longest5Gram_init(self):\n",
    "        self.maxLength = 0\n",
    "        self.longest5Gram = None\n",
    "    \n",
    "    def mapper_longest5Gram(self, _, line):\n",
    "        fields = line.strip().split('\\t')\n",
    "        if len(fields[0]) > self.maxLength: \n",
    "            self.maxLength = len(fields[0])\n",
    "            self.longest5Gram = fields[0]\n",
    "            \n",
    "    def mapper_longest5Gram_final(self):\n",
    "        yield self.longest5Gram, self.maxLength\n",
    "     \n",
    "    def reducer_longest5Gram_init(self):\n",
    "        self.maxLength = 0\n",
    "        self.longest5Gram = None\n",
    "    \n",
    "    def reducer_longest5Gram(self, key, values):\n",
    "        for val in values:\n",
    "            if val > self.maxLength:\n",
    "                self.maxLength = val\n",
    "                self.longest5Gram = key\n",
    "        \n",
    "    def reducer_longest5Gram_final(self):\n",
    "        yield self.maxLength, self.longest5Gram\n",
    "    \n",
    "    \"\"\"\n",
    "    Top 10 most frequent words\n",
    "    - This is our standard word count\n",
    "    - Loop through each word in the 5-gram and emit (word, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    def mapper_topWords(self, _, line):\n",
    "        fields = line.strip().split('\\t')\n",
    "        words = fields[0].lower().split()\n",
    "        count, pages_count, books_count = int(fields[1]), int(fields[2]), int(fields[3])\n",
    "        for word in words:\n",
    "            self.increment_counter('total', 'words', 1)\n",
    "            yield word, 1\n",
    "        \n",
    "    def combiner_topWords(self, key, values):\n",
    "        yield key, sum(values)\n",
    "        \n",
    "    def reducer_topWords(self, key, values):\n",
    "        yield key, sum(values)\n",
    " \n",
    "    \"\"\"\n",
    "    Densely appearing words\n",
    "    - For each word, emit count and pages_count\n",
    "    - Combiner sums count and pages_count\n",
    "    - Reducer sums count and pages_count, then emits count/pages_count\n",
    "    \"\"\"\n",
    "    \n",
    "    def mapper_denseWords(self, _, line):\n",
    "        fields = line.strip().split('\\t')\n",
    "        words = fields[0].lower().split()\n",
    "        count, pages_count, books_count = int(fields[1]), int(fields[2]), int(fields[3])\n",
    "        for word in words:\n",
    "            yield word, (count, pages_count)\n",
    "        \n",
    "    def combiner_denseWords(self, key, values):\n",
    "        count, pages_count = 0.0, 0.0\n",
    "        for val in values:\n",
    "            count += val[0]\n",
    "            pages_count += val[1]\n",
    "        yield key, (count, pages_count)\n",
    "        \n",
    "    def reducer_denseWords(self, key, values):\n",
    "        count, pages_count = 0.0, 0.0\n",
    "        for val in values:\n",
    "            count += val[0]\n",
    "            pages_count += val[1]\n",
    "        yield key, count/pages_count\n",
    "\n",
    "    \"\"\"\n",
    "    Frequent5-grams\n",
    "    - Use count to determine the most frequent 5-gram\n",
    "    - Sum counts in combiner and reducer\n",
    "    \"\"\"\n",
    "        \n",
    "    def mapper_frequent5Gram(self, _, line):\n",
    "        fields = line.strip().split('\\t')\n",
    "        yield fields[0].lower(), float(fields[1])\n",
    "        \n",
    "    def combiner_frequent5Gram(self, key, values):\n",
    "        yield key, sum(values)\n",
    "        \n",
    "    def reducer_frequent5Gram(self, key, values):\n",
    "        yield key, sum(values)\n",
    "        \n",
    "    \"\"\"\n",
    "    Sorting functions\n",
    "    - We need these to get the top and bottom values\n",
    "    - Utilize only one reducer instead of writing a custom partitioner\n",
    "    \"\"\"\n",
    "    def mapper_sort(self, key, value):\n",
    "        yield float(value), key\n",
    "        \n",
    "    def reducer_sort_init(self):\n",
    "        self.count = 0\n",
    "    \n",
    "    def reducer_top10(self, key, values):\n",
    "        for val in values:\n",
    "            if self.count < 10:\n",
    "                yield key, val\n",
    "                self.count += 1\n",
    "                \n",
    "    def reducer_top100(self, key, values):\n",
    "        for val in values:\n",
    "            if self.count < 100:\n",
    "                yield key, val\n",
    "                self.count += 1\n",
    "                \n",
    "    def reducer_top10000(self, key, values):\n",
    "        for val in values:\n",
    "            if self.count < 10000:\n",
    "                yield key, val\n",
    "                self.count += 1\n",
    "    \n",
    "    \"\"\"\n",
    "    Multi-step pipeline definitions\n",
    "    Based on user input when calling runner function\n",
    "    \"\"\"\n",
    "    def steps(self):\n",
    "        self.part = self.options.part\n",
    "        if self.part == '1':\n",
    "            return [\n",
    "                MRStep(mapper_init=self.mapper_longest5Gram_init,\n",
    "                       mapper=self.mapper_longest5Gram,\n",
    "                       mapper_final=self.mapper_longest5Gram_final,\n",
    "                       reducer_init=self.reducer_longest5Gram_init,\n",
    "                       reducer=self.reducer_longest5Gram,\n",
    "                       reducer_final=self.reducer_longest5Gram_final,\n",
    "                       jobconf={'mapred.reduce.tasks': 1})\n",
    "            ]\n",
    "        elif self.part == '2':\n",
    "            return [\n",
    "                MRStep(mapper=self.mapper_topWords,\n",
    "                       combiner=self.combiner_topWords,\n",
    "                       reducer=self.reducer_topWords),\n",
    "                MRStep(mapper=self.mapper_sort,\n",
    "                       reducer_init=self.reducer_sort_init,\n",
    "                       reducer=self.reducer_top10,\n",
    "                       jobconf={'mapred.output.key.comparator.class':'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                                'mapred.text.key.partitioner.options':'-k1,1',\n",
    "                                'stream.num.map.output.key.fields':1,\n",
    "                                'mapred.text.key.comparator.options':'-k1,1nr',\n",
    "                                'mapred.reduce.tasks': 1})\n",
    "            ]\n",
    "        elif self.part == '3':\n",
    "            return [\n",
    "                MRStep(mapper=self.mapper_denseWords,\n",
    "                       combiner=self.combiner_denseWords,\n",
    "                       reducer=self.reducer_denseWords),\n",
    "                MRStep(mapper=self.mapper_sort,\n",
    "                       reducer_init=self.reducer_sort_init,\n",
    "                       reducer=self.reducer_top100,\n",
    "                       jobconf={'mapred.output.key.comparator.class':'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                                'mapred.text.key.partitioner.options':'-k1,1',\n",
    "                                'stream.num.map.output.key.fields':1,\n",
    "                                'mapred.text.key.comparator.options':'-k1,1nr',\n",
    "                                'mapred.reduce.tasks': 1})\n",
    "            ]\n",
    "        elif self.part == '4':\n",
    "            return [\n",
    "                MRStep(mapper=self.mapper_frequent5Gram,\n",
    "                       combiner=self.combiner_frequent5Gram,\n",
    "                       reducer=self.reducer_frequent5Gram),\n",
    "                MRStep(mapper=self.mapper_sort,\n",
    "                       reducer_init=self.reducer_sort_init,\n",
    "                       reducer=self.reducer_top100,\n",
    "                       jobconf={'mapred.output.key.comparator.class':'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                                'mapred.text.key.partitioner.options':'-k1,1',\n",
    "                                'stream.num.map.output.key.fields':1,\n",
    "                                'mapred.text.key.comparator.options':'-k1,1nr',\n",
    "                                'mapred.reduce.tasks': 1})\n",
    "            ]\n",
    "        elif self.part == '5':\n",
    "            return [\n",
    "                MRStep(mapper=self.mapper_topWords,\n",
    "                       combiner=self.combiner_topWords,\n",
    "                       reducer=self.reducer_topWords),\n",
    "                MRStep(mapper=self.mapper_sort,\n",
    "                       reducer_init=self.reducer_sort_init,\n",
    "                       reducer=self.reducer_top10000,\n",
    "                       jobconf={'mapred.output.key.comparator.class':'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                                'mapred.text.key.partitioner.options':'-k1,1',\n",
    "                                'stream.num.map.output.key.fields':1,\n",
    "                                'mapred.text.key.comparator.options':'-k1,1nr',\n",
    "                                'mapred.reduce.tasks': 1})\n",
    "            ]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using configs in /etc/mrjob.conf\n",
      "using existing scratch bucket mrjob-ac40f1afcc0b86ce\n",
      "using s3://mrjob-ac40f1afcc0b86ce/tmp/ as our scratch dir on S3\n",
      "Creating persistent job flow to run several jobs in...\n",
      "creating tmp directory /tmp/no_script.cloudera.20160214.231258.206908\n",
      "writing master bootstrap script to /tmp/no_script.cloudera.20160214.231258.206908/b.py\n",
      "Copying non-input files into s3://mrjob-ac40f1afcc0b86ce/tmp/no_script.cloudera.20160214.231258.206908/files/\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Creating Elastic MapReduce job flow\n",
      "Job flow created with ID: j-98XEIC78B7U2\n",
      "j-98XEIC78B7U2\n"
     ]
    }
   ],
   "source": [
    "# Create job flow so that we don't need to keep spinning up clusters\n",
    "!python -m mrjob.tools.emr.create_job_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from MRJob5_3 import job\n",
    "\n",
    "def runJob5_3(filename, part, s3bucket):\n",
    "\n",
    "    #mr_job = job(args=[filename, '--part', str(part)])\n",
    "    #mr_job = job(args=[filename, '--part', str(part), '-r', 'hadoop', '--hadoop-home', '/usr/'])\n",
    "    mr_job = job(args=[filename, '--part', str(part), '--no-output', '--output-dir', s3bucket,\n",
    "                       '-r', 'emr', '--emr-job-flow-id', 'j-98XEIC78B7U2'])\n",
    "    \n",
    "    output = []\n",
    "\n",
    "    with mr_job.make_runner() as runner: \n",
    "        # Run MRJob\n",
    "        runner.run()\n",
    "\n",
    "        # Write stream_output to file\n",
    "        for line in runner.stream_output():\n",
    "            output.append(mr_job.parse_output_line(line))\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_output(output, part):\n",
    "    for item in output:\n",
    "        if part == 3:\n",
    "            print '{:5.5f}  {:<100s}'.format(item[0], item[1])\n",
    "        else:\n",
    "            print '{:11,d}  {:<100s}'.format(int(item[0]), item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        159  AIOPJUMRXUYVASLYHYPSIBEMAPODIKR UFRYDIUUOLBIGASUAURUSREXLISNAYE RNOONDQSRUNSUBUNOUGRABBERYAIRTC UTAHRAPTOREDILEIPMILBDUMMYUVERI SYEVRAHVELOCYALLOSAURUSLINROTSR\n"
     ]
    }
   ],
   "source": [
    "myfile = 's3://filtered-5grams/'\n",
    "#myfile = './filtered-5Grams/short-5gram.txt'\n",
    "\n",
    "output_bucket = 's3://ms-w261-hw05/hw5_3a'\n",
    "\n",
    "!aws s3 rm --recursive {output_bucket}\n",
    "\n",
    "part = 1\n",
    "output = runJob5_3(myfile, part, output_bucket)\n",
    "format_output(output, part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 27,502,442  the                                                                                                 \n",
      " 18,191,779  of                                                                                                  \n",
      " 12,075,971  to                                                                                                  \n",
      "  7,881,239  in                                                                                                  \n",
      "  7,853,465  a                                                                                                   \n",
      "  7,767,900  and                                                                                                 \n",
      "  4,316,884  that                                                                                                \n",
      "  3,847,383  is                                                                                                  \n",
      "  3,288,731  be                                                                                                  \n",
      "  2,763,613  for                                                                                                 \n"
     ]
    }
   ],
   "source": [
    "output_bucket = 's3://ms-w261-hw05/hw5_3b'\n",
    "\n",
    "!aws s3 rm --recursive {output_bucket}\n",
    "\n",
    "part = 2\n",
    "output = runJob5_3(myfile, part, output_bucket)\n",
    "format_output(output, part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.conf:Got unexpected keyword arguments: ssh_tunnel\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    }
   ],
   "source": [
    "output_bucket = 's3://ms-w261-hw05/hw5_3c'\n",
    "\n",
    "!aws s3 rm --recursive {output_bucket}\n",
    "\n",
    "part = 3\n",
    "output = runJob5_3(myfile, part, output_bucket)\n",
    "format_output(output, part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW5.4\n",
    "### 1. Build stripes of word co-occurrence for the 1,000 words ranked 9,001 - 10,000\n",
    "### 2. Using two (symmetric) comparison methods of your choice, pairwise compare all stripes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.compat:Detected hadoop configuration property names that do not match hadoop version 2.6.0:\n",
      "The have been translated as follows\n",
      " mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "mapred.reduce.tasks: mapreduce.job.reduces\n",
      "mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n"
     ]
    }
   ],
   "source": [
    "myfile = './filtered-5Grams/googlebooks-eng-all-5gram-20090715-0-filtered.txt'\n",
    "#myfile = './filtered-5Grams/short-5gram.txt'\n",
    "\n",
    "part = 5\n",
    "output = runJob5_3(myfile, part)\n",
    "\n",
    "with open('basisWords.txt', 'w') as myfile:\n",
    "    for i in output[:1000]:\n",
    "        myfile.write(i[1]+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MRJob5_4.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MRJob5_4.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class stripes(MRJob):\n",
    "    \n",
    "    \"\"\"\n",
    "    Build stripes\n",
    "    - Read in basis words from basisWords.txt\n",
    "    - Emit stripes where the key and each value's key is in the basis\n",
    "    \"\"\"\n",
    "    \n",
    "    def mapper_buildStripe_init(self):\n",
    "        self.vocab = set()\n",
    "        with open('basisWords.txt','r') as myfile:\n",
    "            for word in myfile:\n",
    "                self.vocab.add(word.strip())\n",
    "        \n",
    "    def mapper_buildStripe(self, _, line):\n",
    "        fields = line.strip().split('\\t')\n",
    "        words = fields[0].lower().split()\n",
    "        wordList = sorted(list(set(words)))\n",
    "        for index1 in range(len(wordList)-1):\n",
    "            stripe = {}\n",
    "            if wordList[index1] in self.vocab:\n",
    "                for index2 in range(index1+1,len(wordList)):\n",
    "                    if wordList[index2] in self.vocab:\n",
    "                        stripe[wordList[index2]] = 1\n",
    "            if len(stripe) > 0:\n",
    "                yield wordList[index1], stripe\n",
    "            \n",
    "    def combiner_buildStripe(self, key, values):\n",
    "        stripe = {}\n",
    "        for val in values:\n",
    "            for word in val:\n",
    "                if word in stripe:\n",
    "                    stripe[word] += val[word]\n",
    "                else:\n",
    "                    stripe[word] = val[word]\n",
    "        yield key, stripe\n",
    "        \n",
    "    def reducer_buildStripe(self, key, values):\n",
    "        stripe = {}\n",
    "        for val in values:\n",
    "            for word in val:\n",
    "                if word in stripe:\n",
    "                    stripe[word] += val[word]\n",
    "                else:\n",
    "                    stripe[word] = val[word]\n",
    "        yield key, stripe\n",
    "    \n",
    "            \n",
    "        \n",
    "    \"\"\"\n",
    "    Multi-step pipeline definitions\n",
    "    Based on user input when calling runner function\n",
    "    \"\"\"\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init=self.mapper_buildStripe_init,\n",
    "                   mapper=self.mapper_buildStripe,\n",
    "                   combiner=self.combiner_buildStripe,\n",
    "                   reducer=self.reducer_buildStripe,\n",
    "                   jobconf={'mapred.reduce.tasks': 2})\n",
    "        ]\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    stripes.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from MRJob5_4 import stripes\n",
    "\n",
    "def runJob5_4(filename):\n",
    "\n",
    "    mr_job = stripes(args=[filename, '--file', 'basisWords.txt'])\n",
    "    #mr_job = stripes(args=[filename, '-r', 'hadoop', '--hadoop-home', '/usr/', '--file', 'basisWords.txt'])\n",
    "    output = []\n",
    "\n",
    "    with mr_job.make_runner() as runner: \n",
    "        # Run MRJob\n",
    "        runner.run()\n",
    "\n",
    "        # Write stream_output to file\n",
    "        for line in runner.stream_output():\n",
    "            output.append(mr_job.parse_output_line(line))\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['among', 'another', 'often', 'certain', 'mind', 'states', 'known', 'done', 'something', 'human', 'sense', 'seen', 'subject', 'information', 'united', 'want', 'god', 'away', 'question', 'least', 'better', 'enough', 'going', 'development', 'interest', 'themselves', 'ever', 'body', 'took', 'here', 'hand', 'water', 'effect', 'cannot', 'nothing', 'necessary', 'law', 'change', 'brought', 'kind', 'off', 'whether', 'study', 'value', 'person', 'common', 'become', 'went', 'side', 'fact', 'once'])\n",
      "[('among', {'human': 2, 'brought': 1, 'something': 1, 'want': 1, 'sense': 1, 'subject': 1, 'god': 2, 'least': 3, 'better': 1, 'going': 1, 'interest': 1, 'themselves': 9, 'ever': 3, 'body': 1, 'took': 2, 'known': 5, 'law': 1, 'states': 2, 'kind': 1, 'whether': 2, 'value': 1, 'common': 7, 'become': 1, 'side': 1}), ('another', {'body': 1, 'often': 2, 'certain': 2, 'nothing': 1, 'done': 1, 'want': 2, 'sense': 1, 'subject': 2, 'god': 1, 'away': 1, 'question': 2, 'going': 1, 'themselves': 3, 'ever': 3, 'development': 2, 'human': 3, 'effect': 2, 'here': 2, 'took': 1, 'cannot': 1, 'known': 1, 'law': 1, 'change': 1, 'kind': 5, 'necessary': 1, 'study': 1, 'value': 1, 'person': 10, 'common': 1, 'become': 1, 'went': 2, 'side': 4, 'once': 1}), ('away', {'often': 2, 'certain': 1, 'mind': 2, 'states': 1, 'done': 3, 'want': 1, 'subject': 1, 'god': 4, 'question': 1, 'enough': 3, 'going': 5, 'ever': 1, 'body': 1, 'took': 6, 'hand': 2, 'water': 1, 'cannot': 2, 'nothing': 2, 'off': 1, 'whether': 1, 'went': 10, 'fact': 1}), ('become', {'kind': 1, 'united': 1, 'necessary': 2, 'once': 1, 'god': 1, 'question': 2, 'effect': 1, 'least': 2, 'better': 1, 'person': 2, 'enough': 2, 'common': 2, 'sense': 1, 'known': 2, 'themselves': 1, 'law': 2, 'ever': 1, 'subject': 3}), ('better', {'known': 10, 'going': 1, 'off': 11, 'often': 1, 'something': 5, 'fact': 2, 'study': 1, 'mind': 1, 'least': 1, 'person': 1, 'cannot': 1, 'done': 2, 'here': 1, 'want': 1, 'sense': 2, 'nothing': 7, 'seen': 2, 'themselves': 2, 'ever': 2, 'change': 1, 'once': 1}), ('body', {'information': 1, 'united': 1, 'mind': 5, 'took': 1, 'brought': 1, 'person': 2, 'enough': 1, 'cannot': 3, 'common': 2, 'human': 12, 'water': 3, 'seen': 1, 'law': 1, 'change': 2, 'subject': 1}), ('brought', {'states': 1, 'united': 1, 'often': 2, 'once': 3, 'certain': 2, 'study': 1, 'question': 2, 'here': 2, 'hand': 2, 'water': 2, 'enough': 1, 'cannot': 1, 'least': 2, 'human': 1, 'sense': 1, 'themselves': 3, 'law': 2, 'mind': 5, 'subject': 2}), ('cannot', {'often': 1, 'certain': 1, 'mind': 2, 'known': 2, 'done': 4, 'something': 1, 'human': 1, 'seen': 3, 'god': 2, 'question': 1, 'least': 1, 'themselves': 4, 'ever': 1, 'effect': 1, 'here': 6, 'hand': 1, 'nothing': 1, 'change': 1, 'kind': 1, 'off': 1, 'whether': 1, 'value': 2, 'person': 2, 'fact': 2}), ('certain', {'often': 1, 'mind': 2, 'states': 2, 'known': 2, 'human': 1, 'sense': 2, 'subject': 4, 'information': 3, 'want': 2, 'least': 1, 'enough': 1, 'interest': 2, 'took': 1, 'water': 1, 'nothing': 3, 'law': 1, 'change': 1, 'kind': 4, 'off': 2, 'whether': 3, 'common': 1, 'went': 1, 'fact': 4, 'once': 2}), ('change', {'development': 1, 'necessary': 3, 'mind': 3, 'effect': 2, 'least': 1, 'states': 1, 'person': 1, 'enough': 1, 'value': 4, 'want': 2, 'nothing': 1, 'seen': 1, 'themselves': 1, 'law': 1, 'ever': 1, 'subject': 1}), ('common', {'development': 1, 'united': 1, 'necessary': 1, 'often': 2, 'human': 1, 'took': 1, 'least': 2, 'hand': 1, 'states': 1, 'person': 1, 'enough': 5, 'done': 1, 'value': 2, 'interest': 5, 'sense': 13, 'nothing': 2, 'law': 23, 'side': 1, 'subject': 1}), ('development', {'information': 2, 'united': 1, 'study': 1, 'took': 2, 'done': 1, 'human': 6, 'sense': 1, 'law': 1, 'side': 1, 'fact': 1, 'subject': 2}), ('done', {'kind': 2, 'necessary': 1, 'often': 3, 'whether': 2, 'god': 2, 'study': 3, 'mind': 1, 'effect': 1, 'value': 1, 'here': 2, 'person': 2, 'enough': 3, 'subject': 1, 'least': 3, 'nothing': 11, 'law': 1, 'ever': 4, 'something': 8}), ('effect', {'whether': 1, 'study': 3, 'took': 1, 'least': 2, 'human': 1, 'law': 1, 'side': 2}), ('enough', {'information': 4, 'kind': 3, 'off': 1, 'often': 4, 'whether': 2, 'god': 1, 'here': 2, 'water': 2, 'person': 1, 'states': 1, 'going': 1, 'least': 5, 'human': 1, 'sense': 1, 'known': 1, 'seen': 2, 'themselves': 1, 'law': 1, 'ever': 1, 'subject': 1}), ('ever', {'known': 4, 'kind': 1, 'united': 1, 'off': 1, 'whether': 2, 'god': 1, 'mind': 2, 'question': 1, 'hand': 1, 'person': 2, 'going': 3, 'something': 1, 'want': 2, 'nothing': 3, 'seen': 8, 'went': 1, 'once': 1}), ('fact', {'known': 5, 'information': 1, 'kind': 1, 'necessary': 1, 'often': 3, 'something': 2, 'whether': 1, 'mind': 1, 'question': 3, 'took': 2, 'least': 2, 'value': 2, 'person': 1, 'going': 1, 'here': 2, 'human': 1, 'sense': 1, 'nothing': 2, 'themselves': 2, 'law': 1, 'once': 1}), ('god', {'often': 1, 'whether': 1, 'question': 2, 'mind': 1, 'took': 1, 'here': 1, 'hand': 1, 'person': 1, 'human': 4, 'sense': 2, 'nothing': 1, 'seen': 1}), ('going', {'off': 6, 'whether': 1, 'study': 1, 'question': 1, 'took': 1, 'here': 3, 'person': 1, 'something': 2, 'interest': 1, 'nothing': 2, 'law': 1, 'once': 1}), ('hand', {'off': 4, 'often': 1, 'took': 5, 'person': 1, 'want': 1, 'seen': 1, 'went': 2, 'side': 4}), ('here', {'kind': 1, 'necessary': 2, 'whether': 2, 'mind': 2, 'question': 3, 'least': 1, 'something': 2, 'want': 1, 'sense': 2, 'nothing': 4, 'off': 1, 'once': 1, 'side': 2, 'subject': 1}), ('human', {'kind': 1, 'united': 1, 'necessary': 1, 'study': 1, 'mind': 17, 'value': 1, 'person': 1, 'least': 1, 'something': 1, 'interest': 4, 'sense': 3, 'nothing': 3, 'seen': 3, 'once': 1, 'subject': 4}), ('information', {'states': 1, 'necessary': 3, 'whether': 4, 'question': 1, 'least': 1, 'water': 1, 'person': 1, 'value': 2, 'want': 3, 'nothing': 1, 'themselves': 2, 'once': 1, 'subject': 1}), ('interest', {'kind': 2, 'least': 1, 'necessary': 1, 'often': 1, 'question': 4, 'took': 9, 'value': 2, 'something': 1, 'themselves': 2, 'law': 1, 'subject': 1}), ('kind', {'whether': 1, 'study': 1, 'mind': 1, 'took': 1, 'value': 1, 'water': 1, 'person': 3, 'want': 2, 'nothing': 1, 'seen': 1, 'law': 2, 'once': 1}), ('known', {'once': 2, 'whether': 5, 'mind': 1, 'water': 1, 'person': 2, 'something': 1, 'nothing': 5, 'law': 2, 'subject': 1}), ('law', {'united': 1, 'necessary': 1, 'question': 1, 'study': 4, 'mind': 1, 'something': 1, 'seen': 1, 'themselves': 1, 'side': 1, 'subject': 2}), ('least', {'took': 1, 'states': 1, 'person': 1, 'something': 1, 'want': 1, 'once': 3}), ('mind', {'united': 1, 'off': 1, 'often': 4, 'question': 1, 'took': 2, 'states': 5, 'person': 1, 'something': 1, 'nothing': 2, 'went': 2, 'once': 1, 'subject': 1}), ('necessary', {'states': 1, 'united': 1, 'often': 2, 'whether': 2, 'study': 1, 'water': 2, 'sense': 2, 'seen': 1}), ('nothing', {'question': 1, 'want': 2, 'sense': 1, 'seen': 3, 'themselves': 1, 'subject': 1}), ('off', {'took': 6, 'value': 1, 'side': 4, 'water': 3, 'person': 1, 'seen': 1, 'went': 9, 'subject': 1, 'themselves': 1, 'once': 3}), ('often', {'study': 1, 'question': 1, 'something': 1, 'want': 1, 'seen': 6, 'themselves': 3, 'went': 3}), ('once', {'united': 1, 'study': 1, 'took': 4, 'states': 2, 'seen': 6, 'went': 1, 'subject': 2}), ('person', {'whether': 9, 'question': 1, 'value': 1, 'states': 1, 'something': 2, 'seen': 1, 'subject': 2}), ('question', {'united': 1, 'whether': 43, 'study': 2, 'took': 1, 'side': 3, 'states': 1, 'went': 1, 'themselves': 1}), ('seen', {'whether': 1, 'study': 1, 'value': 2, 'water': 1, 'something': 1, 'themselves': 1, 'side': 3}), ('sense', {'themselves': 1, 'united': 1, 'something': 3, 'value': 2}), ('side', {'water': 2, 'themselves': 1, 'went': 2, 'value': 1}), ('something', {'whether': 1, 'went': 1, 'want': 6, 'subject': 1}), ('states', {'study': 1, 'united': 295, 'took': 1}), ('study', {'themselves': 1, 'went': 1, 'want': 1, 'subject': 2}), ('subject', {'went': 1, 'took': 1}), ('themselves', {'water': 1, 'whether': 1, 'united': 1, 'took': 1, 'want': 2}), ('took', {'water': 1, 'went': 2}), ('value', {'water': 1}), ('want', {'water': 1, 'whether': 5})]\n"
     ]
    }
   ],
   "source": [
    "myfile = './filtered-5Grams/googlebooks-eng-all-5gram-20090715-0-filtered.txt'\n",
    "#myfile = './filtered-5Grams/short-5gram.txt'\n",
    "\n",
    "output = runJob5_4(myfile)\n",
    "print output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
