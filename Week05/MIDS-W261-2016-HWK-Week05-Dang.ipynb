{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Student**: Minhchau Dang\n",
    "* **Email Address**: minhchau.dang@berkeley.edu\n",
    "* **Course**: 2016-0111 DATASCI W261: Machine Learning at Scale\n",
    "* **Section**: Spring 2016, Section 2\n",
    "* **Assignment**: Homework 4, Week 4\n",
    "* **Submission Date**: February 14, 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook requires some nbextensions.\n",
    "\n",
    "* [toc2](https://github.com/ipython-contrib/IPython-notebook-extensions/tree/master/nbextensions/usability/toc2) provides a button to create a floating table of contents\n",
    "* [toggle_all_line_numbers](https://github.com/ipython-contrib/IPython-notebook-extensions/tree/master/nbextensions/usability/toggle_all_line_numbers) provides a button to see line numbers for all code cells\n",
    "* [autosaveclasses](https://github.com/holatuwol/jupyter-magic/tree/master/nbextensions/autosaveclasses.js) avoids usage of `%%writefile` (cells with a class definition are saved to disk when run)\n",
    "\n",
    "If they are not yet installed, run the following cell and restart the notebook server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p .local/share/jupyter/nbextensions/\n",
    "\n",
    "nbextdl() {\n",
    "    mkdir -p $(ipython locate)/nbextensions/$(dirname $2)\n",
    "    curl --silent -L \\\n",
    "        \"https://raw.githubusercontent.com/$1/master/nbextensions/$2\" \\\n",
    "        > \"$(ipython locate)/nbextensions/$2\"\n",
    "}\n",
    "\n",
    "nbextdl ipython-contrib/IPython-notebook-extensions usability/toc2/main.js\n",
    "nbextdl ipython-contrib/IPython-notebook-extensions usability/toc2/main.css\n",
    "nbextdl ipython-contrib/IPython-notebook-extensions usability/toc2/icon.png\n",
    "nbextdl ipython-contrib/IPython-notebook-extensions usability/toc2/image.png\n",
    "\n",
    "nbextdl ipython-contrib/IPython-notebook-extensions usability/toggle_all_line_numbers/main.js\n",
    "nbextdl ipython-contrib/IPython-notebook-extensions usability/toggle_all_line_numbers/icon.png\n",
    "\n",
    "nbextdl holatuwol/jupyter-magic autosaveclasses.js"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoload the extensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "require(['base/js/utils'], function(utils) {\n",
       "    utils.load_extensions('usability/toc2/main');\n",
       "    utils.load_extensions('usability/toggle_all_line_numbers/main');\n",
       "    utils.load_extensions('autosaveclasses');\n",
       "});"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "require(['base/js/utils'], function(utils) {\n",
    "    utils.load_extensions('usability/toc2/main');\n",
    "    utils.load_extensions('usability/toggle_all_line_numbers/main');\n",
    "    utils.load_extensions('autosaveclasses');\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the HDFS base folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdfs_base_folder = '/tmp'\n",
    "runner = 'hadoop'\n",
    "\n",
    "mapper_count = 10\n",
    "reducer_count = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What is a data warehouse?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What is a Star schema? When is it used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In the database world, what is 3NF?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Does machine learning use data in 3NF? If so why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 5.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Using MRJob, implement a hashside join (memory-backed map-side) for left,\n",
    "right and inner joins. Run your code on the  data used in HW 4.4.\n",
    "\n",
    "> (Recall HW 4.4: Find the most frequent visitor of each page using mrjob and the output of 4.2  (i.e., transformed log file). In this output please include the webpage URL, webpageID and Visitor ID.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download 4.4 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!wget --quiet https://www.dropbox.com/sh/m0nxsf4vs5cyrp2/AADCHtrJ4CBCDO1po_OAWg0ia/anonymous-msweb.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform 4.4 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copyFromLocal: `/tmp/anonymous-msweb.data.visitor': File exists\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from csv import reader\n",
    "\n",
    "input_file_name = 'anonymous-msweb.data'\n",
    "\n",
    "visitor_file_name = input_file_name + '.visitor'\n",
    "visitor_file = open(visitor_file_name, 'w')\n",
    "\n",
    "webpage_file_name = input_file_name + '.webpage'\n",
    "webpage_file = open(webpage_file_name, 'w')\n",
    "\n",
    "with open(input_file_name) as input_file:\n",
    "    case_id = None\n",
    "\n",
    "    # Update the case ID when we see a new case (line with a C).\n",
    "    # Output the vote when we see a new vote (line with a V).\n",
    "    # Add the 'A' lines to the web page file.\n",
    "\n",
    "    for row in reader(input_file):\n",
    "        if row[0] == 'A':\n",
    "            print >> webpage_file, ','.join(row)\n",
    "            continue\n",
    "\n",
    "        if row[0] == 'C':\n",
    "            case_id = row[1]\n",
    "            continue\n",
    "\n",
    "        if row[0] == 'V':\n",
    "            row.extend(['C', case_id])\n",
    "            print >> visitor_file, ','.join(row)\n",
    "\n",
    "visitor_file.close()\n",
    "webpage_file.close()\n",
    "\n",
    "!hdfs dfs -copyFromLocal $visitor_file_name $hdfs_base_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create 5.2 job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "class WebPageHashJoin(MRJob):\n",
    "\n",
    "    \"\"\"\n",
    "    Allow join type to be passed through as a configuration option.\n",
    "    \"\"\"\n",
    "    def configure_options(self):\n",
    "        super(WebPageHashJoin, self).configure_options()\n",
    "        self.add_passthrough_option('--join-type', type = 'string', default = 'left')\n",
    "\n",
    "    \"\"\"\n",
    "    Load the web page data into memory to be the left table.\n",
    "    \"\"\"\n",
    "    def mapper_init(self):\n",
    "        self.vroot_data = {}\n",
    "        self.seen_vroots = set()\n",
    "\n",
    "        with open('anonymous-msweb.data.webpage', 'r') as webpage_file:\n",
    "            for row in csv.reader(webpage_file):\n",
    "                if len(row) != 5:\n",
    "                    continue\n",
    "\n",
    "                vroot_id = row[1]\n",
    "                vroot_datum = [row[0]] + row[2:]\n",
    "                self.vroot_data[vroot_id] = vroot_datum\n",
    "\n",
    "    \"\"\"\n",
    "    Mapper for outer joins which only yields the join result if both results are\n",
    "    not None.\n",
    "    \"\"\"\n",
    "    def mapper_inner_join(self, _, line):\n",
    "        vroot_id, left_result, right_result = self.get_join_row(line)\n",
    "\n",
    "        if left_result is not None and right_result is not None:\n",
    "            yield vroot_id, (left_result, right_result)\n",
    "\n",
    "    \"\"\"\n",
    "    Mapper for outer joins which always yields the join result.\n",
    "    \"\"\"\n",
    "    def mapper_outer_join(self, _, line):\n",
    "        vroot_id, left_result, right_result = self.get_join_row(line)\n",
    "\n",
    "        self.seen_vroots.add(vroot_id)\n",
    "\n",
    "        yield vroot_id, (left_result, right_result)\n",
    "\n",
    "    \"\"\"\n",
    "    Look up the web page data based on the vroot and combine with row.\n",
    "    \"\"\"\n",
    "    def get_join_row(self, line):\n",
    "        row = csv.reader([line]).next()\n",
    "\n",
    "        vroot_id = row[1]\n",
    "\n",
    "        if vroot_id in self.vroot_data:\n",
    "            vroot_datum = self.vroot_data[vroot_id]\n",
    "        else:\n",
    "            vroot_datum = None\n",
    "\n",
    "        visit_datum = [row[0]] + row[2:]\n",
    "\n",
    "        return vroot_id, vroot_datum, visit_datum\n",
    "\n",
    "    \"\"\"\n",
    "    Mapper finalizer which ensures that all values for the left table are emitted.\n",
    "    Should only be used with a left outer join.\n",
    "    \"\"\"\n",
    "    def mapper_final_left_join(self):\n",
    "        for vroot_id, vroot_datum in self.vroot_data.iteritems():\n",
    "            if vroot_id in self.seen_vroots:\n",
    "                continue\n",
    "\n",
    "            yield vroot_id, (vroot_datum, None)\n",
    "\n",
    "    \"\"\"\n",
    "    Reducer finds the most frequent visitor for the given vroot.\n",
    "    \"\"\"\n",
    "    def reducer(self, vroot_id, join_rows):\n",
    "        none_row = None\n",
    "        regular_row = False\n",
    "\n",
    "        # Emit any non-none rows, because none rows could come from multiple reducers\n",
    "        # and might not be valid output.\n",
    "\n",
    "        for join_row in join_rows:\n",
    "            if join_row[0] is None or join_row[1] is None:\n",
    "                none_row = join_row\n",
    "                continue\n",
    "\n",
    "            regular_row = True\n",
    "            yield vroot_id, join_row\n",
    "\n",
    "        # If we never found a non-none row, then emit the none row.\n",
    "\n",
    "        if not regular_row:\n",
    "            yield vroot_id, none_row\n",
    "\n",
    "    \"\"\"\n",
    "    Build out different steps depending on the join type.\n",
    "    \"\"\"\n",
    "    def steps(self):\n",
    "        # Inner join just needs the hash join initialization and uses the\n",
    "        # inner join mapper.\n",
    "\n",
    "        if self.options.join_type == 'inner':\n",
    "            step = MRStep(\n",
    "                mapper_init = self.mapper_init,\n",
    "                mapper = self.mapper_inner_join,\n",
    "                reducer = self.reducer)\n",
    "\n",
    "        # Left join needs to emit keys at the end for anything that was not\n",
    "        # seen in the input and uses the outer join mapper.\n",
    "\n",
    "        elif self.options.join_type == 'left':\n",
    "            step = MRStep(\n",
    "                mapper_init = self.mapper_init,\n",
    "                mapper = self.mapper_outer_join,\n",
    "                mapper_final = self.mapper_final_left_join,\n",
    "                reducer = self.reducer)\n",
    "\n",
    "        # Right join just needs the hash join initialization and otherwise\n",
    "        # uses the outer join mapper.\n",
    "\n",
    "        else:\n",
    "            step = MRStep(\n",
    "                mapper_init = self.mapper_init,\n",
    "                mapper = self.mapper_outer_join,\n",
    "                reducer = self.reducer)\n",
    "\n",
    "        return [step]\n",
    "\n",
    "if __name__ == '__main__' and sys.argv[0].find('ipykernel') == -1:\n",
    "    WebPageHashJoin().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Justify which table you chose as the Left table in this hashside join."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose the web page table as the left table in this mapper-side hash join because it was the smaller of the two tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Please report the number of rows resulting from:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a utility function to report the row counts by using `wc` on the output folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def report_join_row_count(join_type):\n",
    "    problem_id = 'mrjob_52_' + join_type\n",
    "    mrjob_status_file = 'mrjob_52_' + join_type + '_progress.txt'\n",
    "\n",
    "    !python WebPageHashJoin.py -r local \\\n",
    "        --hadoop-version=2.7.1 \\\n",
    "        --strict-protocols \\\n",
    "        --join-type=$join_type \\\n",
    "        --output-dir=$hdfs_base_folder/$problem_id \\\n",
    "        --no-output \\\n",
    "        --file anonymous-msweb.data.webpage \\\n",
    "        anonymous-msweb.data.visitor \\\n",
    "        > $mrjob_status_file 2>&1\n",
    "\n",
    "    !wc -l $hdfs_base_folder/$problem_id/* | tail -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (1) Left joining Table Left with Table Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  98663 total\n",
      "CPU times: user 218 ms, sys: 30.2 ms, total: 248 ms\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%time report_join_row_count('left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (2) Right joining Table Left with Table Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  98654 total\n",
      "CPU times: user 143 ms, sys: 24 ms, total: 167 ms\n",
      "Wall time: 41.8 s\n"
     ]
    }
   ],
   "source": [
    "%time report_join_row_count('right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (3) Inner joining Table Left with Table Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  98654 total\n",
      "CPU times: user 27.3 ms, sys: 6.34 ms, total: 33.6 ms\n",
      "Wall time: 9.84 s\n"
     ]
    }
   ],
   "source": [
    "%time report_join_row_count('inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 5.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download ngrams data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A large subset of the Google n-grams dataset\n",
    "\n",
    "> https://aws.amazon.com/datasets/google-books-ngrams/\n",
    "\n",
    "> which we have placed in a bucket/folder on Dropbox on s3:\n",
    "\n",
    "> * https://www.dropbox.com/sh/tmqpc4o0xswhkvz/AACUifrl6wrMrlK6a3X3lZ9Ea?dl=0\n",
    "> * s3://filtered-5grams/\n",
    "\n",
    "> This bucket contains (~200) files (10Meg each) in the format:\n",
    "\n",
    "> ```\n",
    "    (ngram) \\t (count) \\t (pages_count) \\t (books_count)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Download the ngrams data set.\n",
    "\n",
    "if not os.path.isdir('ngrams'):\n",
    "    !mkdir ngrams\n",
    "    !aws s3 sync --quiet s3://filtered-5grams/ ngrams/\n",
    "    !hdfs dfs -copyFromLocal ngrams $hdfs_base_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create first10 data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For HW 5.3-5.5, for the Google n-grams dataset unit test and regression test your code using the\n",
    "first 10 lines of the following file:\n",
    "\n",
    "    googlebooks-eng-all-5gram-20090715-0-filtered.txt\n",
    "\n",
    "Once you are happy with your test results proceed to generating your results on the Google n-grams dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Use the first 10 lines of the first file for unit testing.\n",
    "\n",
    "if not os.path.isdir('first10'):\n",
    "    !mkdir first10\n",
    "    !head -10 ngrams/googlebooks-eng-all-5gram-20090715-0-filtered.txt > first10/test.txt\n",
    "    !hdfs dfs -copyFromLocal first10 $hdfs_base_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set switch function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_job(python_args, problem_id, data_set, input_folder):\n",
    "\n",
    "    # Use the notebook variables\n",
    "\n",
    "    global runner\n",
    "    global hdfs_base_folder\n",
    "\n",
    "    # Determine the input folder based on job type\n",
    "\n",
    "    if runner == 'hadoop':\n",
    "        input_folder = 'hdfs://' + hdfs_base_folder + '/' + input_folder\n",
    "\n",
    "    # Create a new output folder for each problem + data set pair\n",
    "\n",
    "    if runner == 'hadoop':\n",
    "        !hdfs dfs -mkdir -p $hdfs_base_folder/$problem_id\n",
    "        !hdfs dfs -rm -r -f -skipTrash $hdfs_base_folder/$problem_id/$data_set > /dev/null\n",
    "    else:\n",
    "        !mkdir -p $hdfs_base_folder/$problem_id\n",
    "        !rm -rf $hdfs_base_folder/$problem_id/$data_set\n",
    "\n",
    "    # Run the program\n",
    "\n",
    "    mrjob_status_file = problem_id + '_progress.txt'\n",
    "\n",
    "    !python $python_args \\\n",
    "        -r $runner \\\n",
    "        --hadoop-version=2.7.1 \\\n",
    "        --strict-protocols \\\n",
    "        --output-dir=$hdfs_base_folder/$problem_id/$data_set \\\n",
    "        --no-output \\\n",
    "        --jobconf mapreduce.job.maps=$mapper_count \\\n",
    "        --jobconf mapreduce.job.reduces=$reducer_count \\\n",
    "        $input_folder \\\n",
    "        > $mrjob_status_file 2>&1\n",
    "\n",
    "def save_job_output(problem_id, data_set, file_name):\n",
    "\n",
    "    # Use the notebook variables\n",
    "\n",
    "    global hdfs_base_folder\n",
    "\n",
    "    # Print the desired output\n",
    "\n",
    "    if runner == 'hadoop':\n",
    "        !hdfs dfs -cat $hdfs_base_folder/$problem_id/$data_set/* > $file_name\n",
    "    elif runner == 'local':\n",
    "        !cat $hdfs_base_folder/$problem_id/$data_set/* > $file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 5.3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Do some EDA on this dataset using mrjob: Longest 5-gram (number of characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're doing data exploration, we can avoid the single reducer problem as well as the strange Hadoop combiner sort bug by emitting one value per reducer and then doing a simple sort of the output files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create 5.3.1 job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.protocol import RawProtocol\n",
    "\n",
    "class LongestNGram(MRJob):\n",
    "    INPUT_PROTOCOL = RawProtocol\n",
    "\n",
    "    \"\"\"\n",
    "    Yield the NGram itself and its length.\n",
    "    \"\"\"\n",
    "    def mapper(self, ngram, _):\n",
    "        ngram_length = len(ngram)\n",
    "        yield None, (ngram_length, ngram)\n",
    "\n",
    "    \"\"\"\n",
    "    Yield the longest ngram from the incoming pairs.\n",
    "    \"\"\"\n",
    "    def combiner(self, _, pairs):\n",
    "        yield None, self.get_longest_ngram(pairs)\n",
    "\n",
    "    \"\"\"\n",
    "    Yield the longest ngram from the incoming pairs.\n",
    "    \"\"\"\n",
    "    def reducer(self, _, pairs):\n",
    "        ngram_length, ngram = self.get_longest_ngram(pairs)\n",
    "        yield ngram_length, ngram\n",
    "\n",
    "    \"\"\"\n",
    "    Identify the longest ngram. In the case of ties, it doesn't matter which\n",
    "    one wins, so we'll choose the first one alphabetically.\n",
    "    \"\"\"\n",
    "    def get_longest_ngram(self, pairs):\n",
    "        longest_ngram = None\n",
    "        longest_ngram_length = 0\n",
    "\n",
    "        for ngram_length, ngram in pairs:\n",
    "            if ngram_length > longest_ngram_length:\n",
    "                longest_ngram = ngram\n",
    "                longest_ngram_length = ngram_length\n",
    "            elif ngram_length == longest_ngram_length and ngram < longest_ngram:\n",
    "                longest_ngram = ngram\n",
    "\n",
    "        return longest_ngram_length, longest_ngram\n",
    "\n",
    "if __name__ == '__main__' and sys.argv[0].find('ipykernel') == -1:\n",
    "    LongestNGram().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run 5.3.1 job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 486 ms, sys: 59.3 ms, total: 545 ms\n",
      "Wall time: 2min 31s\n"
     ]
    }
   ],
   "source": [
    "%time run_job('LongestNGram.py', 'mrjob_531', 'ngrams', 'ngrams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159\t\"AIOPJUMRXUYVASLYHYPSIBEMAPODIKR UFRYDIUUOLBIGASUAURUSREXLISNAYE RNOONDQSRUNSUBUNOUGRABBERYAIRTC UTAHRAPTOREDILEIPMILBDUMMYUVERI SYEVRAHVELOCYALLOSAURUSLINROTSR\"\r\n"
     ]
    }
   ],
   "source": [
    "save_job_output('mrjob_531', 'ngrams', 'mrjob_531_output.txt')\n",
    "!sort -k1nr 'mrjob_531_output.txt' | head -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 5.3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Do some EDA on this dataset using mrjob: Top 10 most frequent words (please use the count information), i.e., unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're doing data exploration, we can avoid the single reducer problem as well as the strange Hadoop combiner sort bug by emitting one value per reducer and then doing a simple sort of the output files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create 5.3.2 job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.protocol import RawProtocol\n",
    "\n",
    "class MostFrequentUnigrams(MRJob):\n",
    "    INPUT_PROTOCOL = RawProtocol\n",
    "\n",
    "    \"\"\"\n",
    "    Yield the word counts.\n",
    "    \"\"\"\n",
    "    def mapper(self, doc_id, value):\n",
    "        doc_id = doc_id.lower()\n",
    "\n",
    "        split_value = value.split('\\t')\n",
    "        ngram_count = int(split_value[0])\n",
    "\n",
    "        word_counts = {}\n",
    "\n",
    "        for word in doc_id.split(' '):\n",
    "            if word in word_counts:\n",
    "                word_counts[word] += ngram_count\n",
    "            else:\n",
    "                word_counts[word] = ngram_count\n",
    "\n",
    "        for word, count in word_counts.iteritems():\n",
    "            yield word, count\n",
    "\n",
    "    \"\"\"\n",
    "    Yield the summed word counts.\n",
    "    \"\"\"\n",
    "    def combiner(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "\n",
    "    \"\"\"\n",
    "    Yield the summed word counts.\n",
    "    \"\"\"\n",
    "    def reducer(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "\n",
    "if __name__ == '__main__' and sys.argv[0].find('ipykernel') == -1:\n",
    "    MostFrequentUnigrams().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run 5.3.2 job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 952 ms, sys: 164 ms, total: 1.12 s\n",
      "Wall time: 5min 50s\n"
     ]
    }
   ],
   "source": [
    "%time run_job('MostFrequentUnigrams.py', 'mrjob_532', 'ngrams', 'ngrams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_job_output('mrjob_532', 'ngrams', 'mrjob_532_output.txt')\n",
    "!sort -k2nr -k1 mrjob_532_output.txt > mrjob_532_output_sorted.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"the\"\t5490815394\r\n",
      "\"of\"\t3698583299\r\n",
      "\"to\"\t2227866570\r\n",
      "\"in\"\t1421312776\r\n",
      "\"a\"\t1361123022\r\n",
      "\"and\"\t1149577477\r\n",
      "\"that\"\t802921147\r\n",
      "\"is\"\t758328796\r\n",
      "\"be\"\t688707130\r\n",
      "\"as\"\t492170314\r\n"
     ]
    }
   ],
   "source": [
    "!head -10 mrjob_532_output_sorted.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 5.3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Do some EDA on this dataset using mrjob: 20 Most/Least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're doing data exploration, we can avoid the single reducer problem as well as the strange Hadoop combiner sort bug by emitting one value per reducer and then doing a simple sort of the output files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Create 5.3.3 job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.protocol import RawProtocol\n",
    "\n",
    "class MostDenselyAppearingWords(MRJob):\n",
    "    INPUT_PROTOCOL = RawProtocol\n",
    "\n",
    "    \"\"\"\n",
    "    Yield the word counts and page counts.\n",
    "    \"\"\"\n",
    "    def mapper(self, doc_id, value):\n",
    "        doc_id = doc_id.lower()\n",
    "\n",
    "        split_value = value.split('\\t')\n",
    "        ngram_count = int(split_value[0])\n",
    "        page_count = int(split_value[1])\n",
    "\n",
    "        word_counts = {}\n",
    "\n",
    "        for word in doc_id.split(' '):\n",
    "            if word in word_counts:\n",
    "                word_counts[word] += ngram_count\n",
    "            else:\n",
    "                word_counts[word] = ngram_count\n",
    "\n",
    "        for word, count in word_counts.iteritems():\n",
    "            yield word, (count, page_count)\n",
    "\n",
    "    \"\"\"\n",
    "    Yield the summed word counts and page counts.\n",
    "    \"\"\"\n",
    "    def combiner(self, word, pairs):\n",
    "        yield word, self.get_pairs_sum(pairs)\n",
    "\n",
    "    \"\"\"\n",
    "    Yield the summed word counts and page counts.\n",
    "    \"\"\"\n",
    "    def reducer(self, word, pairs):\n",
    "        total_word_count, total_page_count = self.get_pairs_sum(pairs)\n",
    "\n",
    "        yield word, float(total_word_count) / float(total_page_count)\n",
    "\n",
    "    \"\"\"\n",
    "    Yield the summed word counts and page counts.\n",
    "    \"\"\"\n",
    "    def get_pairs_sum(self, pairs):\n",
    "        total_word_count = 0\n",
    "        total_page_count = 0\n",
    "\n",
    "        for word_count, page_count in pairs:\n",
    "            total_word_count += word_count\n",
    "            total_page_count += page_count\n",
    "\n",
    "        return (total_word_count, total_page_count)\n",
    "\n",
    "if __name__ == '__main__' and sys.argv[0].find('ipykernel') == -1:\n",
    "    MostDenselyAppearingWords().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run 5.3.3 job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.02 s, sys: 156 ms, total: 1.17 s\n",
      "Wall time: 5min 46s\n"
     ]
    }
   ],
   "source": [
    "%time run_job('MostDenselyAppearingWords.py', 'mrjob_533', 'ngrams', 'ngrams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_job_output('mrjob_533', 'ngrams', 'mrjob_533_output.txt')\n",
    "!sort -k1nr -k2 mrjob_533_output.txt > mrjob_533_output_sorted.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57.786458333333336\t\"xxxx\"\r\n",
      "37.666666666666664\t\"nnn\"\r\n",
      "30.60614934114202\t\"blah\"\r\n",
      "24.609375\t\"oooooooooooooooo\"\r\n",
      "22.558139534883722\t\"llll\"\r\n",
      "18.963547995139734\t\"oooooo\"\r\n",
      "17.5\t\"xxxxxxxx\"\r\n",
      "16.635396724101255\t\"ooooo\"\r\n",
      "11.290476190476191\t\"choh\"\r\n",
      "10.034392999556934\t\"na\"\r\n",
      "9.965657249196413\t\"nd\"\r\n",
      "9.395604395604396\t\"iooo\"\r\n",
      "8.306397306397306\t\"illl\"\r\n",
      "6.107142857142857\t\"neg\"\r\n",
      "5.357142857142857\t\"oooooooo\"\r\n",
      "4.8193146417445485\t\"iiii\"\r\n",
      "4.216117216117216\t\"vir\"\r\n",
      "4.106772422208664\t\"nn\"\r\n",
      "3.9881422924901186\t\"ome\"\r\n",
      "3.9573934837092732\t\"beep\"\r\n"
     ]
    }
   ],
   "source": [
    "!head -20 mrjob_533_output_sorted.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\t\"zwingst\"\r\n",
      "1.0\t\"zwirnen\"\r\n",
      "1.0\t\"zwischenstaatlicher\"\r\n",
      "1.0\t\"zwitterionic\"\r\n",
      "1.0\t\"zwt\"\r\n",
      "1.0\t\"zwyn\"\r\n",
      "1.0\t\"zx\"\r\n",
      "1.0\t\"zxcvframeqasfuc\"\r\n",
      "1.0\t\"zydeco\"\r\n",
      "1.0\t\"zydom\"\r\n",
      "1.0\t\"zygmunt\"\r\n",
      "1.0\t\"zygomaticofacial\"\r\n",
      "1.0\t\"zygomaticotemporal\"\r\n",
      "1.0\t\"zygosity\"\r\n",
      "1.0\t\"zylindrischen\"\r\n",
      "1.0\t\"zymelman\"\r\n",
      "1.0\t\"zymogens\"\r\n",
      "1.0\t\"zymophore\"\r\n",
      "1.0\t\"zymosan\"\r\n",
      "1.0\t\"zymosis\"\r\n"
     ]
    }
   ],
   "source": [
    "!tail -20 mrjob_533_output_sorted.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 5.3.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Do some EDA on this dataset using mrjob: Distribution of 5-gram sizes (character length).  E.g., count (using the count field) up how many times a 5-gram of 50 characters shows up. Plot the data graphically using a histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create 5.3.4 job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.protocol import RawProtocol\n",
    "\n",
    "class NGramLengthCounts(MRJob):\n",
    "    INPUT_PROTOCOL = RawProtocol\n",
    "\n",
    "    \"\"\"\n",
    "    Yield the length and the count.\n",
    "    \"\"\"\n",
    "    def mapper(self, ngram, value):\n",
    "        ngram_length = len(ngram)\n",
    "\n",
    "        split_value = value.split('\\t')\n",
    "        ngram_count = int(split_value[0])\n",
    "\n",
    "        yield ngram_length, ngram_count\n",
    "\n",
    "    \"\"\"\n",
    "    Yield the summed counts.\n",
    "    \"\"\"\n",
    "    def combiner(self, ngram_length, counts):\n",
    "        yield ngram_length, sum(counts)\n",
    "\n",
    "    \"\"\"\n",
    "    Yield the summed counts.\n",
    "    \"\"\"\n",
    "    def reducer(self, ngram_length, counts):\n",
    "        yield ngram_length, sum(counts)\n",
    "\n",
    "if __name__ == '__main__' and sys.argv[0].find('ipykernel') == -1:\n",
    "    NGramLengthCounts().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run 5.3.4 job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 553 ms, sys: 73.7 ms, total: 627 ms\n",
      "Wall time: 2min 32s\n"
     ]
    }
   ],
   "source": [
    "%time run_job('NGramLengthCounts.py', 'mrjob_534', 'ngrams', 'ngrams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_job_output('mrjob_534', 'ngrams', 'mrjob_534_output.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot length histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA20AAAETCAYAAAClRe5SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3XlclPX6//H3MGyKgwiuoGammZJLhUtimh3bTIssrayO\nlVp6zA3X0kw9lpWpWbae7JjLqfSk1OlUarmbKaYeFZfCzAUEQQRRFmG4f3/44/46OsiQjMzo6/l4\n9Gjmms9c9/XhHoSL+74/t8UwDEMAAAAAAI/kU9EFAAAAAABKRtMGAAAAAB6Mpg0AAAAAPBhNGwAA\nAAB4MJo2AAAAAPBgNG0AAAAA4MF8K7qA999/X1u3blXVqlX15ptvXnRsenq63n//fZ08eVJVqlTR\n4MGDFRoaepkqBQAAAIDLr8KPtHXu3Fnjxo1zaey8efPUqVMnTZs2TQ8//LD+9a9/ubk6AAAAAKhY\nFX6k7YYbblBaWppDLDU1VXPmzFF2drb8/f313HPPKTw8XElJSXrqqackSZGRkXrjjTcqoGIAAAAA\nuHwq/EibMx999JGeeeYZTZ06VU8++aQ+/vhjSdI111yjTZs2SZI2bdqkvLw8nTp1qiJLBQAAAAC3\nqvAjbefLy8vTvn37NHPmTBmGIUmy2+2SpCeffFJz5szR6tWr1bRpU4WGhsrHxyP7TgAAAAAoFx7X\ntBmGoaCgIL3++usXvFatWjWNHDlS0tnmbtOmTapcufLlLhEAAAAALptSD1O9//776t+/v9ksOfPJ\nJ59oyJAhGjVqlP74448yF2EYhvLy8iRJlSpVUs2aNfXzzz8rISFBknTw4EFJUnZ2tnbt2iVJiouL\nU+fOnc0cxWPPVd6xqzmnN9fujpzeXLs7cnpz7e7I6c21uyOnN9fujpzeXLs7cnpz7e7I6c21uyOn\nN9fujpzeXLu35Kzo2ktSatNW2uqO27ZtU2pqqt5++209++yz+sc//uHyxiVp1qxZeumll5Senq6B\nAwdq1apVGjJkiFauXKm33npLI0aM0JYtWySdndiMGTM0bNgwZWVlqUePHmaeK22HeVpOb67dHTm9\nuXZ35PTm2t2R05trd0dOb67dHTm9uXZ35PTm2t2R05trd0dOb67dHTm9uXZvyVnRtZek1NMjna3u\neK74+Hh16tRJktS4cWPl5OQoMzNTISEhLhUwdOhQSdKiRYvUq1cvM/7iiy9eEGvXrp0OHTrkEAMA\nAACAK9klr+KRkZGhsLAw83loaKgyMjIuNS0AAAAAQJLFKF6i8SLS0tL0+uuv680337zgtddee00P\nPvigmjRpIkn6+9//rscff1wNGza8YGxCQoLDYUCOmAEAAAC42i1atMh8HBkZqcjISIfXL3n1yNDQ\nUB0/ftx8fvz4cYWGhjod66yA5ORkSZLNZlN2drbDa67GLvX9Fbkdb8npzbW7I6c31+6OnN5cuzty\nenPt7sjpzbW7I6c31+6OnN5cuztyenPt7sjpzbW7I6c31+4tOSui9vDw8FIPZrl0eqRhGCrpgFxU\nVJTWrFkjSfr1118VFBTk8vVsAAAAAICLK/VI26xZs7R7925lZ2dr4MCB6tWrlwoLC2WxWNSlSxfd\nfPPN2rZtmwYPHqzAwEANHDjwctQNAAAAAFeFUpu24tUdL6Zv377lUgwAAAAAwNElrx4JAAAAAHCf\nS16IBAAAAEDFsVqtstls5Ra7mnO6czuGYejUqVMX5HYFTRsAAADg5ZytTgjP4qwZdBWnRwIAAACA\nB6NpAwAAAAAPRtMGAAAAAB6Mpg0AAAAAPBhNGwAAAAB4MFaPBAAAAK4wSUkBSk62ui1/eLhdERH5\nLo1t27at0tPT5evrK8MwZLFYtG7dOtWsWdNt9V1paNoAAACAK0xyslUxMSFuyx8Xl6mICNfGWiwW\nzZs3T9HR0SWOsdvtslrd12R6O06PBAAAAOBWhmE4PD9y5Ijq1q2rzz//XG3atNEjjzwiSfrll1/0\nwAMPqFmzZrrrrru0ceNG8z2HDx/Www8/rBtuuEG9e/fW+PHjNXjwYEnSxo0bFRUV5bCNdu3aaf36\n9eb2Z8+erejoaDVv3lwDBw5UVlaWQy2LFy9WmzZt1KJFC7399ttmnqKiIr399tuKjo5WkyZN1LVr\nVx09elTjxo3T5MmTHbb59NNP6+OPPy6nr9r/oWkDAAAAUCF+/vlnrVmzRgsXLlRKSor69Omj4cOH\na/fu3XrppZfUv39/ZWRkSJIGDRqkli1baufOnRo6dKgWL14si8Vi5jr38fnmzJmj5cuXa8mSJdq6\ndauqVq2qF1980WFMfHy81q9fr88//1xvvfWWEhMTJUkffvihvv76ay1YsED79u3T9OnTValSJfXs\n2VNfffWV+f6MjAytX79ePXr0KM8vkSSaNgAAAABu1rdvX0VGRioyMlL9+vUz4yNHjlSlSpUUEBCg\nJUuW6C9/+Ytuv/12SdJtt92mli1bauXKlUpKStKOHTs0cuRI+fn5qW3btrrzzjtd3v6CBQs0ZswY\n1apVS35+fho+fLj++9//qqioSNLZhm/EiBHy9/dXs2bN1KxZM+3evVuS9Nlnn2nMmDG69tprJUlN\nmzZVSEiIWrVqpeDgYK1bt06S9PXXX+vWW29VaGhoeXzJHHBNG9wuKSlAqalSrVoBLl+wCgAAgCvH\nJ5984nBN25EjR2SxWFSnTh2H2DfffKMffvhB0tlTGgsLCxUdHa3U1FRVrVpVlSpVMsdHRETo6NGj\nLm3/yJEj6tevn3x8fMzcvr6+SktLM8fUqFHDfBwYGKicnBxJUnJysq655hqneR9++GEtWbJEt912\nm7788kuHhrQ80bTB7c5eCGsr0wWrAAAAuHKcf01bsXNPaQwPD9dDDz2kN95444JxSUlJysrKUm5u\nrtm4JScnm++vXLmycnNzzfF2u13Hjx83n0dERGj69OkXXPcmnW3oLiY8PFx//PGHrr/++gte69Gj\nh7p06aLdu3dr//79uueeey6a68/i9EgAAAAAl935jVyPHj30ww8/aM2aNSoqKlJeXp42btyolJQU\nRUREqEWLFpo+fboKCgq0efNmrVixwnxvw4YNlZ+fr5UrV6qwsFCzZs1SQUGB+foTTzyh1157TUlJ\nSZKk48ePa/ny5SXWcq7evXtr2rRpOnDggCRpz549yszMlCTVqVNHLVq00JAhQ9S1a1cFBARc+hfG\nCY60AQAAAFeY8HC74uIy3ZrfVSUtEHJ+PDw8XJ988ommTJmiv/3tb/L19VWrVq00depUSdLs2bM1\nbNgw3XjjjbrlllvUs2dPcwVIm82mV199VSNHjlRRUZEGDhzocOpl8WmLjz32mI4dO6awsDDdf//9\nuuuuu5zWcu7zZ599VmfOnFHv3r114sQJNWrUyGGFyJ49e2ro0KGaMmWKy1+TsqJpQ7ni+jUAAICK\nFxGR7zGXpZy7bH+xunXr6vDhwxfEW7VqpX//+99O89SvX19Lliwxn8+YMcNs2qSzzVPPnj3N5889\n95z52GKxqH///urfv79LtSxevNh87OPjoyFDhmjIkCFO64qIiFB4eLjatWvn9PXywOmRKFfJyVZ1\n725TcjI3RwQAAMCVraCgQB9//LF69+7t1u3QtKFCJCUFaM0aKT6+spKS3HPuLwAAAOAuiYmJatas\nmdLT0922amQxTo9EhSheUVISq0oCAACgzGJjYyt0+40aNdJvv/12WbbFkTYAAAAA8GA0bQAAAADg\nwWjaAAAAAMCDcU0bAAAA4OVsNpvDc6vVKrvd/qdil/p+b87pzu1c7AbepaFpAwAAALyY3W5Xdna2\nQ8xms/3p2KW+35tzXs7ay4LTIwEAAADAg9G0AQAAAIAHo2kDAAAAAA9G0wYAAAAAHoymDQAAAAA8\nGE0bAAAAAHgwmjYAAAAA8GA0bQAAAADgwbi5Nv6UpKQAJSdbZbVKtWoFKCIi3yNzAgAAAN6OI234\nU5KTrYqJCVH37jYlJ1s9NicAAADg7Vw60rZ9+3bNnTtXhmGoc+fOiomJcXg9JydH77zzjtLT01VU\nVKTu3bvr9ttvd0e9AAAAAHBVKbVpKyoq0pw5czRhwgRVq1ZNL7zwglq3bq2IiAhzzLJly1SvXj2N\nGTNGJ0+e1LBhw3TbbbfJauVoCQAAAABcilJPj0xMTFSdOnVUo0YN+fr6Kjo6WvHx8Q5jLBaLcnNz\nJUl5eXmy2Ww0bAAAAABQDkpt2jIyMhQWFmY+Dw0NVUZGhsOYe+65R0eOHNFzzz2nUaNG6amnnir3\nQgEAAADgalQuq0du375d1157rV5++WWlpKRoypQpevPNNxUYGOgwLiEhQQkJCebzXr16yWazSZL8\n/f3Nx8VcjV3q+ytyO96S8/zYuQdSrVar+Vpx3NXYufGScl5q7eU998u5HW/J6c21uyOnN9fujpze\nXLs7cnpz7e7I6c21uyOnN9fujpzeXLs7cnpz7d6Ss6JqX7RokRmPjIxUZGSkw9hSm7bQ0FClp6eb\nzzMyMhQaGuowZvXq1ebiJLVr11bNmjWVlJSk6667zmGcswKys7MlSTabzXxczNXYpb6/IrfjLTnP\nj9ntlc95bFd2do5D3NXYufGScl5q7eU998u5HW/J6c21uyOnN9fujpzeXLs7cnpz7e7I6c21uyOn\nN9fujpzeXLs7cnpz7d6SsyJqt9ls6tWr1wWvn6vU0yMbNWqklJQUpaWlqbCwUBs2bFBUVJTDmOrV\nq2vnzp2SpMzMTB09elS1atUqLTUAAAAAoBSlHmnz8fFR3759NWXKFBmGoTvuuEN169bVihUrZLFY\n1KVLFz300EN67733NHLkSEnS448/ripVqri9eAAAAAC40rl0TVurVq00a9Ysh9idd95pPq5WrZrG\njRtXvpUBAAAAAEo/PRIAAAAAUHFo2gAAAADAg9G0AQAAAIAHo2kDAAAAAA9G0wYAAAAAHoymDQAA\nAAA8GE0bAAAAAHgwmjYAAAAA8GA0bQAAAADgwXwrugCgNElJAUpNlWrVClBERH5FlwMAAABcVhxp\ng8dLTraqe3ebkpOtFV0KAAAAcNnRtAEAAACAB6NpAwAAAAAPRtMGAAAAAB6Mpg0AAAAAPBhNGwAA\nAAB4MJo2AAAAAPBgNG0AAAAA4MFo2gAAAADAg/lWdAHwfElJAUpNlez2ygoPtysiIr+iSwIAAACu\nGhxpQ6mSk63q3t2mmJgQJSdbK7ocAAAA4KpC0wYAAAAAHoymDQAAAAA8GE0bAAAAAHgwmjYAAAAA\n8GA0bQAAAADgwWjaAAAAAMCD0bQBAAAAgAejaQMAAAAAD0bTBgAAAAAejKYNAAAAADyYb0UXAPwZ\nSUkBSk2VatUKUEREfkWXAwAAALgNR9rglZKTrere3abkZGtFlwIAAAC4FU0bAAAAAHgwmjYAAAAA\n8GAuXdO2fft2zZ07V4ZhqHPnzoqJiblgTEJCgj799FPZ7XYFBwfr5ZdfLvdiAQAAAOBqU2rTVlRU\npDlz5mjChAmqVq2aXnjhBbVu3VoRERHmmJycHM2ZM0fjx49XaGioTp486daiAQAAAOBqUerpkYmJ\niapTp45q1KghX19fRUdHKz4+3mHM+vXr1bZtW4WGhkqSgoOD3VMtAAAAAFxlSj3SlpGRobCwMPN5\naGioEhMTHcYkJyfLbrdr0qRJysvL07333quOHTuWf7UAAAAAcJUpl/u0FRUV6cCBA5owYYLy8/M1\nfvx4XX/99apdu3Z5pAcAAACAq1apTVtoaKjS09PN5xkZGeZpkOeOsdls8vf3l7+/v5o2bao//vjj\ngqYtISFBCQkJ5vNevXrJZrNJkvz9/c3HxVyNXer7K3I73pDTes6t0KxWq2w2m9PYuWNdjZVnzsv1\n9bic2/GWnN5cuztyenPt7sjpzbW7I6c31+6OnN5cuztyenPt7sjpzbW7I6c31+4tOSuq9kWLFpnx\nyMhIRUZGOowttWlr1KiRUlJSlJaWpmrVqmnDhg0aOnSow5jWrVvrk08+UVFRkQoKCvTbb7+pW7du\nF+RyVkB2drYkyWazmY+LuRq71PdX5Ha8IafdXvmcx3ZlZ+c4jZ071tVYeea8XF+Py7kdb8npzbW7\nI6c31+6OnN5cuztyenPt7sjpzbW7I6c31+6OnN5cuztyenPt3pKzImq32Wzq1avXBa+fq9SmzcfH\nR3379tWUKVNkGIbuuOMO1a1bVytWrJDFYlGXLl0UERGhli1bauTIkfLx8VGXLl1Ut27d0lIDAAAA\nAErh0jVtrVq10qxZsxxid955p8Pz+++/X/fff3/5VQYAAAAAKH3JfwAAAABAxaFpAwAAAAAPRtMG\nAAAAAB6Mpg0AAAAAPBhNGwAAAAB4MJo2AAAAAPBgNG0AAAAA4MFo2gAAAADAg9G0AQAAAIAHo2kD\nAAAAAA9G0wYAAAAAHsy3ogsAyktSUoBSUyW7vbLCw+2KiMiv6JIAAACAS8aRNlwxkpOt6t7dppiY\nECUnWyu6HAAAAKBc0LQBAAAAgAfj9Eg44BRDAAAAwLNwpA0OOMUQAAAA8Cw0bQAAAADgwWjaAAAA\nAMCD0bQBAAAAgAejaQMAAAAAD0bTBgAAAAAejKYNAAAAADwYTRsAAAAAeDCaNgAAAADwYDRtAAAA\nAODBaNoAAAAAwIPRtAEAAACAB6NpAwAAAAAPRtMGAAAAAB6Mpg0AAAAAPBhNGwAAAAB4MJo2AAAA\nAPBgNG0AAAAA4MF8K7oAwJ2SkgKUnGyV1SrVqhWgiIj8ii4JAAAAKBOOtOGKlpxsVUxMiLp3tyk5\n2VrR5QAAAABlRtMGAAAAAB7MpaZt+/btGjZsmIYOHaq4uLgSxyUmJuqxxx7Tpk2byq1AAAAAALia\nldq0FRUVac6cORo3bpymT5+uDRs2KCkpyem4f/3rX2rZsqVbCgUAAACAq1GpTVtiYqLq1KmjGjVq\nyNfXV9HR0YqPj79g3Pfff6927dopODjYLYUCAAAAwNWo1KYtIyNDYWFh5vPQ0FBlZGRcMCY+Pl53\n3XVX+VcIAAAAAFexclnyf+7cuXr88cfN54ZhOB2XkJCghIQE83mvXr1ks9kkSf7+/ubjYq7GLvX9\nFbkdT8tpPWeBRavVKpvN5nLs3Pe7GquonJfyNSopdjXn9Oba3ZHTm2t3R05vrt0dOb25dnfk9Oba\n3ZHTm2t3R05vrt0dOb25dm/JWVG1L1q0yIxHRkYqMjLSYWypTVtoaKjS09PN5xkZGQoNDXUY8/vv\nv+utt96SYRjKzs7Wtm3b5Ovrq6ioKIdxzgrIzs6WJNlsNvNxMVdjl/r+ityOp+W02yuf89iu7Owc\nl2Pnvt/VWEXlvJSvUUmxqzmnN9fujpzeXLs7cnpz7e7I6c21uyOnN9fujpzeXLs7cnpz7e7I6c21\ne0vOiqjdZrOpV69eF7x+rlKbtkaNGiklJUVpaWmqVq2aNmzYoKFDhzqMmT17tvn4vffe0y233HJB\nwwYAAAAAKLtSmzYfHx/17dtXU6ZMkWEYuuOOO1S3bl2tWLFCFotFXbp0uRx1AgAAAMBVyaVr2lq1\naqVZs2Y5xO68806nY//2t79delUAAAAAAEku3lwbAAAAAFAxaNoAAAAAwIPRtAEAAACAB6NpAwAA\nAAAPRtMGAAAAAB6Mpg0AAAAAPBhNGwAAAAB4MJo2AAAAAPBgLt1cG7iSJCUFKDnZKqtVqlUrQBER\n+RVdEgAAAFAijrThqpOcbFVMTIi6d7cpOdla0eUAAAAAF0XTBgAAAAAejKYNAAAAADwY17RdxZKS\nApSaynVdAAAAgCfjSNtVLDnZynVdAAAAgIejaQMAAAAAD0bTBgAAAAAejKYNAAAAADwYTRsAAAAA\neDCaNgAAAADwYDRtAAAAAODBaNoAAAAAwIPRtAEAAACAB6NpAwAAAAAPRtMGAAAAAB6Mpg0AAAAA\nPJhvRRcAeIqkpAClpkq1agUoIiK/ossBAAAAJHGkDTAlJ1vVvbtNycnWii4FAAAAMNG0AQAAAIAH\no2kDAAAAAA9G0wYAAAAAHoymDQAAAAA8GE0bAAAAAHgwmjYAAAAA8GA0bQAAAADgwWjaAAAAAMCD\n0bQBAAAAgAfzdWXQ9u3bNXfuXBmGoc6dOysmJsbh9fXr1+urr76SJAUGBqp///6qX79++VcLAAAA\nAFeZUo+0FRUVac6cORo3bpymT5+uDRs2KCkpyWFMzZo1NWnSJE2bNk0PPfSQPvzwQ7cVDAAAAABX\nk1KbtsTERNWpU0c1atSQr6+voqOjFR8f7zDm+uuvV+XKlSVJjRs3VkZGhnuqBQAAAICrTKmnR2Zk\nZCgsLMx8HhoaqsTExBLH//jjj2rVqlX5VAdUsKSkAKWmSnZ7ZYWH2xURkV/RJQEAAOAq49I1ba7a\ntWuXVq9ercmTJzt9PSEhQQkJCebzXr16yWazSZL8/f3Nx8VcjV3q+ytyOxWZ02ot/r/VjBfHzo27\nGist55/ZjjtylmU7qalS9+5nH//nP9m64QZ/p1/LYlfS58MTt+MtOb25dnfk9Oba3ZHTm2t3R05v\nrt0dOb25dnfk9Oba3ZHTm2v3lpwVVfuiRYvMeGRkpCIjIx3Gltq0hYaGKj093XyekZGh0NDQC8Yd\nPHhQH330kV588UVVqVLFaS5nBWRnZ0uSbDab+biYq7FLfX9Fbqcic9rtlf///+3Kzs5xiJ0bdzVW\nWs4/sx135Pwz2zk/7mn7sqJzenPt7sjpzbW7I6c31+6OnN5cuztyenPt7sjpzbW7I6c31+6OnN5c\nu7fkrIjabTabevXqdcHr5yr1mrZGjRopJSVFaWlpKiws1IYNGxQVFeUwJj09XdOnT9fzzz+v2rVr\nl5YSAAAAAOCiUo+0+fj4qG/fvpoyZYoMw9Add9yhunXrasWKFbJYLOrSpYv+/e9/69SpU5ozZ44M\nw5DVatXUqVMvR/0AAAAAcEVz6Zq2Vq1aadasWQ6xO++803w8YMAADRgwoHwrAwAAAACUfnokAAAA\nAKDilOvqkfBMSUkBSk62ymqVatUKYNl6AAAAwItwpO0qkJxsVUxMiLp3tyk52Vr6GwAAAAB4DJo2\nAAAAAPBgNG0AAAAA4MG4pg0oo6SkAKWmnr3xdni4nWsEAQAA4FYcaQPKKDnZqu7dbYqJCeEaQQAA\nALgdTRsAAAAAeDCaNgAAAADwYDRtAAAAAODBaNoAAAAAwIPRtAEAAACAB6NpAwAAAAAPRtMGAAAA\nAB6Mm2sD5SApKUDJyVZZrVKtWgHccBsAAADlhiNtQDlITrYqJiZE3bvbuOE2AAAAyhVNGwAAAAB4\nMJo2AAAAAPBgNG0AAAAA4MFo2gAAAADAg9G0AQAAAIAHY8l/wI2SkgKUmsptAAAAAPDncaQNcKPk\nZCu3AQAAAMAloWkDAAAAAA/G6ZFXmKSkACUnW2W1ckoeAAAAcCXgSNsVJjnZqpiYEE7JAwAAAK4Q\nHGkDLjMWJwEAAEBZcKQNuMxYnAQAAABlQdMGAAAAAB6Mpg0AAAAAPBjXtAEeoPg6N7u9ssLD7Vzr\nBgAAABNH2gAPUHydW0xMCNe6AQAAwAFNGwAAAAB4ME6PBDwUN0oHAACARNPm1bgO6spWfKN0SYqL\ny1RERAUXBAAAgArhUtO2fft2zZ07V4ZhqHPnzoqJiblgzCeffKLt27crICBAgwYNUoMGDcq7Vpzn\n7C/1Nkn8Un814ebcAAAAV5dSr2krKirSnDlzNG7cOE2fPl0bNmxQUlKSw5ht27YpNTVVb7/9tp59\n9ln94x//cFvBwNXO2c25k5ICtGbN2f8DAADgylJq05aYmKg6deqoRo0a8vX1VXR0tOLj4x3GxMfH\nq1OnTpKkxo0bKycnR5mZme6pGMAFLtbIxcdXppkDAADwYqWeHpmRkaGwsDDzeWhoqBITE0sdk5GR\noZCQkHIs9erGKXEoK2enz7K4CQAAgPdhIRIP5KxBK/4FnGvXcCmcLW5SUiPn7HPobPGbkmKu5gQA\nAMDFWQzDMC424Ndff9XixYs1btw4SVJcXJwkOSxG8tFHH+nGG29U+/btJUnDhg3TxIkTLzjSlpCQ\noISEBPN5r169ymcWAAAAAOClFi1aZD6OjIxUZGSkw+ulXtPWqFEjpaSkKC0tTYWFhdqwYYOioqIc\nxkRFRWnNmjWSzjZ5QUFBTk+NjIyMVK9evcz/Siq0rLFLfX9Fbsdbcnpz7e7I6c21uyOnN9fujpze\nXLs7cnpz7e7I6c21uyOnN9fujpzeXLs7cnpz7e7I6c21e0vOiqr93B7p/IZNcuH0SB8fH/Xt21dT\npkyRYRi64447VLduXa1YsUIWi0VdunTRzTffrG3btmnw4MEKDAzUwIEDS0sLAAAAAHCBS9e0tWrV\nSrNmzXKI3XnnnQ7P+/btW35VAQAAAAAkSdaJEydOrOgiitWsWfNPxy71/RW5HW/J6c21uyOnN9fu\njpzeXLs7cnpz7e7I6c21uyOnN9fujpzeXLs7cnpz7e7I6c21uyOnN9fuLTkrunZnSl2IBAAAAABQ\ncUpdiAQAAAAAUHFo2gAAAADAg9G0AQAAAIAHo2kDAAAAAA9WIatHJiUlaeXKlfrpp5+0bds2HTx4\nUFWqVFFwcLDTsQcPHlTVqlXl63v2DgWJiYn6+eef1bhxYx05ckRr167VqVOnVKdOHfN9s2fPVps2\nbRxy7d27V//5z39UUFCgiIgInTlzRl9++aXmzJmjQ4cOqWnTpvLz85MkFRYWat26dTp58qRq1aql\n9evXa8WKFTp27JgqVaqk1atX66efftKOHTt07Ngx1alTx3wvXJOVlaXAwECXxmZnZysgIMDNFV0a\n5uPZ85GuvDldafORXJ8T86kYV+JnDgC8wWVfPTIuLk4bNmxQdHS0QkNDJUkZGRlmLCYmxhz77bff\natmyZQoICNDp06f11FNP6Y8//tD27dt18OBBde3aVb/99ptOnDihkydPKjg4WHXq1JFhGEpISJDV\nalXTpk01ZswY/fDDD1q2bJlOnDih2rVrKyoqSqmpqQoICNCPP/4oi8UiX19fPfLII7r11ls1d+5c\n2e125ef+Wt31AAAgAElEQVTnKygoSHl5eWrbtq2+++47HT9+XHfffbe2bdumBg0aKCgoSJs3b1a/\nfv2c3sG8vGRlZalq1aoujc3OzpbNZvtT28nJydHSpUsVHx+vrKwsWSwWVa1aVVFRUYqJiVFQUJDD\n2NGjR6tJkya66aab1KFDB2VmZmrx4sXau3evJk6cqO+++04//fSTwsPD1bt3b4WEhMgwDMXGxmrm\nzJmSzt7E/dNPP9X+/ftlt9sVGxurevXqaf/+/Zo5c6YyMjLk7++vfv36qUOHDpKk/fv3a8GCBapW\nrZp69+6t999/X7/99psqVaokPz8/ZWdny9fXVzVq1FDVqlWVmprq0nzi4uK0Zs0aPfnkk2Waz9ix\nY/XEE0+oefPmqlKlinJycvTpp5/ql19+0Q033KB+/frp+PHjZZpPYmKiateurYYNG2rfvn06fvx4\nmeZ0pc2Hz5zn7yNn85GkzMxMTZ48Wc2aNdMjjzxSpn3k6nzcsY8u13wsFovS09PVsWNHPfjgg6pd\nu7ZbP3Oe/j1Uu3Zt3Xnnnbr99ttd+bEFAFe8y960DR06VNOnTzePmhUrLCxUbGys3n77bTM2YsQI\nvfLKKxo+fLgmTZqkGTNmKCMjQx988IFGjx6ttLQ0vf/++5o0aZLq1KmjxMRE/e1vf5NhGJo1a5YC\nAwM1YMAANWvWTC+88IJeeOEFvfzyy5o6darGjRsnX19fvf766xo9erRee+01DRkyRJGRkdqyZYsK\nCgr0zDPPKCoqSsOHD9eHH34oHx8fjRgxQpI0ffp05efna+rUqRo9erT+9a9/aeXKlQoICPD6H5Z2\nu10tWrTQM888o5CQEEnS//73P/3yyy9KTExUv379zPnMnz9fiYmJGjx4sFatWiWr1aq8vDzdcsst\niouLk81mU4cOHfTZZ58pKChIZ86cMRvPtLQ01ahRQxaLRTfeeKNCQkL0l7/8RePGjVOjRo00evRo\nTZo0SY8//rhmzpypG2+8UWvXrlWDBg0UHR2tNWvWqHfv3jp9+rQWLlyoPn36aO3atQoPD9eOHTvU\nsWNH5eXlaceOHcrPz1eTJk30zDPPSDr7C9eSJUuczqd69eqKj49X06ZNyzSfjIwMWSwWhYaGavbs\n2frggw8UEhKin3/+WV26dNHu3buVm5tbpvm0a9dO48ePV1ZWliZOnKiNGzeWOCdn++hKmw+fOc/f\nR87mM3ToUL3xxhs6fPiw7rnnHq1fv75M+8jV+bRv317Tpk1Tr169/tQ+qsj5NGrUSAMGDJDdbpev\nr69CQkIu+TPnzd9D0dHR+vLLL1WtWjW1adNGGRkZkqTQ0FA1atRIFotF50tKSlKtWrUcfr8oKirS\nyZMnFRISosLCQh06dEg1a9ZUlSpVzDHLli3T3XffbT7Py8vToUOHFB4ebo7btWuXDhw4IH9/f4ex\nxdLT01WpUiUFBQXp2LFj+v333xUeHq6CggIdP35cPj4+qlOnjsLDw5WYmOjSfCTp0KFDql+//iXN\n52Jz2rp1q5o3b66bbrrpT80nIiJChmG4PKc/M5+y7KOS5uOufXT+fKSz/9YHBwfLx8fnqv3M5eXl\nKTk5WWFhYQoODja3dTV95kqaT/369bV///4L5uQK39KHlC+LxaITJ06oRo0aDvGRI0cqPT1dI0eO\nNGMpKSkaN26csrKyVLNmTU2cOFHPPvus5s+fL4vFolq1aqly5cqaOnWqvv32W23dulWVK1dWgwYN\n5O/vr4CAANWvX1/Z2dkqKipScHCw6tWrp40bN8pqteqaa67R/v37ZbFYlJKSIpvNpoEDB6qwsFBD\nhgzR1q1bNW/ePBUWFionJ0dVqlSRYRgqKiqSJBUUFCgvL08zZ85UZGSkatasqbfeekvS//2wfOWV\nVy74YZmVlaW2bdtq1apV+vnnn80fllu2bNGkSZPUoUMHHT16VCdPntTYsWPNH5ZZWVkaM2aM+cOy\nWrVqGjNmjMaNG6fPPvtMo0eP1oIFCzRs2DDzh+W7776r//73vxf8sHzppZfUp08fBQQEmD8s77nn\nHuXl5Wn16tXy9/fXt99+q969e0uSXn31VTVr1kyHDh3S/Pnzzfns379fRUVFatOmjdq0aaMlS5Zo\n6dKlGjp0qFavXq2TJ08qJiZGVqvVPJV01qxZkqTevXvr3XfflSSNGjVK06ZNkyTzAy5JZ86cUaNG\njVSlShUNHDhQiYmJ+utf/6oNGzbo8OHD+vrrrxUdHS1Jateunb788kuNHj1ao0ePVrdu3fTCCy8o\nJydHM2fOVGxsrFl3SEiIli1bJj8/vwvmU7xvx4wZU6b5DBo0SFWqVNHrr79u5po2bZo2bdqke++9\nV2vWrJG/v3+Z5iNJ+fn5qlSpksLCwi46J2f76EqbD585z99HzuYzefJk5eTkqGrVqoqJidGyZcvK\ntI9cnc+YMWOUn5+v48ePq0uXLlq4cGGZ9lFFzkeSqlatqsLCQk2fPl179uy55M+cN38PPfzww4qO\njtYbb7yhgwcPmmfmHD9+XCkpKerXr59atmxpzmnXrl2aMmWKgoKCdO211+rZZ5/VH3/8oX/84x86\ndeqUYmNjtXTpUp06dUpZWVlq166d6tWrJ8Mw9Pnnn6ugoECS1KhRI82aNUunTp1S5cqV9dxzz+nI\nkSPavHmzbrrpJs2fP1+ff/657r77bnXo0EF169ZVXFycVqxYIT8/P3Xv3l3/+c9/VLNmTSUkJCgs\nLEzZ2dlq0qSJjh07ptTUVDVu3Ng8knqx+cyePVsnTpxQ8+bNyzSfuLg4rVu3Tu3atVO3bt20d+/e\nEuf022+/afny5WrQoIEGDBhQpvmcPn1aubm5ys3NVURExEX3kavzCQwM1P79+835SCrTPnI2H0nl\nvo+czadmzZravHmzZsyYoeDgYPXv3/+q+cw5m0/t2rW1d+9eDR48WO3bt9fXX3991XzmnM2nSZMm\nmj9/voqKihQeHq7ff//dnJPVatXzzz+v6tWr62Iu+zVttWvX1ltvvaXt27dr9+7d+uWXX7Rs2TL9\n/vvveuqpp9SzZ0916NBBHTp00L59+/TII49o586d6t69u3x9fbV582b5+vpq165d+uCDD2SxWGSx\nWFS3bl1t2rRJSUlJ2r17t5KSklRYWKgff/xRP/zwgwoKCnTrrbeqXbt2+v7777V79275+PhoyZIl\nOnnypA4cOKB+/fopJCREPj4+MgxD8fHx8vHx0UMPPaQvv/xSe/bs0cGDB3XmzBklJydr0aJFuu++\n+/TLL7+ob9++2rp1qzp37izp7BHFgIAAHThwQEeOHNGOHTu0Y8cO84flkCFD1KFDB6WkpGj9+vUa\nOHCgNm/erFOnTmnkyJEKDAxUQUGBDMPQO++8o/vuu09Lly7VJ598oq5du+qLL75QbGysgoKCtHLl\nSuXm5uquu+7SihUr1LNnT61du1YvvPCCNm7cqKefflp79uzR9u3blZGRofDwcO3Zs0cDBgxQXFyc\nRo4cqRUrVmjQoEGaP3+++ReLtWvXqmvXrpKkDRs2qFmzZvL19dWECRN0++236/bbb9fatWvl4+Oj\nbt26SZKaNm2qFStWaP369crJyVGnTp104403qkmTJoqMjNRXX32lkydPqkmTJoqLi1NgYKD27dun\nPXv2qGvXrrJYLDIMQ5s2bVLjxo1ltVq1efNmHT58WNnZ2fLz89N9992nm2++Wdu2bVOrVq0UHx+v\nnJwc1ahRQ/v371d+fr4OHTqkkJAQHT58WJUrV1ZmZqYSExPN+WRmZmrt2rWqW7euJk+e7DCfV155\nRT/++KO6detWpvmsWLFCubm5kuQwJ8MwtGjRImVmZqpt27Zlmk9ERIS+//57FRYW6p577tGWLVtK\nnJOzfXSlzYfPnOfvI2fz8fHx0dq1a2WxWHTffffp1KlTZdpHrs6nW7duWrdunTIyMuTn56cdO3aU\naR9V5HwCAgK0evVqNWnSRG3btlWNGjUu+TNXnt9DWVlZl+V7qGrVqjp8+LA6duyoN954Q/7+/nrt\ntdcUFRWlqKgoJSYmKiQkREuWLFFGRoa2bdumbdu2afHixTIMQ3PnzpW/v7/effdd7dy5U6+++qp+\n+eUXrVmzRi+++KK++eYbNWvWTHv37lX9+vVVWFiohIQENW7cWIWFhVq9erUGDhyo3bt3a/z48Zoz\nZ44OHz6siRMnqnnz5tq8ebOsVqvq1q2rzz//XKtWrdLOnTs1fvx43XvvvXr11Vc1bdo0LV++XJMn\nT9bGjRs1ceJE7dixQ9nZ2XriiSd0+PBhxcbGKioqSh07dtSBAweczicyMlJpaWl65JFHyjSfX3/9\nVXl5eapfv74iIyP17rvvljinrVu3asyYMfr222+1adOmMs1n/PjxWrp0qapXr64JEyZcdB+5Op9u\n3bppyZIlOnr0qBo2bKiCgoIy7SNn88nJydHKlSs1Y8YMdezYscz7yNX5XHfddVqwYIGCgoI0YcIE\nvfLKK1fNZ87ZfB588EGtW7dOv//+u7p06aJ//vOfV81nztl8OnTooFWrVsnHx0dTp05V+/btzTmF\nhoZq4cKF6tix40V7qMt+pK1Vq1aaNWvWBYc1q1WrpmuuucbhCNzw4cNltVrVrFkzMzZ58mT5+fnp\n7rvvlo/P/y1+WVhYqKFDh6p+/fraunWrKlWqZB4lOt9zzz2nBx54wDxqVlBQoCZNmjiM6datm9q3\nb2/W16lTJ+3cuVNdunRRQECAkpKS1L17d0VERGjDhg1atWqVhg8fbr6/Tp06uvbaa+Xj46OXXnrJ\nYU7Fv2RJUo8ePbRixQq9/PLLysvLU6dOnSRJ3bt3V/v27RUbG6tPP/1UvXr1UlFRkb755hsZhqGc\nnBwZhiGLxaK77rpLn332mXbt2qWWLVvqn//8p3Jzc7Vo0SI1aNBATZs2VdOmTXXgwAE1b95c69ev\nl8ViMX9R+O677+Tj46MtW7aoSpUqGjp0qOLi4pSenq6nn35akhQQEKCcnByHOUrSLbfccsGF5rff\nfruuvfZaLViwQI8++qgZLygoUIsWLRQZGakpU6bIz8/P/Frcfvvtys7OVnBwsG699Vb98ssvWrFi\nhY4ePSq73a68vDyFhoaqR48eZr7+/ftr4cKF8vf317hx47R8+XIdOnRIBw4ckNVq1VdffaWBAwcq\nODhYixYtUl5enjmfkJAQNW7cWA8++OAF89m1a5c5rizzyc/P11133XXBnO69916FhoZq3rx52rJl\nS5nm8/777ysoKEh+fn56+umnVbdu3RLn5Gwf/Zn5NGvWrNT5VKtWTfPnz3eYT7Vq1co0n6eeekr1\n6tUr03z4zHnvZ+5///uffvvtN0kq8z4q6TN3/nx8fHw0dOhQLVy4UKtWrSrzPirP+RR/H5U0n/P3\nT0pKikJDQzVw4ECHz0dZP3P5+flu+R668cYbL5hP586dnX4PpaSkqLCw0OV/F86dz9dff21+DQoK\nCnTvvfc61Ll69Wo9/vjj2rZtmxo2bGjG4+PjVXy1R7t27RQREaExY8bo119/lb+/v6pXr67w8HDN\nmDFD8+bNU1FRke6//34FBARo6dKl6tmzpyRpzJgxatiwoSpVqqT8/HwZhqHg4GCdOXNG/v7+kiQ/\nPz899thjeuyxx5SYmKipU6dqypQpCgsLk7+/v6pUqaKioiLVrFlTklS9enWlp6fLbrcrOjpacXFx\nDnPatGmT/Pz8LphPy5YttWPHjjLPZ82aNQoKCjLnlJOTU+KcLBaLwsPDVbVqVU2fPr1M8yn+WmRn\nZ5e6j1ydjyTNnDlTI0eOVF5ennr27FmmfeRsPhs2bFBaWpomT56syZMnl3kfuTqfN998U3a7XUFB\nQapZs+ZV9ZlzNh9JstlsOn36tPn4avnMOZtPMavVesGcWrRooblz56o0l/2ativRqVOnFBcXpy1b\ntigrK0vS2R+WrVq10hNPPOGwsxYsWKCAgADzgyBJX3zxhfnD8txr+lJSUrRw4UJ16tRJS5cu1aFD\nh3T//febr999990KDg5WZmamZs+eraCgIPMXzuzsbPXq1UudO3c2PyB//PGHFi5cKIvFoj59+mj5\n8uVatWqV7Ha7ebrowIEDFR4ern379mndunV64oknzJXCkpKStHnzZt17770Oq4f98MMPqlWrlho3\nbnzRsefGfHx8lJKSYv5F9s/mPHLkiOLj40uNJSYm6tdff1XXrl11+PBhbd++XREREeaKpY0aNdKR\nI0e0fft2SdINN9xQptihQ4cUHx+va6+9ttScxdu3WCwXzXlunTabTRaLxWFseHi4br75ZvNr9s47\n72jw4MEOn83Zs2fr+eefL3PszJkzmj17tsOpVpeas6QanY3ds2ePEhMTVb9+ffNUh7179yoxMVH1\n6tVzOKXDWdxZbM+ePdq9e7caNWpUak5Xt1/SuI0bN6pZs2Zq27at8vPzFRcXpwMHDigoKEi9e/dW\nWFiYzpw5o6VLl2rXrl1q2LChHnnkEfn6+jqNxcXFaefOnWbMarVq8eLFOnDggEJCQvTYY4+pevXq\n5rbOfb/Var0gVtJ2li5datZ5bs6vvvpKP//8s5o2barHH39clStXlnR2sag2bdo4nNLhLObq2DNn\nzmjx4sW6++67S815KduRzv6hb/369QoNDVWLFi20bt067dmzR/n5+erQoYNuuukmrV+/Xnv27FFe\nXp5DbN++fapdu7YqVaqk6tWrq0WLFk7Hrl27Vj/++KOqV69+0Zzr1q3Tvn37VKdOnVJzllTnvn37\nZLPZ5OfnpxMnTsjHx0fh4eG6/vrrtWPHDofrJzp06KDs7Gxt2rTJIe5s7LmxgoIC1atXT02bNr1o\nzvT09Itu/2Kx9PR0Wa1W1alTR5mZmdqyZYvat29v7rsvvvhCFotFnTt3dvgDyNixY5WVlaX333/f\njMXGxsrHx0fHjh3ThAkTzNNQi4qK9PzzzyssLEz33XefZsyYofr168swDKWlpem9997T8ePH9c47\n7yg9PV1RUVHat2+fmjZtqp9++kkDBgxwWPzm3XffVWFhodLT0xUWFiar1ar09HRlZmYqKChINWrU\nUGhoqIKDg/XTTz8pIyPDbKzT09O1ZMkSdejQQc8++6zDfMaOHatx48aZp6O6Op8FCxYoMzNTtWvX\nLnVOp0+fVs2aNdWtWzdzTq7Op0+fPlq8eLGWLl2qXr16XXQflXU+o0aN0qOPPqqvv/66TPvI2XyK\n53T8+HEFBgbK39+/TPvI1fkcP35cQ4YMkY+Pj3nd6NXymXM2nypVqujAgQMaP368br31Vkm6aj5z\nzubTqlUrLV68WHa7XY8++qi2bNlizik/P19jxowxL7EqCU2bm61atco8ZfJisdLGnjlzRikpKapf\nv3655Swp5mzVzrS0NC1btkxZWVkKCgrSU089pdatW+u7777TggUL1LJlSx08eFBPPfWU0tPT9f33\n3zuMdfZ+Z+Nat26tb7/9VgsXLnQp5/Lly81v6pJi5684mpiYqGbNmmnlypUyDENVq1ZVixYtzL+a\n79+/X5UqVVKnTp1cjpWUMzExUYZhXDD2/FhZcjpbMVWStm3bZq4Y2qhRIxmGoe3bt/+pmHT2fPDi\nprc45mw7rm77Yjm3bt1qNqBRUVHmZ6VmzZq65ZZbVKVKFS1btkxt2rTRf/7zH/Xo0UMxMTHmqrAn\nT568YOz5seXLl7uU88cffyxx+6XltNls+v77752uUtuuXTu9+eabatKkiUaNGqUPP/xQAQEBio+P\nV8eOHXX48GHZbLYyxS5nznNX2X300UfVrl07DR48WIGBgapVq5aio6N16623Oo0FBwerT58+Lo0t\nLdahQ4cSt+3KdorfX7xC8JkzZ1S5cmXl5+crMzNTJ06ckN1uV2RkpPLy8pSVlXVBrG3btlq0aJEM\nw1C9evXMFYbPH+tqzvz8fLVp0+aScgYGBmrXrl0KDg5WQECAGjRooJSUFO3bt0/t27dXUlKSudrx\nypUrFRYWptatW5urIDsb62rMXTk3b96sBx54QJmZmeaZOUFBQWrdurXDEQJJ2rFjh4KDg9WgQQMz\nlpiYqOrVq2vlypUOR/uOHTumvXv3qk2bNlq8eLH27NnjcDSyWrVq8vX1VWZmpr7//nsFBwfLbrcr\nLCxMOTk56tKli8O27Xa7Nm7cKIvFonbt2um3337TunXrlJmZqapVq6phw4a644475OPjowMHDuin\nn35Sfn6+pLNn8jRt2lQNGjRwOPp5KfNJTEy84A9mJc2p+NSxcxdMO38+iYmJWrt2rdP5nDlzRnv3\n7tXvv/9+0X30Z+ZTvChNWfaRs/mUdU7n7yNX5yNJO3fu1N69ex3+MO9sH5X3Z64s83HHZ66k+Zw8\neVIJCQkKDAw0DyqUx2du37592r9/v8d+5pzNp/iPhH5+fkpNTdU111zjMKesrKwL1vu4gAG3GjBg\ngEuxsox1d87Y2FgjNzfXGDBggJGammqMGTPG6N+/v5Gbm2uMGjXKjP33v/81YmNjjREjRhiGYVx0\nrKsxd+Ts37+/YbfbjREjRhh//etfjdOnTxuGYRjDhw83YmNjjby8PDNePPfhw4eXOXY5c44ePdqY\nOXOmMWjQICMhIcHYtWuX8eijjxqTJ082vvnmm3KJ9e/f3xg8eLAxefJkY9euXW7PmZCQYCQkJBhj\nx441srKyjFGjRhm5ublGbGysGTMMwxgxYoQRGxtrGIZx0bGuxtyRc9iwYWZs9OjR5vfWsGHDjJEj\nRxqGYZjxYcOGGYZhGCNHjixz7HLmHDVqlGG3241BgwYZ7733nvHMM88YTz75pLFy5Upj06ZNF41N\nmTLFGDhwoHHq1Clj+/btZX6/u7azatUqIzs72+jXr5/5b0RRUZERGxt70VjxZ2bEiBFGYWFhmd/v\njpyxsbFGYWGhMWLECCMvL894+eWXjdjYWCM1NdUYNWqUGTMMwxg6dKi5fy821tWYu3KmpaUZo0aN\nMgCUn8zMTJfj5R1zV86rxWW/pu1KdO6Kl8WOHj0q6ew5+ee+fvToUaex88e6GnNHTmerdvbv319f\nfPGFDMMwY9OnT9eJEycUFhYmSRcd62rMHTmdrTgqSb6+vjIMQwEBAWbcarUqMDBQVqu1zLHLmdPZ\niqmhoaG66aabtHXrVkVGRl5yzN/fX2+99Za+/fZbLVmyRE8++aTbclosFvM0hOKVXg3DUGFhoSSZ\nMUnmogfnrgrrbKyrMXfkdLZK7XXXXafq1asrOTlZksx4vXr1tHTpUvn6+pq37XA1djlzOltld9iw\nYfrf//6nnTt3as6cOSXGtm/frnfffVdDhgzRnDlz1LJlyzK93x3bcbZCsGEYys3NVWFhofLz80uM\nFV9vUVRUpNzc3IuOvVw5pbNLUtvtdnNlY+nsX4fPj1mtVtntdvPf/ouNdTXmjpyVK1fW8ePHNWzY\nMPPeczabTUFBQTp16pSys7PNW+wUn568ffv2i469lNif2c7p06d18uTJi94KSDq70ueLL76o8zmL\nX0qsvHM6u41R8b0NN23aZC4u52qsOOekSZNUt27dUnMuXbpUmzdvLreczm7BtHHjRhUUFKhp06b6\n61//at6WqbCw0CG2adMm1ahRQ1WqVFFgYKDD/RvPH+tqzs2bN6t69eou5ywsLNQNN9zgkPPc20dJ\n0osvvmiutVB8+Y7x/+/7+fLLL5txV2OSnI5ztp2SxpYl5/n3Jp03b5527typ66+/Xn369JG/v78+\n/fRT7dq166IxPz8/zZs3T/v371eVKlU0bNgwhYSE6PTp05o/f75DzuKxpeUsy3aK76ta/LO4T58+\n5j4qCU1bOcjKytK4ceMc/hEeO3asBg0apPfee09jxoxxiAcGBl4QO3+sqzF35Jw1a5Z69OihDz/8\nUJIUGBio6667TkePHtWhQ4fM2NixYzVgwAAdPHjQ3G5JY12NuSNnRESEMjMzdejQIX322WdmXh8f\nH/OXjNdee03S2Qap+HqQqVOnlil2OXP6+Pjojjvu0KpVq7RkyRJVrVpVRUVF6tatm2699VZ9+umn\nlxyz2+3mCnOXI+fYsWPNxXVOnDihnJwcjRkzRhkZGQoLC9OJEydUrVo1nT59WpmZmRo7duxFx7oa\nc0fOAQMG6OOPP9aRI0fk6+ur8ePHKywsTCEhIapfv74GDx4sm82m8ePHq1q1atqzZ4+sVqssFkuZ\nYpczp8Vi0YcffqjnnnvO/LxWrlxZw4YNM0+7KSkWFRWlGjVq6O9//7v5uS7L+92xna5du+r7779X\nQECAHn74Yc2YMUMBAQHq27evAgIC9Oijj5YYq1mzpk6dOqWCggKNHj36omMvV87AwEA9++yzqlu3\nrsaPH68HHnhAubm5Gj16tE6fPm3GJCk6OlpLly7VBx98oL1795Y41tWYu3JOmzZNgYGBmjhxovnL\nzMSJE+Xv7y8/Pz/zGvDMzExNmDBB0tnFyi429lJif3Y7xbdLKOlWQElJSUpMTNTvv//uEJPkEHc1\n5o6cJW3H2W2MDMNQeHi47Ha7NmzYUKbY0KFD9d577ykjI0MPPfTQZc/p7BZMVatWlc1mU2JiohkL\nCQm5IPbCCy/olVdeUVpamm677baLjnU159ixYy8pp7PbR2VkZJjX3hX/gVyS+QcSSWb8UmLu2E5G\nRobeeecd896k8+bNU0hIiPn730cffaTg4GBVq1bN5diYMWP00ksv6aOPPtLo0aM1f/78S87pynbm\nzZtnxjdt2mTGL4amrRzcfPPNysvLczhvNioqSpUrV1bz5s0dzlGNiopSenr6BbHzx7oac0dOZ6t2\nDh48WFarVSkpKWbMarXqtddec/gHvKSxrsbckbOkFUdHjRplrmpUHJ80aZJyc3M1aNCgMscuZ07J\n+Yqp0tl/3GJjY8stdrlynr/a67vvvqv8/HzziG+xmTNnXhBzNtbVmLtynr9KbWhoqPlLXk5Ojo4d\nOydKdSIAAATsSURBVOYQv5TY5cjpbJXd4h+o514X4SxWHHcWc+X97thOSSsE+/v7m0eKLxbr0qWL\neY+gP/N+d+QsXlSqXr165s1amzdvrqSkJIdYjx491Lp1a4dVkEsa62rMHTkzMjLMBQSKnThxQrNm\nzdLQoUPNWEhIiHlT3XP/Uu1s7KXELnU7Jd03LyEhQRaL5YJYUFCQcnJyzLirMXfkLGk7Jd3zb8CA\nAdqxY4fDvSddiRXfAzE0NLTCcp5/v8KffvpJEydO1KhRo3Tq1KkSY9LZlQ0Nw3C4f6Or73dHzpLu\n73nPPfdox44devLJJ80bTz/99NNq1KhRucXcsZ2S7k26bds2devWTWvWrFFaWlqZYtLZe2SmpaWV\nW86ybEeSObY0NG3l4Pwlms+N3XDDDX9qrKsxd+Qs/ovGuT9simPnH7oNCwtz+AtKSWNdjbkjp5+f\n3wVzLGk7fn5+8vP7f+3dsUvzQBgG8KfCZ0DRiiLiUKxIJwWhoCBFnBx11L2Kuqurg4N/RDeXQrsF\nFyd3Bxdxl9pBDAVN1UEIyTf0u8Nq2vS+9GKCzw9cHq7vxasU0yT3/pG3zalkUdYEgNHRUTk2n8+3\n7SSpI4uypmAYRtuJUKdMZWyUNWdmZnx/L3FLa7+yKGt+JrZLDspUxobJVMeKEySg9SC7aC6umoV9\nvY6aQiaTkQ1jg/IwmY6ak5OTME0Ta2tr8jN9bGwMZ2dn8qoB0LoC5v3bU+3l5aXr2DBZ2Hk6tQI6\nPDzE6+urvBVMZEdHRzg9PZV5r5mOmp3m8WtjdHl5iZOTE3llWyUbHx9HqVRqW7coa/q1YBLvueu6\nXTORu64LAMqv11HTr31UKpWS+fn5OSYmJrC1tYXh4WEcHBz0LdMxTyqVgm3b39pficy2bXmS22sm\narqui4uLi77UVJlHfBEk3r9uIm+uTURERBQkn8/j9vYW5XIZlUoFpmnKf2yazSaq1SpM08T19TUW\nFxcxPT2NSqXSdWyYLOw8QOtLuP39fdl/C2h9+z47O9u2o246nUY6ncbc3Jy88thrpqNmp3kajQZy\nuRzm5+dlZts2crkcHh4eZJ+9XrNsNotarYZGo4GNjY3Ia76/v2N1dRV3d3c4Pj4G0LpqOjIygnq9\nLrfI98sAoF6vY2BgAIVCAQsLC0qv11ETaP0NWpaFpaUllEolNJtNbG5uYmhoCCsrK3AcR+bb29t9\nzXTMs76+Dsdx4DgOstksMpkMXNfF29sbbNtGoVBQygzDwPPzM2zbxtTUVF9qqsxjGIZ8hGd5eRld\n/e8OJkREREQ/4erqqqdMZWyY7DfXTPKx66gZ52P/+PjwarXat3F+eb8zXTXDrEdQpqOmyjxfDXQ/\npSMiIiKKl2q12lOmMjZM9ptrJvnYddSM87EPDg7K58M+j/PL+53pqhlmPYIyHTVV5vmKzbWJiIgo\ndoLa6Yh/4ETul30dGybTMU9Saib52LkeXI84rwfQep7t8fER5XIZXQVeiyMiIiKK2O7urnd/f+9Z\nliV/isWid3Nz4+3s7HzLi8Vi4NgwmY55klIzycfO9eB6xHk9LMvynp6evL29vcDPRO4eSURERLHz\nk+10kty2h+vB9eB6JGc9hM9ttjrh7ZFEREREREQxxo1IiIiIiIiIYownbURERERERDHGkzYiIiIi\nIqIY40kbERERERFRjP0Fs69afnHYivEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f566a18da10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import csv\n",
    "import matplotlib\n",
    "import pandas\n",
    "\n",
    "matplotlib.style.use('ggplot')\n",
    "frequencies = []\n",
    "\n",
    "# Build up the frequencies manually, because there may be gaps in the input\n",
    "# that we need to fill in with zeroes.\n",
    "\n",
    "with open('mrjob_534_output.txt', 'r') as output_file:\n",
    "    for row in csv.reader(output_file, delimiter = '\\t'):\n",
    "        index = int(row[0])\n",
    "\n",
    "        for i in range(len(frequencies), index + 1):\n",
    "            frequencies.append(0)\n",
    "\n",
    "        frequencies[index] = int(row[1])\n",
    "\n",
    "# Use Pandas as a convenient way to render a pretty bar chart.\n",
    "\n",
    "df = pandas.DataFrame(data = frequencies, columns = ['Frequency'])\n",
    "df.plot(kind = 'bar', figsize = (15, 4));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 5.3.5 (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Plot the log-log plot of the frequency distribution of unigrams. Does it follow power law distribution?\n",
    "\n",
    "> For more background see:\n",
    "\n",
    "> * https://en.wikipedia.org/wiki/Log%E2%80%93log_plot\n",
    "> * https://en.wikipedia.org/wiki/Power_law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 5.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For the remainder of this assignment you will work with two datasets:\n",
    "\n",
    "> (1) unit/systems test data set: SYSTEMS TEST DATASET\n",
    "\n",
    "> Three terms, A,B,C and their corresponding strip-docs of co-occurring terms\n",
    "\n",
    ">```\n",
    "    DocA {X:20, Y:30, Z:5}\n",
    "    DocB {X:100, Y:20}\n",
    "    DocC {M:5, N:20, Z:5}\n",
    "```\n",
    "\n",
    "> (2) A large subset of the Google n-grams dataset as was described above\n",
    "\n",
    "> In this part of the assignment we will focus on developing methods\n",
    "for detecting synonyms, using the Google 5-grams dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create unit test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir -p unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing unit/test.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile unit/test.txt\n",
    "DocA\t{'X':20, 'Y':30, 'Z':5}\n",
    "DocB\t{'X':100, 'Y':20}\n",
    "DocC\t{'M':5, 'N':20, 'Z':5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -copyFromLocal unit $hdfs_base_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 5.4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (1) Build stripes for the most frequent 10,000 words using cooccurence information based on\n",
    "the words ranked from 1001,-10,000 as a basis/vocabulary (drop stopword-like terms),\n",
    "and output to a file in your bucket on s3 (bigram analysis, though the words are non-contiguous).\n",
    "\n",
    "> **Design Notes**\n",
    "\n",
    "> For this task you will be able to modify the pattern we used in HW 3.2\n",
    "(feel free to use the solution as reference). To total the word counts\n",
    "across the 5-grams, output the support from the mappers using the total\n",
    "order inversion pattern:\n",
    "\n",
    "> ```\n",
    "    <*word,count>\n",
    "```\n",
    "\n",
    "> to ensure that the support arrives before the cooccurrences.\n",
    "\n",
    "> In addition to ensuring the determination of the total word counts,\n",
    "the mapper must also output co-occurrence counts for the pairs of\n",
    "words inside of each 5-gram. Treat these words as a basket,\n",
    "as we have in HW 3, but count all stripes or pairs in both orders,\n",
    "i.e., count both orderings: (word1,word2), and (word2,word1), to preserve\n",
    "symmetry in our output for (2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dictionary file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already generated the dictionary as part of our exploratory data analysis. Extract the dictionary here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!head -10000 mrjob_532_output_sorted.txt > ngrams_vocabulary.txt\n",
    "!head -10000 mrjob_532_output_sorted.txt | tail -1000 > ngrams_basis.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create 5.4.1 job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.protocol import RawProtocol\n",
    "\n",
    "class StripedNGrams(MRJob):\n",
    "    INPUT_PROTOCOL = RawProtocol\n",
    "\n",
    "    \"\"\"\n",
    "    Load the valid dictionary terms.\n",
    "    \"\"\"\n",
    "    def mapper_init(self):\n",
    "        self.basis = set()\n",
    "        self.vocabulary = set()\n",
    "\n",
    "        with open('ngrams_basis.txt', 'r') as basis_file:\n",
    "            for row in csv.reader(basis_file, delimiter = '\\t'):\n",
    "                self.basis.add(row[0])\n",
    "        \n",
    "        with open('ngrams_vocabulary.txt', 'r') as vocabulary_file:\n",
    "            for row in csv.reader(vocabulary_file, delimiter = '\\t'):\n",
    "                self.vocabulary.add(row[0])\n",
    "\n",
    "    \"\"\"\n",
    "    Yield the per-document stripes.\n",
    "    \"\"\"\n",
    "    def mapper(self, ngram, value):\n",
    "        ngram = ngram.lower()\n",
    "        \n",
    "        # Parse the word counts separately based on whether we are checking the\n",
    "        # unit test data or the actual ngrams data.\n",
    "\n",
    "        if value[0] == '{':\n",
    "            word_counts = eval(value)\n",
    "            self.basis.update(word_counts.keys())\n",
    "        else:\n",
    "            split_value = value.split('\\t')\n",
    "            ngram_count = int(split_value[0])\n",
    "\n",
    "            word_counts = {}\n",
    "\n",
    "            for word in ngram.split(' '):\n",
    "                if word not in self.vocabulary:\n",
    "                    continue\n",
    "\n",
    "                if word in word_counts:\n",
    "                    word_counts[word] += ngram_count\n",
    "                else:\n",
    "                    word_counts[word] = ngram_count\n",
    "\n",
    "        # Emit all the stripes derived from the word count. Only include keys\n",
    "        # in the stripe if the key is in the basis.\n",
    "\n",
    "        if len(word_counts) < 2:\n",
    "            return\n",
    "\n",
    "        for word, count in word_counts.iteritems():\n",
    "            stripe = {\n",
    "                key: value for key, value in word_counts.iteritems()\n",
    "                    if key != word and key in self.basis\n",
    "            }\n",
    "\n",
    "            if len(stripe) == 0:\n",
    "                continue\n",
    "            \n",
    "            stripe['*'] = count\n",
    "\n",
    "            yield word, stripe\n",
    "\n",
    "    \"\"\"\n",
    "    Sum the stripes.\n",
    "    \"\"\"\n",
    "    def combiner(self, word, stripes):\n",
    "        yield word, self.combine_stripes(stripes)\n",
    "\n",
    "    \"\"\"\n",
    "    Sum the stripes and then emit the coccurrence rate.\n",
    "    \"\"\"\n",
    "    def reducer(self, word, stripes):\n",
    "        combined_stripe = self.combine_stripes(stripes)\n",
    "        total_count = combined_stripe['*']\n",
    "\n",
    "        rate_stripe = {\n",
    "            key: float(value) / total_count\n",
    "                for key, value in combined_stripe.iteritems() if key != '*'\n",
    "        }\n",
    "\n",
    "        yield word, rate_stripe\n",
    "\n",
    "    \"\"\"\n",
    "    Sum the stripes.\n",
    "    \"\"\"\n",
    "    def combine_stripes(self, stripes):\n",
    "        combined_stripe = {}\n",
    "\n",
    "        for stripe in stripes:\n",
    "            for key, value in stripe.iteritems():\n",
    "                if key in combined_stripe:\n",
    "                    combined_stripe[key] += value\n",
    "                else:\n",
    "                    combined_stripe[key] = value\n",
    "\n",
    "        return combined_stripe\n",
    "\n",
    "if __name__ == '__main__' and sys.argv[0].find('ipykernel') == -1:\n",
    "    StripedNGrams().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run 5.4.1 unit test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Please unit test and system test your code with with SYSTEMS TEST DATASET and show the results. Please compute the expected answer by hand and show your hand calculations. Then show the results you get with your system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We have the following data set for the data set:\n",
    "\n",
    "    DocA\t{X:20, Y:30, Z:5}\n",
    "    DocB\t{X:100, Y:20}\n",
    "    DocC\t{M:5, N:20, Z:5}\n",
    "\n",
    "This yields the following per-document stripes, sorted by word:\n",
    "\n",
    "    M\t{*:5, N:20, Z:5}\n",
    "    N\t{*:20, M:5, Z:5}\n",
    "    X\t{*:20, Y:30, Z:5}\n",
    "    X\t{*:100, Y:20}\n",
    "    Y\t{*:30, X:20, Z:5}\n",
    "    Y\t{*:20, X:100}\n",
    "    Z\t{*:5, X:20, Y:30}\n",
    "    Z\t{*:5, M:5, N:20}\n",
    "\n",
    "If we sum them, this yields the following stripes:\n",
    "\n",
    "    M\t{*:5, N:20, Z:5}\n",
    "    N\t{*:20, M:5, Z:5}\n",
    "    X\t{*:120, Y:50, Z:5}\n",
    "    Y\t{*:50, X:120, Z:5}\n",
    "    Z\t{*:10, M:5, N:20, X:20, Y:30}\n",
    "\n",
    "This results in the following final cooccurences:\n",
    "\n",
    "    M\t{N:20/5, Z:5/5}\n",
    "    N\t{M:5/20, Z:5/20}\n",
    "    X\t{Y:50/120, Z:5/120}\n",
    "    Y\t{X:120/50, Z:5/50}\n",
    "    Z\t{M:5/10, N:20/10, X:20/10, Y:30/10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 157 ms, sys: 26.1 ms, total: 183 ms\n",
      "Wall time: 53.3 s\n"
     ]
    }
   ],
   "source": [
    "%time run_job('StripedNGrams.py --file ngrams_basis.txt --file ngrams_vocabulary.txt', 'mrjob_541', 'unit', 'unit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"M\"\t{\"Z\": 1.0, \"N\": 4.0}\r\n",
      "\"N\"\t{\"Z\": 0.25, \"M\": 0.25}\r\n",
      "\"X\"\t{\"Y\": 0.4166666666666667, \"Z\": 0.041666666666666664}\r\n",
      "\"Y\"\t{\"X\": 2.4, \"Z\": 0.1}\r\n",
      "\"Z\"\t{\"Y\": 3.0, \"X\": 2.0, \"M\": 0.5, \"N\": 2.0}\r\n"
     ]
    }
   ],
   "source": [
    "save_job_output('mrjob_541', 'unit', 'mrjob_541_unit_output.txt')\n",
    "!cat 'mrjob_541_unit_output.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run 5.4.1 ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Finally show your results on the Google n-grams dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 666 ms, sys: 60.2 ms, total: 726 ms\n",
      "Wall time: 2min 58s\n"
     ]
    }
   ],
   "source": [
    "%time run_job('StripedNGrams.py --file ngrams_basis.txt --file ngrams_vocabulary.txt', 'mrjob_541', 'ngrams', 'ngrams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9994 mrjob_541_ngrams_output.txt\r\n"
     ]
    }
   ],
   "source": [
    "save_job_output('mrjob_541', 'ngrams', 'mrjob_541_ngrams_output.txt')\n",
    "!wc -l mrjob_541_ngrams_output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 5.4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In this part of the assignment we will focus on developing methods\n",
    "for detecting synonyms, using the Google 5-grams dataset.\n",
    "\n",
    "> (2) Using two (symmetric) comparison methods of your choice\n",
    "(e.g., correlations, distances, similarities), pairwise compare\n",
    "all stripes (vectors), and output to a file in your bucket on s3.\n",
    "\n",
    "> **Design Notes**\n",
    "\n",
    "> For this task you will have to determine a method of comparison.\n",
    "Here are a few that you might consider:\n",
    "\n",
    "> * Jaccard\n",
    "> * Cosine similarity\n",
    "> * Spearman correlation\n",
    "> * Euclidean distance\n",
    "> * Taxicab (Manhattan) distance\n",
    "> * Shortest path graph distance (a graph, because our data is symmetric!)\n",
    "> * Pearson correlation\n",
    "> * Kendall correlation\n",
    "\n",
    "> However, be cautioned that some comparison methods are more difficult to\n",
    "parallelize than others, and do not perform more associations than is necessary,\n",
    "since your choice of association will be symmetric.\n",
    "\n",
    "> Please use the inverted index (discussed in live session #5) based pattern to compute the pairwise (term-by-term) similarity matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create matrix file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.isdir('matrix'):\n",
    "    # Unit test file is small so it can appear in one file\n",
    "    \n",
    "    !mkdir -p matrix/unit\n",
    "    !cp mrjob_541_unit_output.txt matrix/unit\n",
    "\n",
    "    # NGrams file is large so split it across files that are 100 lines each.\n",
    "    \n",
    "    !mkdir -p matrix/ngrams\n",
    "    !split -l 100 --additional-suffix=.txt mrjob_541_ngrams_output.txt matrix/ngrams/mrjob_541_ngrams_output_\n",
    "\n",
    "    # Copy the folders we created to HDFS.\n",
    "    \n",
    "    !hdfs dfs -copyFromLocal matrix $hdfs_base_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create 5.4.2 job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import functools\n",
    "import sys\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.protocol import JSONProtocol\n",
    "\n",
    "class SynonymDetection(MRJob):\n",
    "    INPUT_PROTOCOL = JSONProtocol\n",
    "\n",
    "    \"\"\"\n",
    "    Allow distance type to be passed through as a configuration option.\n",
    "    \"\"\"\n",
    "    def configure_options(self):\n",
    "        super(SynonymDetection, self).configure_options()\n",
    "        self.add_passthrough_option('--distance-type', type = 'string', default = 'euclidean')\n",
    "\n",
    "    \"\"\"\n",
    "    Normalize and transpose the matrix.\n",
    "    \"\"\"\n",
    "    def mapper_normalize_transpose(self, word, rate_stripe):\n",
    "\n",
    "        # First compute the magnitude for the vector.\n",
    "\n",
    "        magnitude = math.sqrt(sum([value ** 2 for value in rate_stripe.itervalues()]))\n",
    "\n",
    "        # Divide each value in the vector by the magnitude to normalize.\n",
    "        \n",
    "        for key, value in rate_stripe.iteritems():\n",
    "            normalized_value = value / magnitude\n",
    "            yield key, { word: normalized_value }\n",
    "\n",
    "    \"\"\"\n",
    "    Combine the stripes.\n",
    "    \"\"\"\n",
    "    def combiner_normalize_transpose(self, word, transpose_stripes):\n",
    "        yield word, self.combine_stripes(transpose_stripes)\n",
    "\n",
    "    \"\"\"\n",
    "    Combine the stripes.\n",
    "    \"\"\"\n",
    "    def reducer_normalize_transpose(self, word, transpose_stripes):\n",
    "        yield word, self.combine_stripes(transpose_stripes)\n",
    "\n",
    "    \"\"\"\n",
    "    Compute the cosine distance based on the normalized vectors.\n",
    "    \"\"\"\n",
    "    def mapper_cosine(self, word, transpose_stripe):\n",
    "\n",
    "        # Sort the key-value pairs to ensure we are consistent about what\n",
    "        # we emit in our symmetric matrix.\n",
    "\n",
    "        sorted_pairs = sorted(transpose_stripe.iteritems())\n",
    "        \n",
    "        for i in range(0, len(sorted_pairs)):\n",
    "            left_label, left_value = sorted_pairs[i]\n",
    "            \n",
    "            stripe = {}\n",
    "\n",
    "            for j in range(i + 1, len(sorted_pairs)):\n",
    "                right_label, right_value = sorted_pairs[j]\n",
    "                partial_distance = left_value * right_value\n",
    "\n",
    "                if partial_distance > 0:\n",
    "                    stripe[right_label] = partial_distance\n",
    "            \n",
    "            yield left_label, stripe\n",
    "\n",
    "    \"\"\"\n",
    "    Sum the partial distances based on the data contained in the stripe.\n",
    "    \"\"\"\n",
    "    def combiner_cosine(self, left_label, partial_stripes):\n",
    "        yield left_label, self.combine_stripes(partial_stripes)\n",
    "\n",
    "    \"\"\"\n",
    "    Sum the partial distances and take the logarithm for sorting.\n",
    "    \"\"\"\n",
    "    def reducer_cosine(self, left_label, partial_stripes):\n",
    "        total_stripe = self.combine_stripes(partial_stripes)\n",
    "        \n",
    "        for right_label, sum_distance in total_stripe.iteritems():\n",
    "            coordinate = (left_label, right_label)\n",
    "            yield coordinate, math.log(sum_distance)\n",
    "    \n",
    "    \"\"\"\n",
    "    Emit all the pairs as stripes.\n",
    "    \"\"\"\n",
    "    def mapper_jaccard(self, word, rate_stripe):\n",
    "        nonzero_keys = [key for key, value in rate_stripe.iteritems() if value != 0]\n",
    "\n",
    "        # Emit a stripe for each combination of values that we see\n",
    "        # in the stripe.\n",
    "        \n",
    "        sorted_keys = sorted(nonzero_keys)\n",
    "\n",
    "        for i in range(0, len(sorted_keys)):\n",
    "            left_label = sorted_keys[i]\n",
    "            \n",
    "            stripe = {}\n",
    "\n",
    "            for j in range(i + 1, len(sorted_keys)):\n",
    "                right_label = sorted_keys[j]\n",
    "                stripe[right_label] = 1\n",
    "            \n",
    "            yield left_label, stripe\n",
    "        \n",
    "        # Emit another stripe to use to tally the total counts. We will\n",
    "        # use '*' so that it is the first value we see in the reducer.\n",
    "        \n",
    "        yield '*', { key: 1 for key in sorted_keys }\n",
    "    \n",
    "    \"\"\"\n",
    "    Sum the partial distances based on the data contained in the stripe.\n",
    "    \"\"\"\n",
    "    def combiner_jaccard(self, left_label, partial_stripes):\n",
    "        yield left_label, self.combine_stripes(partial_stripes)\n",
    "\n",
    "    \"\"\"\n",
    "    Reducer which aggregates the stripes and then divides by the counts\n",
    "    that are returned by the '*' stripe. Take the logarithm for sorting.\n",
    "    \"\"\"\n",
    "    def reducer_jaccard(self, left_label, partial_stripes):\n",
    "        total_stripe = self.combine_stripes(partial_stripes)\n",
    "\n",
    "        if left_label == '*':\n",
    "            self.total_counts = total_stripe\n",
    "            return\n",
    "    \n",
    "        for right_label, intersection_size in total_stripe.iteritems():\n",
    "            coordinate = (left_label, right_label)\n",
    "            union_size = self.total_counts[left_label] + self.total_counts[right_label]\n",
    "\n",
    "            # Subtract the logarithms instead of taking the logarithm of\n",
    "            # the quotient to avoid potential underflow.\n",
    "            \n",
    "            jaccard_distance = math.log(intersection_size) - math.log(union_size - intersection_size)\n",
    "            \n",
    "            yield coordinate, jaccard_distance\n",
    "        \n",
    "    \"\"\"\n",
    "    Sum the stripes.\n",
    "    \"\"\"\n",
    "    def combine_stripes(self, stripes):\n",
    "        combined_stripe = {}\n",
    "\n",
    "        for stripe in stripes:\n",
    "            for key, value in stripe.iteritems():\n",
    "                if key in combined_stripe:\n",
    "                    combined_stripe[key] += value\n",
    "                else:\n",
    "                    combined_stripe[key] = value\n",
    "\n",
    "        return combined_stripe\n",
    "\n",
    "    \"\"\"\n",
    "    Define multi-step reducer.\n",
    "    \"\"\"\n",
    "    def steps(self):\n",
    "\n",
    "        transpose_step = MRStep(\n",
    "            mapper = self.mapper_normalize_transpose,\n",
    "            combiner = self.combiner_normalize_transpose,\n",
    "            reducer = self.reducer_normalize_transpose)\n",
    "\n",
    "        # For cosine, we need to first take the transpose in order to create\n",
    "        # the inverted index. Then compute the cosine.\n",
    "        \n",
    "        if self.options.distance_type == 'cosine':\n",
    "            distance_step = MRStep(\n",
    "                mapper = self.mapper_cosine,\n",
    "                combiner = self.combiner_cosine,\n",
    "                reducer = self.reducer_cosine)\n",
    "\n",
    "        # For Jaccard, we first take the transpose in order to create the\n",
    "        # inverted index. Since we need the total counts of each set in\n",
    "        # order to compute the score, we limit it to one reducer.\n",
    "\n",
    "        else:\n",
    "            distance_step = MRStep(\n",
    "                mapper = self.mapper_jaccard,\n",
    "                combiner = self.combiner_jaccard,\n",
    "                reducer = self.reducer_jaccard,\n",
    "                jobconf = {\n",
    "                    'mapreduce.job.reduces': 1\n",
    "                })\n",
    "\n",
    "        return [transpose_step, distance_step]\n",
    "\n",
    "if __name__ == '__main__' and sys.argv[0].find('ipykernel') == -1:\n",
    "    SynonymDetection().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run 5.4.2 unit test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Please unit test and system test your code with with SYSTEMS TEST DATASET and show the results. Please compute the expected answer by hand and show your hand calculations. Then show the results you get with your system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.2 unit log-jaccard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the following cooccurrences:\n",
    "\n",
    "    M\t{N:20/5, Z:5/5}\n",
    "    N\t{M:5/20, Z:5/20}\n",
    "    X\t{Y:50/120, Z:5/120}\n",
    "    Y\t{X:120/50, Z:5/50}\n",
    "    Z\t{M:5/10, N:20/10, X:20/10, Y:30/10}\n",
    "\n",
    "We can binarize it as follows:\n",
    "\n",
    "    M\t{N, Z}\n",
    "    N\t{M, Z}\n",
    "    X\t{Y, Z}\n",
    "    Y\t{X, Z}\n",
    "    Z\t{M, N, X, Y}\n",
    "\n",
    "Therefore, we can compute the Jaccard index based on the interaction and the union of the two sets:\n",
    "\n",
    "    M,N = 1/3\n",
    "    M,X = 1/3\n",
    "    M,Y = 1/3\n",
    "    M,Z = 1/5\n",
    "    N,X = 1/3\n",
    "    N,Y = 1/3\n",
    "    N,Z = 1/5\n",
    "    X,Y = 1/3\n",
    "    X,Z = 1/5\n",
    "    Y,Z = 1/5\n",
    "\n",
    "If we then take the logarithm, we would expect the following result:\n",
    "\n",
    "    M,N = -1.0986122886681098\n",
    "    M,X = -1.0986122886681098\n",
    "    M,Y = -1.0986122886681098\n",
    "    M,Z = -1.6094379124341003\n",
    "    N,X = -1.0986122886681098\n",
    "    N,Y = -1.0986122886681098\n",
    "    N,Z = -1.6094379124341003\n",
    "    X,Y = -1.0986122886681098\n",
    "    X,Z = -1.6094379124341003\n",
    "    Y,Z = -1.6094379124341003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 192 ms, sys: 50.6 ms, total: 243 ms\n",
      "Wall time: 1min 18s\n"
     ]
    }
   ],
   "source": [
    "%time run_job('SynonymDetection.py --distance-type=jaccard', 'mrjob_542', 'unit_jaccard', 'matrix/unit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_job_output('mrjob_542', 'unit_jaccard', 'mrjob_542_unit_jaccard_output.txt')\n",
    "!sort -k1 -k2 mrjob_542_unit_jaccard_output.txt > mrjob_542_unit_jaccard_output_sorted.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"M\", \"N\"]\t-1.0986122886681098\r\n",
      "[\"M\", \"X\"]\t-1.0986122886681098\r\n",
      "[\"M\", \"Y\"]\t-1.0986122886681098\r\n",
      "[\"M\", \"Z\"]\t-1.6094379124341003\r\n",
      "[\"N\", \"X\"]\t-1.0986122886681098\r\n",
      "[\"N\", \"Y\"]\t-1.0986122886681098\r\n",
      "[\"N\", \"Z\"]\t-1.6094379124341003\r\n",
      "[\"X\", \"Y\"]\t-1.0986122886681098\r\n",
      "[\"X\", \"Z\"]\t-1.6094379124341003\r\n",
      "[\"Y\", \"Z\"]\t-1.6094379124341003\r\n"
     ]
    }
   ],
   "source": [
    "!cat mrjob_542_unit_jaccard_output_sorted.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.2 unit log-cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the following co-occurrences:\n",
    "\n",
    "    M\t{N:20/5, Z:5/5}\n",
    "    N\t{M:5/20, Z:5/20}\n",
    "    X\t{Y:50/120, Z:5/120}\n",
    "    Y\t{X:120/50, Z:5/50}\n",
    "    Z\t{M:5/10, N:20/10, X:20/10, Y:30/10}\n",
    "\n",
    "If we normalize each of these vectors, we get the following:\n",
    "\n",
    "    M\t{N: 0.9701425001453319, Z: 0.24253562503633297}\n",
    "    N\t{M: 0.7071067811865475, Z: 0.7071067811865475}\n",
    "    X\t{Y: 0.9950371902099892, Z: 0.0995037190209989}\n",
    "    Y\t{X: 0.9991330730923519, Z: 0.04163054471218133}\n",
    "    Z\t{M: 0.1203858530857692, N: 0.4815434123430768, Y: 0.7223151185146152, X: 0.4815434123430768}\n",
    "\n",
    "If we manually compute the cosine using these normalized vectors, we get the following:\n",
    "\n",
    "    M,N = 0.17149858514250882\n",
    "    M,X = 0.024133196686197622\n",
    "    M,Y = 0.010096890182371906\n",
    "    M,Z = 0.467165729979027\n",
    "    N,X = 0.07035975447302917\n",
    "    N,Y = 0.029437240470473188\n",
    "    N,Z = 0.08512565307587484\n",
    "    X,Y = 0.004142394023732023\n",
    "    X,Z = 0.718730405972978\n",
    "    Y,Z = 0.4811259494017159\n",
    "\n",
    "If we take the logarithm of these cosine values to make it easier to sort in MapReduce (in the event that it starts emitting values in scientific notation), we get the following:\n",
    "\n",
    "    M,N = -1.7631802623080808\n",
    "    M,X = -3.724166930448738\n",
    "    M,Y = -4.595527805282158\n",
    "    M,Z = -0.7610712020869566\n",
    "    N,X = -2.6541338487006025\n",
    "    N,Y = -3.5254947235340226\n",
    "    N,Z = -2.4636268425786025\n",
    "    X,Y = -5.48648139167468\n",
    "    X,Z = -0.33026894849715877\n",
    "    Y,Z = -0.7316261940848433"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 218 ms, sys: 47 ms, total: 265 ms\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%time run_job('SynonymDetection.py --distance-type=cosine', 'mrjob_542', 'unit_cosine', 'matrix/unit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_job_output('mrjob_542', 'unit_cosine', 'mrjob_542_unit_cosine_output.txt')\n",
    "!sort -k1 -k2 mrjob_542_unit_cosine_output.txt > mrjob_542_unit_cosine_output_sorted.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"M\", \"N\"]\t-1.7631802623080808\r\n",
      "[\"M\", \"X\"]\t-3.724166930448738\r\n",
      "[\"M\", \"Y\"]\t-4.595527805282158\r\n",
      "[\"M\", \"Z\"]\t-0.7610712020869566\r\n",
      "[\"N\", \"X\"]\t-2.6541338487006025\r\n",
      "[\"N\", \"Y\"]\t-3.5254947235340226\r\n",
      "[\"N\", \"Z\"]\t-2.4636268425786025\r\n",
      "[\"X\", \"Y\"]\t-5.48648139167468\r\n",
      "[\"X\", \"Z\"]\t-0.33026894849715877\r\n",
      "[\"Y\", \"Z\"]\t-0.7316261940848433\r\n"
     ]
    }
   ],
   "source": [
    "!cat mrjob_542_unit_cosine_output_sorted.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run 5.4.2 ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Finally show your results on the Google n-grams dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.2 ngrams log-jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 175 ms, sys: 50.4 ms, total: 225 ms\n",
      "Wall time: 1min 16s\n"
     ]
    }
   ],
   "source": [
    "%time run_job('SynonymDetection.py --distance-type=jaccard', 'mrjob_542', 'ngrams_jaccard', 'matrix/ngrams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_job_output('mrjob_542', 'ngrams_jaccard', 'mrjob_542_ngrams_jaccard_output.txt')\n",
    "!sort -k3nr mrjob_542_ngrams_jaccard_output.txt > mrjob_542_ngrams_jaccard_output_sorted.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"clues\", \"resolving\"]\t-0.916290731874155\r\n",
      "[\"hydroxide\", \"sulphate\"]\t-0.9555114450274365\r\n",
      "[\"ammonium\", \"sulphate\"]\t-1.0414538748281612\r\n",
      "[\"esophagus\", \"mitral\"]\t-1.0986122886681098\r\n",
      "[\"gaza\", \"nationalist\"]\t-1.0986122886681098\r\n",
      "[\"avert\", \"incur\"]\t-1.1349799328389842\r\n",
      "[\"ammonium\", \"filtered\"]\t-1.1526795099383853\r\n",
      "[\"est\", \"qui\"]\t-1.1526795099383853\r\n",
      "[\"ammonium\", \"hydroxide\"]\t-1.1631508098056809\r\n",
      "[\"filtered\", \"sulphate\"]\t-1.2237754316221159\r\n",
      "[\"articular\", \"cartilage\"]\t-1.252762968495368\r\n",
      "[\"articulation\", \"experimentally\"]\t-1.252762968495368\r\n",
      "[\"sabha\", \"singapore\"]\t-1.252762968495368\r\n",
      "[\"ce\", \"est\"]\t-1.2809338454620642\r\n",
      "[\"acetic\", \"hydroxide\"]\t-1.2992829841302609\r\n",
      "[\"amazement\", \"diversion\"]\t-1.2992829841302609\r\n",
      "[\"farthest\", \"remotest\"]\t-1.2992829841302609\r\n",
      "[\"miseries\", \"misfortunes\"]\t-1.3268709406490897\r\n",
      "[\"prosecuted\", \"sued\"]\t-1.33500106673234\r\n",
      "[\"approximated\", \"entropy\"]\t-1.3862943611198904\r\n",
      "[\"approximated\", \"logarithm\"]\t-1.3862943611198904\r\n",
      "[\"backbone\", \"gaza\"]\t-1.3862943611198906\r\n",
      "[\"backbone\", \"nationalist\"]\t-1.3862943611198906\r\n",
      "[\"backbone\", \"randomly\"]\t-1.3862943611198906\r\n",
      "[\"filtered\", \"hydroxide\"]\t-1.3862943611198906\r\n",
      "[\"foreground\", \"transitional\"]\t-1.3862943611198906\r\n",
      "[\"osmotic\", \"randomly\"]\t-1.3862943611198906\r\n",
      "[\"ammonium\", \"insoluble\"]\t-1.4271163556401456\r\n",
      "[\"abyss\", \"insensible\"]\t-1.4350845252893227\r\n",
      "[\"alzheimer's\", \"screening\"]\t-1.4469189829363256\r\n",
      "[\"ovary\", \"placenta\"]\t-1.4469189829363256\r\n",
      "[\"ovary\", \"plexus\"]\t-1.4469189829363256\r\n",
      "[\"peritoneal\", \"placenta\"]\t-1.4469189829363256\r\n",
      "[\"acetic\", \"sulphate\"]\t-1.466337068793427\r\n",
      "[\"cornea\", \"pancreas\"]\t-1.4816045409242158\r\n",
      "[\"alzheimer's\", \"correlate\"]\t-1.504077396776274\r\n",
      "[\"ce\", \"qui\"]\t-1.504077396776274\r\n",
      "[\"abnormalities\", \"morphology\"]\t-1.5040773967762742\r\n",
      "[\"dysfunction\", \"skeletal\"]\t-1.5040773967762742\r\n",
      "[\"lutheran\", \"psychoanalytic\"]\t-1.5040773967762742\r\n",
      "[\"abnormalities\", \"atrophy\"]\t-1.5314763709643886\r\n",
      "[\"complexion\", \"hue\"]\t-1.5314763709643886\r\n",
      "[\"dryness\", \"sulphate\"]\t-1.5404450409471486\r\n",
      "[\"alzheimer's\", \"dementia\"]\t-1.5581446180465497\r\n",
      "[\"ills\", \"vicissitudes\"]\t-1.5581446180465497\r\n",
      "[\"pancreas\", \"placenta\"]\t-1.5581446180465497\r\n",
      "[\"phosphorus\", \"precursor\"]\t-1.5581446180465497\r\n",
      "[\"phosphorus\", \"sulphate\"]\t-1.5581446180465497\r\n",
      "[\"alveolar\", \"skeletal\"]\t-1.5804503755608483\r\n",
      "[\"accrue\", \"fide\"]\t-1.6094379124341003\r\n",
      "[\"acetic\", \"filtered\"]\t-1.6094379124341003\r\n",
      "[\"alveolar\", \"maxillary\"]\t-1.6094379124341003\r\n",
      "[\"annum\", \"purchases\"]\t-1.6094379124341003\r\n",
      "[\"articulation\", \"disruption\"]\t-1.6094379124341003\r\n",
      "[\"backbone\", \"osmotic\"]\t-1.6094379124341003\r\n",
      "[\"chord\", \"incubation\"]\t-1.6094379124341003\r\n",
      "[\"classify\", \"revert\"]\t-1.6094379124341003\r\n",
      "[\"classify\", \"unfamiliar\"]\t-1.6094379124341003\r\n",
      "[\"concede\", \"deduce\"]\t-1.6094379124341003\r\n",
      "[\"confuse\", \"dictate\"]\t-1.6094379124341003\r\n",
      "[\"confuse\", \"formative\"]\t-1.6094379124341003\r\n",
      "[\"correlate\", \"schizophrenia\"]\t-1.6094379124341003\r\n",
      "[\"dietary\", \"uptake\"]\t-1.6094379124341003\r\n",
      "[\"environments\", \"impacts\"]\t-1.6094379124341003\r\n",
      "[\"filtered\", \"soda\"]\t-1.6094379124341003\r\n",
      "[\"ieee\", \"undergraduate\"]\t-1.6094379124341003\r\n",
      "[\"kashmir\", \"sabha\"]\t-1.6094379124341003\r\n",
      "[\"nationals\", \"restrictive\"]\t-1.6094379124341003\r\n",
      "[\"ovary\", \"uterine\"]\t-1.6094379124341003\r\n",
      "[\"palestinian\", \"unprecedented\"]\t-1.6094379124341003\r\n",
      "[\"skeletal\", \"uterine\"]\t-1.6094379124341003\r\n",
      "[\"accrue\", \"dividend\"]\t-1.6094379124341005\r\n",
      "[\"accumulate\", \"maxillary\"]\t-1.6094379124341005\r\n",
      "[\"cons\", \"iran\"]\t-1.6094379124341005\r\n",
      "[\"demonstrating\", \"inferences\"]\t-1.6094379124341005\r\n",
      "[\"electorate\", \"individually\"]\t-1.6094379124341005\r\n",
      "[\"entropy\", \"logarithm\"]\t-1.6094379124341005\r\n",
      "[\"entropy\", \"unrelated\"]\t-1.6094379124341005\r\n",
      "[\"halfway\", \"lobby\"]\t-1.6094379124341005\r\n",
      "[\"monitored\", \"operative\"]\t-1.6094379124341005\r\n",
      "[\"predicting\", \"rating\"]\t-1.6094379124341005\r\n",
      "[\"singapore\", \"sri\"]\t-1.6094379124341005\r\n",
      "[\"arthritis\", \"transplantation\"]\t-1.6486586255873819\r\n",
      "[\"cartilage\", \"necrosis\"]\t-1.6486586255873819\r\n",
      "[\"au\", \"qui\"]\t-1.6582280766035324\r\n",
      "[\"dryness\", \"insoluble\"]\t-1.6582280766035324\r\n",
      "[\"lymphocytes\", \"placenta\"]\t-1.6582280766035324\r\n",
      "[\"shoe\", \"steamer\"]\t-1.6582280766035324\r\n",
      "[\"acetic\", \"ammonium\"]\t-1.6739764335716714\r\n",
      "[\"commonest\", \"congestive\"]\t-1.6739764335716714\r\n",
      "[\"complexion\", \"flush\"]\t-1.6863989535702288\r\n",
      "[\"abnormalities\", \"rabbit\"]\t-1.7047480922384253\r\n",
      "[\"advertisement\", \"intrusted\"]\t-1.7047480922384253\r\n",
      "[\"americas\", \"nova\"]\t-1.7047480922384253\r\n",
      "[\"arkansas\", \"tourism\"]\t-1.7047480922384253\r\n",
      "[\"bombing\", \"brunt\"]\t-1.7047480922384253\r\n",
      "[\"bucket\", \"halfway\"]\t-1.7047480922384253\r\n",
      "[\"bucket\", \"lobby\"]\t-1.7047480922384253\r\n",
      "[\"bucket\", \"upstairs\"]\t-1.7047480922384253\r\n",
      "[\"contributors\", \"painters\"]\t-1.7047480922384253\r\n"
     ]
    }
   ],
   "source": [
    "!head -100 mrjob_542_ngrams_jaccard_output_sorted.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.2 ngrams log-cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 221 ms, sys: 31.4 ms, total: 252 ms\n",
      "Wall time: 1min 19s\n"
     ]
    }
   ],
   "source": [
    "%time run_job('SynonymDetection.py --distance-type=cosine', 'mrjob_542', 'ngrams_cosine', 'matrix/ngrams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_job_output('mrjob_542', 'ngrams_cosine', 'mrjob_542_ngrams_cosine_output.txt')\n",
    "!sort -k3nr mrjob_542_ngrams_cosine_output.txt > mrjob_542_ngrams_cosine_output_sorted.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"remission\", \"sinners\"]\t-0.0032786960928299263\r\n",
      "[\"bacon\", \"cheese\"]\t-0.02775930743649666\r\n",
      "[\"glasgow\", \"lutheran\"]\t-0.03296908447869242\r\n",
      "[\"backbone\", \"randomly\"]\t-0.03503814691209745\r\n",
      "[\"excesses\", \"parked\"]\t-0.03856764883299232\r\n",
      "[\"hundredth\", \"zenith\"]\t-0.03939167143715864\r\n",
      "[\"adviser\", \"disclosure\"]\t-0.04250176481101884\r\n",
      "[\"repaid\", \"testified\"]\t-0.04827938434133423\r\n",
      "[\"officially\", \"rs\"]\t-0.05950863190029749\r\n",
      "[\"humiliation\", \"nuisance\"]\t-0.07533933711020277\r\n",
      "[\"complexion\", \"hue\"]\t-0.08333875146566773\r\n",
      "[\"fossil\", \"spherical\"]\t-0.08375319124259495\r\n",
      "[\"discomfort\", \"nuisance\"]\t-0.08703976109333353\r\n",
      "[\"hay\", \"neat\"]\t-0.09315707176381062\r\n",
      "[\"flourishing\", \"plentiful\"]\t-0.09387858817533977\r\n",
      "[\"weep\", \"wept\"]\t-0.10078934123510001\r\n",
      "[\"fibrous\", \"flexor\"]\t-0.10759708291419438\r\n",
      "[\"hydroxide\", \"purified\"]\t-0.11180066983853443\r\n",
      "[\"flush\", \"roses\"]\t-0.11324335375998704\r\n",
      "[\"funded\", \"individually\"]\t-0.11381486062555761\r\n",
      "[\"crowns\", \"danube\"]\t-0.11556477825388847\r\n",
      "[\"contending\", \"rivalry\"]\t-0.12375334942311102\r\n",
      "[\"futility\", \"matthew\"]\t-0.12939141122971803\r\n",
      "[\"trembled\", \"unprecedented\"]\t-0.13668330267816906\r\n",
      "[\"crosses\", \"uterine\"]\t-0.13775312145537358\r\n",
      "[\"amplifier\", \"cathode\"]\t-0.1386177270116658\r\n",
      "[\"discomfort\", \"humiliation\"]\t-0.14081281266531695\r\n",
      "[\"feathers\", \"serpent\"]\t-0.14365884035518048\r\n",
      "[\"anarchy\", \"realizes\"]\t-0.144411074945735\r\n",
      "[\"explanatory\", \"favorably\"]\t-0.14695207942287505\r\n",
      "[\"annum\", \"trademarks\"]\t-0.14872023166408474\r\n",
      "[\"survivor\", \"uniqueness\"]\t-0.14903226640527367\r\n",
      "[\"printer\", \"probe\"]\t-0.15001271077948908\r\n",
      "[\"terminology\", \"usages\"]\t-0.15610614842568385\r\n",
      "[\"au\", \"est\"]\t-0.15789654226412542\r\n",
      "[\"polarization\", \"probe\"]\t-0.16318020735965927\r\n",
      "[\"acetic\", \"ether\"]\t-0.16764959759253026\r\n",
      "[\"illuminated\", \"probe\"]\t-0.16910179549404578\r\n",
      "[\"nuisance\", \"sack\"]\t-0.17047736427189583\r\n",
      "[\"alzheimer's\", \"screening\"]\t-0.1721996502658673\r\n",
      "[\"boxes\", \"hay\"]\t-0.17244062877473876\r\n",
      "[\"jones\", \"linguistics\"]\t-0.17527410118392334\r\n",
      "[\"boxes\", \"neat\"]\t-0.18507180453697175\r\n",
      "[\"grandson\", \"versailles\"]\t-0.1910510188452996\r\n",
      "[\"displeasure\", \"indebtedness\"]\t-0.19323670730067394\r\n",
      "[\"shouts\", \"turmoil\"]\t-0.197401535312193\r\n",
      "[\"disposing\", \"phosphorus\"]\t-0.19786175409476753\r\n",
      "[\"axe\", \"steering\"]\t-0.1989748456154889\r\n",
      "[\"clues\", \"socioeconomic\"]\t-0.20018723656568782\r\n",
      "[\"shells\", \"transitional\"]\t-0.20034170330064385\r\n",
      "[\"frost\", \"indiana\"]\t-0.2015296533533511\r\n",
      "[\"calamity\", \"impending\"]\t-0.20350364641322183\r\n",
      "[\"buffer\", \"terminals\"]\t-0.20552354174361334\r\n",
      "[\"compounded\", \"monitored\"]\t-0.2100024030209659\r\n",
      "[\"polarization\", \"printer\"]\t-0.2160580634487107\r\n",
      "[\"avenge\", \"efficiently\"]\t-0.21667253073909498\r\n",
      "[\"correlate\", \"drained\"]\t-0.21932552501530114\r\n",
      "[\"autobiography\", \"crazy\"]\t-0.22173725558315346\r\n",
      "[\"illuminated\", \"printer\"]\t-0.22197965158309724\r\n",
      "[\"diplomacy\", \"survivor\"]\t-0.2231291059971503\r\n",
      "[\"humiliation\", \"sack\"]\t-0.22425041584387917\r\n",
      "[\"kinship\", \"unfamiliar\"]\t-0.22543320222504148\r\n",
      "[\"inspector\", \"ordinances\"]\t-0.22945762675979348\r\n",
      "[\"articular\", \"experimentally\"]\t-0.23005316083390642\r\n",
      "[\"cooled\", \"freshness\"]\t-0.23172616842418195\r\n",
      "[\"concede\", \"hampered\"]\t-0.2349736098705823\r\n",
      "[\"illuminated\", \"polarization\"]\t-0.23514714816326746\r\n",
      "[\"discomfort\", \"sack\"]\t-0.2359508398270099\r\n",
      "[\"confidential\", \"consultant\"]\t-0.23627731392800902\r\n",
      "[\"individually\", \"sued\"]\t-0.2389333123267986\r\n",
      "[\"arkansas\", \"southeastern\"]\t-0.2398223922206708\r\n",
      "[\"implementing\", \"relies\"]\t-0.24096719643465572\r\n",
      "[\"excitation\", \"probe\"]\t-0.24799659975325594\r\n",
      "[\"lighting\", \"wasting\"]\t-0.2505854866577924\r\n",
      "[\"extracellular\", \"pancreas\"]\t-0.2542504282522134\r\n",
      "[\"clan\", \"kashmir\"]\t-0.254366725585451\r\n",
      "[\"ce\", \"est\"]\t-0.25519899458551226\r\n",
      "[\"holocaust\", \"individuality\"]\t-0.25560495882036643\r\n",
      "[\"dame\", \"linguistics\"]\t-0.2576311636094004\r\n",
      "[\"designer\", \"frost\"]\t-0.2586655316587161\r\n",
      "[\"inspect\", \"inspector\"]\t-0.26248847191125574\r\n",
      "[\"au\", \"ce\"]\t-0.27160230495092774\r\n",
      "[\"clubs\", \"sabha\"]\t-0.2727446553924146\r\n",
      "[\"tray\", \"whoever\"]\t-0.2775266604797696\r\n",
      "[\"dame\", \"jones\"]\t-0.2799023451134225\r\n",
      "[\"alzheimer's\", \"rating\"]\t-0.2808288382777394\r\n",
      "[\"bedside\", \"foe\"]\t-0.28143203779418485\r\n",
      "[\"cognizance\", \"interpreter\"]\t-0.28311583544456576\r\n",
      "[\"canons\", \"glasgow\"]\t-0.2901007034314636\r\n",
      "[\"coating\", \"negation\"]\t-0.29134382574502576\r\n",
      "[\"canons\", \"lutheran\"]\t-0.2917086847663463\r\n",
      "[\"precursor\", \"radioactive\"]\t-0.2923929949437485\r\n",
      "[\"buenos\", \"peru\"]\t-0.292431751833597\r\n",
      "[\"comprehended\", \"lawn\"]\t-0.2950341249927408\r\n",
      "[\"contradictory\", \"substantive\"]\t-0.29544257627055154\r\n",
      "[\"contamination\", \"mast\"]\t-0.2989955749857697\r\n",
      "[\"bucket\", \"lobby\"]\t-0.3000558405085481\r\n",
      "[\"designer\", \"indiana\"]\t-0.3017333828043913\r\n",
      "[\"mast\", \"sac\"]\t-0.30202981791573913\r\n",
      "[\"contradict\", \"ethic\"]\t-0.3077989072265286\r\n"
     ]
    }
   ],
   "source": [
    "!head -100 mrjob_542_ngrams_cosine_output_sorted.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 5.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In this part of the assignment your will evaluate the success of you synonym detector.\n",
    "Take the top 1,000 closest/most similar/correlative pairs of words as determined\n",
    "by your measure in (2), and use the synonyms function in the accompanying\n",
    "python code:\n",
    "\n",
    "> ``nltk_synonyms.py``\n",
    "\n",
    "> Note: This will require installing the python nltk package:\n",
    "\n",
    "> http://www.nltk.org/install.html\n",
    "\n",
    "> and downloading its data with ``nltk.download()``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download verifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download synonym script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!wget --quiet https://www.dropbox.com/sh/0cv65h44zylqwe3/AADbmhKuESCLaV_IntPhO2a2a/nltk_synonyms.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For each (word1,word2) pair, check to see if word1 is in the list,\n",
    "synonyms(word2), and vice-versa. If one of the two is a synonym of the other,\n",
    "then consider this pair a 'hit', and then report the precision, recall, and F1 measure  of\n",
    "your detector across your 1,000 best guesses. Report the macro averages of these measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Please unit test and system test your code with with SYSTEMS TEST DATASET and show the results. Please compute the expected answer by hand and show your hand calculations. Then show the results you get with your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Finally show your results on the Google n-grams dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 5.5.1 (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There is also a corpus of stopwords, that is, high-frequency words like \"the\", \"to\" and \"also\" that we sometimes want to filter out of a document before further processing. Stopwords usually have little lexical content, and their presence in a text fails to distinguish it from other texts.\n",
    "\n",
    "> Python's nltk comes with a prebuilt list of stopwords (see below). Using this stopword list filter out these tokens from your analysis and rerun the experiments in 5.5 and disucuss the results of using a stopword list and without using a stopword list.\n",
    "\n",
    "> ```\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 5.6 (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There are many good ways to build our synonym detectors, so for optional homework,\n",
    "measure co-occurrence by (left/right/all) consecutive words only,\n",
    "or make stripes according to word co-occurrences with the accompanying\n",
    "2-, 3-, or 4-grams (note here that your output will no longer\n",
    "be interpretable as a network) inside of the 5-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 5.7 (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Once again, benchmark your top 10,000 associations (as in 5.5), this time for your\n",
    "results from 5.6. Has your detector improved?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": false,
   "toc_threshold": 4,
   "toc_window_display": true
  },
  "toc_position": {
   "left": "1623.83px",
   "right": "20px",
   "top": "121px",
   "width": "200px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
