{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook requires some nbextensions.\n",
    "\n",
    "* [toc2](https://github.com/ipython-contrib/IPython-notebook-extensions/tree/master/nbextensions/usability/toc2) provides a button to create a floating table of contents\n",
    "* [toggle_all_line_numbers](https://github.com/ipython-contrib/IPython-notebook-extensions/tree/master/nbextensions/usability/toggle_all_line_numbers) provides a button to see line numbers for all code cells\n",
    "* [autosaveclasses](https://github.com/holatuwol/jupyter-magic/tree/master/nbextensions/autosaveclasses.js) avoids usage of `%%writefile` (cells with a class definition are saved to disk when run)\n",
    "\n",
    "If they are not yet installed, run the following cell and restart the notebook server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "nbextdl() {\n",
    "    mkdir -p $(ipython locate)/nbextensions/$(dirname $2)\n",
    "    curl --silent -L \\\n",
    "        \"https://raw.githubusercontent.com/$1/master/nbextensions/$2\" \\\n",
    "        > \"$(ipython locate)/nbextensions/$2\"\n",
    "}\n",
    "\n",
    "nbextdl ipython-contrib/IPython-notebook-extensions usability/toc2/main.js\n",
    "nbextdl ipython-contrib/IPython-notebook-extensions usability/toc2/main.css\n",
    "nbextdl ipython-contrib/IPython-notebook-extensions usability/toc2/icon.png\n",
    "nbextdl ipython-contrib/IPython-notebook-extensions usability/toc2/image.png\n",
    "\n",
    "nbextdl ipython-contrib/IPython-notebook-extensions usability/toggle_all_line_numbers/main.js\n",
    "nbextdl ipython-contrib/IPython-notebook-extensions usability/toggle_all_line_numbers/icon.png\n",
    "\n",
    "nbextdl holatuwol/jupyter-magic autosaveclasses.js"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the extensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "require(['base/js/utils'], function(utils) {\n",
       "    utils.load_extensions('usability/toc2/main');\n",
       "    utils.load_extensions('usability/toggle_all_line_numbers/main');\n",
       "    utils.load_extensions('autosaveclasses');\n",
       "});"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "require(['base/js/utils'], function(utils) {\n",
    "    utils.load_extensions('usability/toc2/main');\n",
    "    utils.load_extensions('usability/toggle_all_line_numbers/main');\n",
    "    utils.load_extensions('autosaveclasses');\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also set the base folder to use for storing files in HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdfs_base_folder = '/tmp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 4.0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What is MrJob? How is it different to Hadoop MapReduce? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MRJob is a framework provided by Yelp as an alternate way to write Hadoop streaming jobs.\n",
    "\n",
    "It allows you to write the mappers, combiners, and reducers as a single Python class. In doing so, it eliminates the boilerplate line parsing code found in Hadoop streaming jobs. It also provides a \"map reduce step\" (``MRStep``) abstraction that allows you to chain together a pre-determined set of map reduce steps.\n",
    "\n",
    "MRJob is more limited than standard MapReduce in the sense that only those abstractions that have been implemented in the framework are available (for example, secondary sort does not work cleanly)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What are the ``mapper_init()``, ``mapper_final()``, ``combiner_init()``, ``combiner_final()``, ``reducer_init()``, ``reducer_final()`` methods? When are they called?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``mapper_init``, ``combiner_init``, and ``reducer_init`` are called before the mapper, combiner, and reducer phases, respectively. ``mapper_final``, ``combiner_final``, and ``reducer_final`` are called after the mapper, combiner, and reducer phases, respectively.\n",
    "\n",
    "These functions are used as part of the setup and tear-down of a MapReduce phase. They are equivalent to the commands that run before and after the stream iteration loop in a traditional Hadoop streaming job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What is serialization in the context of MrJob or Hadoop?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serialization is the process of encoding the data between each of the different phases of a MapReduce job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> When it used in these frameworks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serialization is used whenever data is written to persistent storage (such as in creating spill files or creating output files) or transmitted over over the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What is the default serialization mode for input and outputs for MrJob?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The default serialization mode for the input protocol (data sent during the first mapping phase) is ``RawValueProtocol``, which assumes that there are no keys and the whole line should be treated as the value.\n",
    "* The default serialization mode for internal communication (data sent between phases) is ``JSONProtocol`` which encodes the data in JSON.\n",
    "* The default serialization mode for the output protocol (the final step) is ``JSONProtocol`` which encodes the data in JSON."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Recall the Microsoft logfiles data from the async lecture. The logfiles are described are located at:\n",
    "\n",
    "> * https://kdd.ics.uci.edu/databases/msweb/msweb.html\n",
    "> * http://archive.ics.uci.edu/ml/machine-learning-databases/anonymous/\n",
    "\n",
    "> This dataset records which areas (Vroots) of www.microsoft.com each user visited in a one-week timeframe in Feburary 1998.\n",
    "\n",
    "> Here, you must preprocess the data on a single node (i.e., not on a cluster of nodes) from the format:\n",
    "\n",
    "    C,\"10001\",10001   #Visitor id 10001\n",
    "    V,1000,1          #Visit by Visitor 10001 to page id 1000\n",
    "    V,1001,1          #Visit by Visitor 10001 to page id 1001\n",
    "    V,1002,1          #Visit by Visitor 10001 to page id 1002\n",
    "    C,\"10002\",10002   #Visitor id 10001\n",
    "\n",
    "> Note: #denotes comments\n",
    "\n",
    "> to the format:\n",
    "\n",
    "    V,1000,1,C,10001\n",
    "    V,1001,1,C,10001\n",
    "    V,1002,1,C,10001\n",
    "\n",
    "> Write the python code to accomplish this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the file from Dropbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!wget --quiet https://www.dropbox.com/sh/m0nxsf4vs5cyrp2/AADCHtrJ4CBCDO1po_OAWg0ia/anonymous-msweb.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformat the data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from csv import reader\n",
    "\n",
    "input_file_name = 'anonymous-msweb.data'\n",
    "\n",
    "visitor_file = open(input_file_name + '.visitor', 'w')\n",
    "webpage_file = open(input_file_name + '.webpage', 'w')\n",
    "\n",
    "with open(input_file_name) as input_file:\n",
    "    case_id = None\n",
    "\n",
    "    # Update the case ID when we see a new case (line with a C).\n",
    "    # Output the vote when we see a new vote (line with a V).\n",
    "    # Skip all other lines.\n",
    "\n",
    "    for row in reader(input_file):\n",
    "        if row[0] == 'A':\n",
    "            print >> webpage_file, ','.join(row)\n",
    "            continue\n",
    "\n",
    "        if row[0] == 'C':\n",
    "            case_id = row[1]\n",
    "            continue\n",
    "\n",
    "        if row[0] == 'V':\n",
    "            row.extend(['C', case_id])\n",
    "            print >> visitor_file, ','.join(row)\n",
    "\n",
    "visitor_file.close()\n",
    "webpage_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V,1000,1,C,10001\r\n",
      "V,1001,1,C,10001\r\n",
      "V,1002,1,C,10001\r\n",
      "V,1001,1,C,10002\r\n",
      "V,1003,1,C,10002\r\n",
      "V,1001,1,C,10003\r\n",
      "V,1003,1,C,10003\r\n",
      "V,1004,1,C,10003\r\n",
      "V,1005,1,C,10004\r\n",
      "V,1006,1,C,10005\r\n"
     ]
    }
   ],
   "source": [
    "!head anonymous-msweb.data.visitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Find the 5 most frequently visited pages using MrJob from the output of 4.2 (i.e., transformed log file)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "class mrjob_43(MRJob):\n",
    "    \n",
    "    \"\"\"\n",
    "    Mapper yields the vroot and a 1 (count).\n",
    "    \"\"\"\n",
    "    def mapper_get_visits(self, _, line):\n",
    "        row = csv.reader([line]).next()\n",
    "\n",
    "        if len(row) != 5:\n",
    "            return\n",
    "\n",
    "        vroot_id = row[1]\n",
    "        yield vroot_id, 1\n",
    "    \n",
    "    \"\"\"\n",
    "    Combiner sums the visit counts for the given vroot.\n",
    "    \"\"\"\n",
    "    def combiner_get_visits(self, vroot_id, visits):\n",
    "        total_visits = sum(visits)\n",
    "        yield vroot_id, total_visits\n",
    "    \n",
    "    \"\"\"\n",
    "    Reducer sums the visit counts for the given vroot.\n",
    "    \"\"\"\n",
    "    def reducer_get_visits(self, vroot_id, visits):\n",
    "        total_visits = sum(visits)\n",
    "        yield vroot_id, total_visits\n",
    "\n",
    "    \"\"\"\n",
    "    Mapper pushes all the vroot-visits pairs to the same reducer, flipping the\n",
    "    tuple. Pad the number with zeros so that MRJob reverse order works.\n",
    "    \"\"\"\n",
    "    def mapper_top5(self, vroot_id, visits):\n",
    "        padded_visits = '%06d' % (visits)\n",
    "        yield None, (padded_visits, vroot_id)\n",
    "\n",
    "    \"\"\"\n",
    "    Initialize the webpage URLs.\n",
    "    \"\"\"\n",
    "    def reducer_top5_init(self):\n",
    "        self.vroot_names = {}\n",
    "        \n",
    "        with open('anonymous-msweb.data.webpage', 'r') as webpage_file:\n",
    "            for row in csv.reader(webpage_file):\n",
    "                if len(row) != 5:\n",
    "                    continue\n",
    "\n",
    "                vroot_id = row[1]\n",
    "                vroot_name = row[3].strip()\n",
    "\n",
    "                self.vroot_names[vroot_id] = vroot_name\n",
    "        \n",
    "    \"\"\"\n",
    "    Reducer identifies the top 5 by relying on the job configuration's sort and\n",
    "    simply emitting the first 5. Also make sure to remove the padded zeros.\n",
    "    \"\"\"\n",
    "    def reducer_top5(self, _, pairs):\n",
    "        emits_remaining = 5\n",
    "        \n",
    "        for pair in pairs:\n",
    "            if emits_remaining == 0:\n",
    "                continue\n",
    "            \n",
    "            emits_remaining -= 1\n",
    "\n",
    "            visits = int(pair[0])\n",
    "\n",
    "            vroot_id = pair[1]\n",
    "            vroot_name = self.vroot_names[vroot_id]\n",
    "            \n",
    "            yield visits, vroot_name\n",
    "\n",
    "    \"\"\"\n",
    "    Declare the two-step map reduce. Since we are using None as the key, it\n",
    "    won't get emitted, and -k1 corresponds to the value.\n",
    "    \"\"\"\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper = self.mapper_get_visits,\n",
    "                combiner = self.combiner_get_visits,\n",
    "                reducer = self.reducer_get_visits),\n",
    "            MRStep(\n",
    "                mapper = self.mapper_top5,\n",
    "                reducer_init = self.reducer_top5_init,\n",
    "                reducer = self.reducer_top5,\n",
    "                jobconf = {\n",
    "                    'stream.num.map.output.key.fields' : 2,\n",
    "                    'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                    'mapred.text.key.comparator.options': '-k1r',\n",
    "                    'mapred.reduce.tasks': 1\n",
    "                })\n",
    "        ]\n",
    "\n",
    "if __name__ == '__main__' and sys.argv[0].find('ipykernel') == -1:\n",
    "    mrjob_43().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r -f -skipTrash $hdfs_base_folder/mrjob_43_output > /dev/null\n",
    "\n",
    "!python mrjob_43.py -r hadoop \\\n",
    "    --strict-protocols \\\n",
    "    --file=anonymous-msweb.data.webpage \\\n",
    "    --output-dir=$hdfs_base_folder/mrjob_43_output \\\n",
    "    --no-output \\\n",
    "    anonymous-msweb.data.visitor \\\n",
    "    > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirm the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -cat $hdfs_base_folder/mrjob_43_output/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 4.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Find the most frequent visitor of each page using MrJob and the output of 4.2  (i.e., transfromed log file). In this output please include the webpage URL, webpageID and Visitor ID."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "class mrjob_44(MRJob):\n",
    "    \n",
    "    \"\"\"\n",
    "    Mapper yields the vroot and the visitor.\n",
    "    \"\"\"\n",
    "    def mapper_top1(self, _, line):\n",
    "        row = csv.reader([line]).next()\n",
    "\n",
    "        vroot_id = row[1]\n",
    "        visitor_id = row[4]\n",
    "        \n",
    "        yield vroot_id, visitor_id\n",
    "\n",
    "    \"\"\"\n",
    "    Initialize the webpage URLs.\n",
    "    \"\"\"\n",
    "    def reducer_top1_init(self):\n",
    "        self.vroot_urls = {}\n",
    "        \n",
    "        with open('anonymous-msweb.data.webpage', 'r') as webpage_file:\n",
    "            for row in csv.reader(webpage_file):\n",
    "                if len(row) != 5:\n",
    "                    continue\n",
    "\n",
    "                vroot_id = row[1]\n",
    "                vroot_url = row[4].strip()\n",
    "\n",
    "                self.vroot_urls[vroot_id] = vroot_url\n",
    "        \n",
    "    \"\"\"\n",
    "    Reducer finds the most frequent visitor for the given vroot.\n",
    "    \"\"\"\n",
    "    def reducer_top1(self, vroot_id, visitor_ids):\n",
    "        top_visitor = { 'id': None, 'visits': 0 }\n",
    "        current_visitor = { 'id': None, 'visits': 0 }\n",
    "        \n",
    "        for visitor_id in visitor_ids:\n",
    "            if visitor_id == current_visitor['id']:\n",
    "                current_visitor['visits'] += 1\n",
    "                continue\n",
    "            \n",
    "            if current_visitor['visits'] > top_visitor['visits']:\n",
    "                top_visitor = current_visitor\n",
    "            \n",
    "            current_visitor = { 'id': visitor_id, 'visits': 1 }\n",
    "        \n",
    "        if current_visitor['visits'] > top_visitor['visits']:\n",
    "            top_visitor = current_visitor\n",
    "\n",
    "        vroot_url = self.vroot_urls[vroot_id]\n",
    "        yield (vroot_id, vroot_url), top_visitor['id']\n",
    "\n",
    "    \"\"\"\n",
    "    Declare the one-step map reduce.\n",
    "    \"\"\"\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper = self.mapper_top1,\n",
    "                reducer_init = self.reducer_top1_init,\n",
    "                reducer = self.reducer_top1,\n",
    "                jobconf = {\n",
    "                    'stream.num.map.output.key.fields' : 2,\n",
    "                    'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                    'mapred.text.key.comparator.options': '-k1 -k2'\n",
    "                })\n",
    "        ]\n",
    "        \n",
    "if __name__ == '__main__' and sys.argv[0].find('ipykernel') == -1:\n",
    "    mrjob_44().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r -f -skipTrash $hdfs_base_folder/mrjob_44_output > /dev/null\n",
    "\n",
    "!python mrjob_44.py -r hadoop \\\n",
    "    --strict-protocols \\\n",
    "    --file=anonymous-msweb.data.webpage \\\n",
    "    --output-dir=$hdfs_base_folder/mrjob_44_output \\\n",
    "    --no-output \\\n",
    "    anonymous-msweb.data.visitor \\\n",
    "    > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirm the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -cat $hdfs_base_folder/mrjob_44_output/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 4.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here you will use a different dataset consisting of word-frequency distributions \n",
    "for 1,000 Twitter users. These Twitter users use language in very different ways,\n",
    "and were classified by hand according to the criteria:\n",
    "\n",
    "> * 0: Human, where only basic human-human communication is observed.\n",
    "> * 1: Cyborg, where language is primarily borrowed from other sources (e.g., jobs listings, classifieds postings, advertisements, etc...).\n",
    "> * 2: Robot, where language is formulaically derived from unrelated sources (e.g., weather/seismology, police/fire event logs, etc...).\n",
    "> * 3: Spammer, where language is replicated to high multiplicity (e.g., celebrity obsessions, personal promotion, etc... )\n",
    "\n",
    "> Check out the preprints of our recent research,\n",
    "which spawned this dataset:\n",
    "\n",
    "> * http://arxiv.org/abs/1505.04342\n",
    "> * http://arxiv.org/abs/1508.01843\n",
    "\n",
    "> The main data lie in the accompanying file:\n",
    "\n",
    "> `topUsers_Apr-Jul_2014_1000-words.txt`\n",
    "\n",
    "> and are of the form:\n",
    "\n",
    "    USERID,CODE,TOTAL,WORD1_COUNT,WORD2_COUNT,...\n",
    "\n",
    "> where\n",
    "\n",
    "> * USERID = unique user identifier\n",
    "> * CODE = 0/1/2/3 class code\n",
    "> * TOTAL = sum of the word counts\n",
    "\n",
    "> Using this data, you will implement a 1000-dimensional K-means algorithm in MrJob on the users\n",
    "by their 1000-dimensional word stripes/vectors using several \n",
    "centroid initializations and values of K.\n",
    "\n",
    "> Note that each \"point\" is a user as represented by 1000 words, and that\n",
    "word-frequency distributions are generally heavy-tailed power-laws\n",
    "(often called Zipf distributions), and are very rare in the larger class\n",
    "of discrete, random distributions. For each user you will have to normalize\n",
    "by its \"TOTAL\" column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the files from Dropbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!wget --quiet https://www.dropbox.com/sh/m0nxsf4vs5cyrp2/AACBUw7kflSmuQJ7jn-uBMV1a/topUsers_Apr-Jul_2014_1000-words.txt\n",
    "!wget --quiet https://www.dropbox.com/sh/m0nxsf4vs5cyrp2/AAD5G0PHKMgdqTPC1w-2rR2ya/topUsers_Apr-Jul_2014_1000-words_summaries.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "import csv\n",
    "import math\n",
    "import sys\n",
    "\n",
    "class KMeansJob(MRJob):\n",
    "\n",
    "    \"\"\"\n",
    "    Load the centroids.\n",
    "    \"\"\"\n",
    "    def mapper_init(self):\n",
    "        self.centroids = {}\n",
    "        \n",
    "        with open('centroids.txt', 'r') as centroids_file:\n",
    "            for centroid_row in csv.reader(centroids_file):\n",
    "                if len(centroid_row) <= 3:\n",
    "                    continue\n",
    "                    \n",
    "                key = centroid_row[0]\n",
    "                point = [float(x) for x in centroid_row[1:]]\n",
    "                self.centroids[key] = point\n",
    "\n",
    "    \"\"\"\n",
    "    Yield the cluster and the point associated with that prediction so that\n",
    "    the next iteration can average it.\n",
    "    \"\"\"\n",
    "    def mapper(self, _, line):\n",
    "        row = csv.reader([line]).next()\n",
    "        \n",
    "        if len(row) <= 3:\n",
    "            return\n",
    "\n",
    "        total = int(row[2])\n",
    "        point = [float(x) / total for x in row[3:]]\n",
    "        centroid = self.get_closest_centroid(point)\n",
    "        \n",
    "        yield centroid, [1] + point\n",
    "    \n",
    "    \"\"\"\n",
    "    Combine the counts that are being emitted.\n",
    "    \"\"\"\n",
    "    def combiner(self, centroid, data):\n",
    "        yield centroid, self.get_totals(data)\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns the sum of all the features described in pairs.\n",
    "    \"\"\"\n",
    "    def get_totals(self, data):\n",
    "        total = None\n",
    "\n",
    "        for datum in data:\n",
    "            if total is None:\n",
    "                total = datum\n",
    "            else:\n",
    "                total = [x + y for x, y in zip(total, datum)]\n",
    "        \n",
    "        return total\n",
    "    \n",
    "    \"\"\"\n",
    "    Find the new point to use for the centroid.\n",
    "    \"\"\"\n",
    "    def reducer(self, centroid, data):\n",
    "        totals = self.get_totals(data)\n",
    "        \n",
    "        point_count = totals[0]\n",
    "        point_sum = totals[1:]\n",
    "        \n",
    "        mean_point = [x / point_count for x in point_sum]\n",
    "\n",
    "        yield centroid, mean_point\n",
    "        \n",
    "    \"\"\"\n",
    "    Return the ID of the centroid closest to the given row.\n",
    "    \"\"\"\n",
    "    def get_closest_centroid(self, point):\n",
    "        minimum_key = None\n",
    "        minimum_distance = sys.maxint\n",
    "        \n",
    "        for key, centroid in self.centroids.iteritems():\n",
    "            distance = self.get_distance(point, centroid)\n",
    "            \n",
    "            if distance < minimum_distance:\n",
    "                minimum_key = key\n",
    "                minimum_distance = distance\n",
    "        \n",
    "        return minimum_key        \n",
    "    \n",
    "    \"\"\"\n",
    "    Returns the distance between the two points. For now use Euclidean\n",
    "    distance.\n",
    "    \"\"\"\n",
    "    def get_distance(self, values1, values2):\n",
    "        difference = [x - y for x, y in zip(values1, values2)]\n",
    "        squared_difference = [x * x for x in difference]\n",
    "        sum_squared_difference = sum(squared_difference)\n",
    "        euclidean_distance = math.sqrt(sum_squared_difference)\n",
    "        \n",
    "        return euclidean_distance\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper_init = self.mapper_init,\n",
    "                mapper = self.mapper,\n",
    "                combiner = self.combiner,\n",
    "                reducer = self.reducer)\n",
    "        ]\n",
    "\n",
    "if __name__ == '__main__' and sys.argv[0].find('ipykernel') == -1:\n",
    "    KMeansJob().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "# If we have just one counter, everything should categorize to it. Make sure\n",
    "# the map/reduce job can handle this degenerate case.\n",
    "\n",
    "with open('centroids.txt', 'w') as centroid_file:\n",
    "    print >> centroid_file, ','.join([str(x) for x in numpy.repeat(0, 1001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /tmp/mrjob_45_output\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r -f -skipTrash $hdfs_base_folder/mrjob_45_output\n",
    "\n",
    "!python KMeansJob.py -r hadoop \\\n",
    "    --strict-protocols \\\n",
    "    --file=centroids.txt \\\n",
    "    --output-dir=$hdfs_base_folder/mrjob_45_output \\\n",
    "    --no-output \\\n",
    "    topUsers_Apr-Jul_2014_1000-words.txt \\\n",
    "    > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirm the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   1 ubuntu hadoop          0 2016-02-10 09:14 /tmp/mrjob_45_output/_SUCCESS\r\n",
      "-rw-r--r--   1 ubuntu hadoop      23182 2016-02-10 09:14 /tmp/mrjob_45_output/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls $hdfs_base_folder/mrjob_45_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Try several parameterizations and initializations and iterate until a threshold (try 0.001) is reached.\n",
    "\n",
    "> Note that you do not have to compute the aggregated distribution or the \n",
    "class-aggregated distributions, which are rows in the auxiliary file:\n",
    "\n",
    "> `topUsers_Apr-Jul_2014_1000-words_summaries.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the base driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a base utility class that serves as a driver until convergence happens. Child classes are expected to extend it in order to provide the first guess at centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import numpy\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "from KMeansJob import KMeansJob\n",
    "\n",
    "class KMeansJobDriver:\n",
    "    \n",
    "    \"\"\"\n",
    "    Stores the given centroids to a file.\n",
    "    \"\"\"\n",
    "    def store_centroids(self, file_name, centroids):            \n",
    "        with open(file_name, 'w') as centroids_file:\n",
    "            for key, point in centroids.iteritems():\n",
    "                print >> centroids_file, key + ',' + ','.join([str(x) for x in point])\n",
    "    \n",
    "    \"\"\"\n",
    "    Iterates until the threshold for convergence has been satisfied.\n",
    "    \"\"\"\n",
    "    def run(self, runner_type, output_folder, threshold):\n",
    "\n",
    "        # Initialize the centroids.\n",
    "        \n",
    "        pre_centroids = None\n",
    "        post_centroids = self.get_initial_centroids()\n",
    "\n",
    "        # Iterate until we have converged.\n",
    "        \n",
    "        converged = False\n",
    "        iteration = 0\n",
    "        \n",
    "        time_start = time.time()\n",
    "\n",
    "        while not converged:\n",
    "            iteration += 1\n",
    "            pre_centroids = post_centroids\n",
    "\n",
    "            # Write the centroids.txt file for the next iteration.\n",
    "            \n",
    "            iteration_file_name = 'centroids_%03d.txt' % iteration\n",
    "\n",
    "            self.store_centroids(iteration_file_name, post_centroids)\n",
    "            shutil.copyfile(iteration_file_name, 'centroids.txt')\n",
    "            \n",
    "            # Run the next iteration.\n",
    "            \n",
    "            iteration_output_folder = '%s/%03d' % (output_folder, iteration)\n",
    "\n",
    "            mr_job = KMeansJob(args = [\n",
    "                '-r', runner_type,\n",
    "                '--strict-protocols',\n",
    "                '--file=centroids.txt',\n",
    "                '--output-dir=' + iteration_output_folder,\n",
    "                'topUsers_Apr-Jul_2014_1000-words.txt'\n",
    "            ])\n",
    "            \n",
    "            with mr_job.make_runner() as runner:\n",
    "                runner.run()\n",
    "                \n",
    "                post_centroids = {}\n",
    "\n",
    "                for line in runner.stream_output():\n",
    "                    key, point = mr_job.parse_output_line(line)\n",
    "                    post_centroids[key] = point\n",
    "                \n",
    "                # Account for missing centroids and leave them in\n",
    "                # the same place they were before the iteration.\n",
    "                \n",
    "                for key, point in pre_centroids.iteritems():\n",
    "                    if key not in post_centroids:\n",
    "                        post_centroids[key] = point                \n",
    "\n",
    "            self.store_centroids('centroids_next.txt', post_centroids)\n",
    "\n",
    "            # Print iteration results and check for convergence.\n",
    "\n",
    "            maximum_change = self.get_maximum_change(pre_centroids, post_centroids)\n",
    "            print 'Iteration', iteration, 'has maximum change', maximum_change\n",
    "            converged = maximum_change <= threshold\n",
    "        \n",
    "        time_end = time.time()\n",
    "        duration = time_end - time_start\n",
    "        \n",
    "        print 'Converged in', iteration, 'iteration(s), which required', duration, 'second(s)'\n",
    "            \n",
    "    \"\"\"\n",
    "    Returns the maximum change for any given coordinate position in the centroids.\n",
    "    Account for case where nothing winds up in some cluster.\n",
    "    \"\"\"\n",
    "    def get_maximum_change(self, pre_centroids, post_centroids):\n",
    "        best_point_difference = 0\n",
    "        \n",
    "        for key, pre_point in pre_centroids.iteritems():\n",
    "            post_point = post_centroids[key]\n",
    "\n",
    "            point_difference = numpy.array(pre_point) - numpy.array(post_point)\n",
    "            max_point_difference = max(numpy.abs(point_difference))\n",
    "            \n",
    "            if max_point_difference > best_point_difference:\n",
    "                best_point_difference = max_point_difference\n",
    "\n",
    "        return best_point_difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> After convergence, print out a summary of the classes present in each cluster.\n",
    "In particular, report the composition as measured by the total\n",
    "portion of each class type (0-3) contained in each cluster,\n",
    "and discuss your findings and any differences in outcomes across parts A-D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provide summary job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "from mrjob.step import MRStep\n",
    "from KMeansJob import KMeansJob\n",
    "\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "class KMeansSummaryJob(KMeansJob):\n",
    "\n",
    "    \"\"\"\n",
    "    Emit the centroid and its corresponding code.\n",
    "    \"\"\"\n",
    "    def mapper(self, _, line):\n",
    "        row = csv.reader([line]).next()\n",
    "\n",
    "        if len(row) <= 3:\n",
    "            return\n",
    "\n",
    "        # Update the centroid summary so that it remembers the code for\n",
    "        # the point that belongs to it.\n",
    "\n",
    "        code = row[1]\n",
    "\n",
    "        total = int(row[2])\n",
    "        point = [float(x) / total for x in row[3:]]\n",
    "        centroid = self.get_closest_centroid(point)\n",
    "\n",
    "        yield code, { centroid: 1 }\n",
    "\n",
    "    \"\"\"\n",
    "    Emit the total counts for each code\n",
    "    \"\"\"\n",
    "    def combiner(self, code, centroid_counts):\n",
    "        yield code, self.get_totals(centroid_counts)\n",
    "    \n",
    "    \"\"\"\n",
    "    Emit the total counts for each code\n",
    "    \"\"\"\n",
    "    def reducer(self, code, centroid_counts):\n",
    "        totals = self.get_totals(centroid_counts)\n",
    "\n",
    "        total = sum(totals.itervalues())\n",
    "        proportions = { key : float(count) / total for key, count in totals.iteritems() }\n",
    "        \n",
    "        yield code, proportions\n",
    "\n",
    "    \"\"\"\n",
    "    Compute the totals for each code as if it were a word counter.\n",
    "    \"\"\"\n",
    "    def get_totals(self, centroid_counts):\n",
    "        totals = {}\n",
    "        \n",
    "        for centroid_count in centroid_counts:\n",
    "            for centroid, count in centroid_count.iteritems():\n",
    "                if centroid in totals:\n",
    "                    totals[centroid] += count\n",
    "                else:\n",
    "                    totals[centroid] = count\n",
    "        \n",
    "        return totals\n",
    "\n",
    "    \"\"\"\n",
    "    Specify steps.\n",
    "    \"\"\"\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper_init = self.mapper_init,\n",
    "                mapper = self.mapper,\n",
    "                combiner = self.combiner,\n",
    "                reducer = self.reducer)\n",
    "        ]\n",
    "\n",
    "\n",
    "if __name__ == '__main__' and sys.argv[0].find('ipykernel') == -1:\n",
    "    KMeansSummaryJob().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provide summary pretty print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas\n",
    "\n",
    "\"\"\"\n",
    "Create a summary table from the JSON output in the noted file name.\n",
    "\"\"\"\n",
    "def get_summary_table(file_name):\n",
    "    summary_items = {}\n",
    "    \n",
    "    with open(file_name, 'r') as summary_file:\n",
    "        for line in summary_file:\n",
    "\n",
    "            split_line = line.strip().split('\\t')\n",
    "            code = split_line[0]\n",
    "            summary = json.loads(split_line[1])\n",
    "            \n",
    "            summary_items[code] = { key: '{:.2%}'.format(value) for key, value in summary.iteritems() }\n",
    "    \n",
    "    df = pandas.DataFrame.from_dict(summary_items)\n",
    "    df.sort_index(axis = 1, inplace = True)\n",
    "    df.fillna('0.00%', inplace = True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW4.5a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (A) K=4 uniform random centroid-distributions over the 1000 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import csv\n",
    "import random\n",
    "import sys\n",
    "\n",
    "from KMeansJobDriver import KMeansJobDriver\n",
    "\n",
    "class driver_45a(KMeansJobDriver):\n",
    "\n",
    "    \"\"\"\n",
    "    Choose 4 random indices and iterate over the CSV until we find the users\n",
    "    at the specified indices.\n",
    "    \"\"\"\n",
    "    def get_initial_centroids(self):\n",
    "\n",
    "        row_indices = random.sample(xrange(1, 1001), 4)\n",
    "\n",
    "        centroids = {}        \n",
    "        row_number = 0\n",
    "        \n",
    "        with open('topUsers_Apr-Jul_2014_1000-words.txt', 'r') as user_file:\n",
    "            for row in csv.reader(user_file):\n",
    "                row_number += 1\n",
    "                \n",
    "                if row_number not in row_indices:\n",
    "                    continue\n",
    "\n",
    "                user_id = row[0]\n",
    "                total = int(row[2])\n",
    "                point = [float(x) / total for x in row[3:]]\n",
    "\n",
    "                centroids[user_id] = point\n",
    "        \n",
    "        return centroids\n",
    "\n",
    "if __name__ == '__main__' and sys.argv[0].find('ipykernel') == -1:\n",
    "    driver_45a().run(sys.argv[1], sys.argv[2], 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /tmp/mrjob_45a_output\n",
      "Iteration 1 has maximum change 0.0775211232155\n",
      "Iteration 2 has maximum change 0.030128726541\n",
      "Iteration 3 has maximum change 0.00762067793484\n",
      "Iteration 4 has maximum change 0\n",
      "Converged in 4 iteration(s), which required 187.624292135 second(s)\n"
     ]
    }
   ],
   "source": [
    "!rm -f centroids*.txt\n",
    "\n",
    "!hdfs dfs -rm -r -f -skipTrash $hdfs_base_folder/mrjob_45a_output > /dev/null\n",
    "!python driver_45a.py hadoop $hdfs_base_folder/mrjob_45a_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!rm -rf mrjob_45a_summary\n",
    "\n",
    "!python KMeansSummaryJob.py -r local \\\n",
    "    --strict-protocols \\\n",
    "    --file=centroids.txt \\\n",
    "    --output-dir=mrjob_45a_summary \\\n",
    "    --no-output \\\n",
    "    topUsers_Apr-Jul_2014_1000-words.txt \\\n",
    "    > /dev/null 2>&1\n",
    "\n",
    "!cat mrjob_45a_summary/* > mrjob_45a_summary.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>\"0\"</th>\n",
       "      <th>\"1\"</th>\n",
       "      <th>\"2\"</th>\n",
       "      <th>\"3\"</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>196249823</th>\n",
       "      <td>0.13%</td>\n",
       "      <td>87.91%</td>\n",
       "      <td>66.67%</td>\n",
       "      <td>3.88%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2300389891</th>\n",
       "      <td>0.27%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>57.28%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42911831</th>\n",
       "      <td>0.00%</td>\n",
       "      <td>8.79%</td>\n",
       "      <td>7.41%</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453748967</th>\n",
       "      <td>99.60%</td>\n",
       "      <td>3.30%</td>\n",
       "      <td>25.93%</td>\n",
       "      <td>38.83%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               \"0\"     \"1\"     \"2\"     \"3\"\n",
       "196249823    0.13%  87.91%  66.67%   3.88%\n",
       "2300389891   0.27%   0.00%   0.00%  57.28%\n",
       "42911831     0.00%   8.79%   7.41%   0.00%\n",
       "453748967   99.60%   3.30%  25.93%  38.83%"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_summary_table('mrjob_45a_summary.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW4.5b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (B) K=2 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import csv\n",
    "import numpy\n",
    "import sys\n",
    "\n",
    "from KMeansJobDriver import KMeansJobDriver\n",
    "\n",
    "class driver_45b(KMeansJobDriver):\n",
    "\n",
    "    \"\"\"\n",
    "    Load the class distributions from topUsers_Apr-Jul_2014_1000-words_summaries.txt\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        distribution_file_name = 'topUsers_Apr-Jul_2014_1000-words_summaries.txt'\n",
    "\n",
    "        with open(distribution_file_name, 'r') as distribution_file:\n",
    "            reader = csv.reader(distribution_file)\n",
    "            header_row = reader.next()\n",
    "\n",
    "            # Load the row for the total population\n",
    "            \n",
    "            total_row = reader.next()\n",
    "\n",
    "            total = int(total_row[2])\n",
    "            point = [float(x) / total for x in total_row[3:]]\n",
    "            \n",
    "            self.centroid = point\n",
    "\n",
    "    \"\"\"\n",
    "    Choose 2 centroids.\n",
    "    \"\"\"\n",
    "    def get_initial_centroids(self):\n",
    "        return { str(i) : self.get_perturbed_centroid() for i in range(0, 2) }\n",
    "        \n",
    "    \"\"\"\n",
    "    Choose a new centroid by adding a random perturbation to the all-class centroid.\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_perturbed_centroid(self):\n",
    "\n",
    "        # Choose random numbers between 0 and 0.01\n",
    "\n",
    "        perturbation = numpy.random.random(len(self.centroid)) / 100\n",
    "\n",
    "        # Create the perturbed point and re-normalize so that the\n",
    "        # values still sum to 1.\n",
    "\n",
    "        perturbed_point = numpy.array(self.centroid) + perturbation\n",
    "        perturbed_point /= numpy.sum(perturbed_point)\n",
    "        \n",
    "        return perturbed_point        \n",
    "        \n",
    "if __name__ == '__main__' and sys.argv[0].find('ipykernel') == -1:\n",
    "    driver_45b().run(sys.argv[1], sys.argv[2], 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 has maximum change 0.0443779813147\n",
      "Iteration 2 has maximum change 0.048027636263\n",
      "Iteration 3 has maximum change 0.0282316079689\n",
      "Iteration 4 has maximum change 0.000689009986548\n",
      "Converged in 4 iteration(s), which required 186.39784193 second(s)\n"
     ]
    }
   ],
   "source": [
    "!rm -f centroids*.txt\n",
    "\n",
    "!hdfs dfs -rm -r -f -skipTrash $hdfs_base_folder/mrjob_45b_output > /dev/null\n",
    "!python driver_45b.py hadoop $hdfs_base_folder/mrjob_45b_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm -rf mrjob_45b_summary\n",
    "\n",
    "!python KMeansSummaryJob.py -r local \\\n",
    "    --strict-protocols \\\n",
    "    --file=centroids.txt \\\n",
    "    --output-dir=mrjob_45b_summary \\\n",
    "    --no-output \\\n",
    "    topUsers_Apr-Jul_2014_1000-words.txt \\\n",
    "    > /dev/null 2>&1\n",
    "\n",
    "!cat mrjob_45b_summary/* > mrjob_45b_summary.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>\"0\"</th>\n",
       "      <th>\"1\"</th>\n",
       "      <th>\"2\"</th>\n",
       "      <th>\"3\"</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.13%</td>\n",
       "      <td>96.70%</td>\n",
       "      <td>74.07%</td>\n",
       "      <td>3.88%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>99.87%</td>\n",
       "      <td>3.30%</td>\n",
       "      <td>25.93%</td>\n",
       "      <td>96.12%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      \"0\"     \"1\"     \"2\"     \"3\"\n",
       "0   0.13%  96.70%  74.07%   3.88%\n",
       "1  99.87%   3.30%  25.93%  96.12%"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_summary_table('mrjob_45b_summary.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW4.5c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (C) K=4 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import csv\n",
    "import numpy\n",
    "import sys\n",
    "\n",
    "from driver_45b import driver_45b\n",
    "\n",
    "class driver_45c(driver_45b):\n",
    "\n",
    "    \"\"\"\n",
    "    Choose 4 centroids.\n",
    "    \"\"\"\n",
    "    def get_initial_centroids(self):\n",
    "        return { str(i) : self.get_perturbed_centroid() for i in range(0, 4) }\n",
    "\n",
    "if __name__ == '__main__' and sys.argv[0].find('ipykernel') == -1:\n",
    "    driver_45c().run(sys.argv[1], sys.argv[2], 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 has maximum change 0.105617448782\n",
      "Iteration 2 has maximum change 0.0588165443685\n",
      "Iteration 3 has maximum change 0.0121952665909\n",
      "Iteration 4 has maximum change 0\n",
      "Converged in 4 iteration(s), which required 188.59821105 second(s)\n"
     ]
    }
   ],
   "source": [
    "!rm -f centroids*.txt\n",
    "\n",
    "!hdfs dfs -rm -r -f -skipTrash $hdfs_base_folder/mrjob_45c_output > /dev/null\n",
    "!python driver_45c.py hadoop $hdfs_base_folder/mrjob_45c_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm -rf mrjob_45c_summary\n",
    "\n",
    "!python KMeansSummaryJob.py -r local \\\n",
    "    --strict-protocols \\\n",
    "    --file=centroids.txt \\\n",
    "    --output-dir=mrjob_45c_summary \\\n",
    "    --no-output \\\n",
    "    topUsers_Apr-Jul_2014_1000-words.txt \\\n",
    "    > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>\"0\"</th>\n",
       "      <th>\"1\"</th>\n",
       "      <th>\"2\"</th>\n",
       "      <th>\"3\"</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.13%</td>\n",
       "      <td>40.66%</td>\n",
       "      <td>70.37%</td>\n",
       "      <td>3.88%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.27%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>5.56%</td>\n",
       "      <td>56.31%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00%</td>\n",
       "      <td>56.04%</td>\n",
       "      <td>3.70%</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>99.60%</td>\n",
       "      <td>3.30%</td>\n",
       "      <td>20.37%</td>\n",
       "      <td>39.81%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      \"0\"     \"1\"     \"2\"     \"3\"\n",
       "0   0.13%  40.66%  70.37%   3.88%\n",
       "1   0.27%   0.00%   5.56%  56.31%\n",
       "2   0.00%  56.04%   3.70%   0.00%\n",
       "3  99.60%   3.30%  20.37%  39.81%"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!cat mrjob_45c_summary/* > mrjob_45c_summary.txt\n",
    "\n",
    "get_summary_table('mrjob_45c_summary.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW4.5d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (D) K=4 \"trained\" centroids, determined by the sums across the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import csv\n",
    "import numpy\n",
    "import sys\n",
    "\n",
    "from KMeansJobDriver import KMeansJobDriver\n",
    "\n",
    "class driver_45d(KMeansJobDriver):\n",
    "\n",
    "    \"\"\"\n",
    "    Load the class distributions from topUsers_Apr-Jul_2014_1000-words_summaries.txt\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.centroids = {}\n",
    "        distribution_file_name = 'topUsers_Apr-Jul_2014_1000-words_summaries.txt'\n",
    "\n",
    "        with open(distribution_file_name, 'r') as distribution_file:\n",
    "            reader = csv.reader(distribution_file)\n",
    "            header_row = reader.next()\n",
    "\n",
    "            # Load the row for the total population\n",
    "            \n",
    "            total_row = reader.next()\n",
    "            \n",
    "            for row in reader:\n",
    "                code = row[1]\n",
    "                total = int(total_row[2])\n",
    "                point = [float(x) / total for x in total_row[3:]]\n",
    "                \n",
    "                self.centroids[code] = point\n",
    "\n",
    "    \"\"\"\n",
    "    Return the 'trained' centroids\n",
    "    \"\"\"\n",
    "    def get_initial_centroids(self):\n",
    "        return self.centroids\n",
    "\n",
    "if __name__ == '__main__' and sys.argv[0].find('ipykernel') == -1:\n",
    "    driver_45d().run(sys.argv[1], sys.argv[2], 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 has maximum change 0.0104099262664\n",
      "Iteration 2 has maximum change 0.0741558818057\n",
      "Iteration 3 has maximum change 0.0165985372109\n",
      "Iteration 4 has maximum change 0.0134138361426\n",
      "Iteration 5 has maximum change 0.00672879986323\n",
      "Iteration 6 has maximum change 0.0101538237874\n",
      "Iteration 7 has maximum change 0.0203770724963\n",
      "Iteration 8 has maximum change 0.0241936258467\n",
      "Iteration 9 has maximum change 0.0186661807583\n",
      "Iteration 10 has maximum change 0.0215314932318\n",
      "Iteration 11 has maximum change 0.00123803087056\n",
      "Iteration 12 has maximum change 0\n",
      "Converged in 12 iteration(s), which required 560.313692808 second(s)\n"
     ]
    }
   ],
   "source": [
    "!rm -f centroids*.txt\n",
    "\n",
    "!hdfs dfs -rm -r -f -skipTrash $hdfs_base_folder/mrjob_45d_output > /dev/null\n",
    "!python driver_45d.py hadoop $hdfs_base_folder/mrjob_45d_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm -rf mrjob_45d_summary\n",
    "\n",
    "!python KMeansSummaryJob.py -r local \\\n",
    "    --strict-protocols \\\n",
    "    --file=centroids.txt \\\n",
    "    --output-dir=mrjob_45d_summary \\\n",
    "    --no-output \\\n",
    "    topUsers_Apr-Jul_2014_1000-words.txt \\\n",
    "    > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>\"0\"</th>\n",
       "      <th>\"1\"</th>\n",
       "      <th>\"2\"</th>\n",
       "      <th>\"3\"</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.13%</td>\n",
       "      <td>40.66%</td>\n",
       "      <td>70.37%</td>\n",
       "      <td>3.88%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.27%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>5.56%</td>\n",
       "      <td>56.31%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00%</td>\n",
       "      <td>56.04%</td>\n",
       "      <td>3.70%</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>99.60%</td>\n",
       "      <td>3.30%</td>\n",
       "      <td>20.37%</td>\n",
       "      <td>39.81%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      \"0\"     \"1\"     \"2\"     \"3\"\n",
       "0   0.13%  40.66%  70.37%   3.88%\n",
       "1   0.27%   0.00%   5.56%  56.31%\n",
       "2   0.00%  56.04%   3.70%   0.00%\n",
       "3  99.60%   3.30%  20.37%  39.81%"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!cat mrjob_45c_summary/* > mrjob_45d_summary.txt\n",
    "\n",
    "get_summary_table('mrjob_45d_summary.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW4.5 Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Report the composition as measured by the total\n",
    "portion of each class type (0-3) contained in each cluster,\n",
    "and discuss your findings and any differences in outcomes across parts A-D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For all initializations, class type 0 was predominantly in only one cluster.\n",
    "* For random initialization and 2 perturbed centroids, class type 1 was predominantly in only one cluster. For 4 perturbed centroids and trained centroids, class type 1 had a 3/2 split between two clusters.\n",
    "* For all initializations, class type 3 had pproximately 70% of all of its elements in one cluster.\n",
    "* For 2 perturbed centroids, class type 4 was predominantly in only one cluster. For all other cases, class type 3 had a 3/2 split between clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": false,
   "toc_threshold": 4,
   "toc_window_display": true
  },
  "toc_position": {
   "left": "1551.31px",
   "right": "20px",
   "top": "148px",
   "width": "296px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
