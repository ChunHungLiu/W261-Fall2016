{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASCI W261: Machine Learning at Scale\n",
    "## Assignment Week 2\n",
    "Miki Seltzer (miki.seltzer@berkeley.edu)<br>\n",
    "W261-2, Spring 2016<br>\n",
    "Submission: 25-Jan-2016 9pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### HW2.0:\n",
    "\n",
    "#### What is a race condition in the context of parallel computation? Give an example.\n",
    "A race condition is when a section of code is executed by multiple processes, and the order in which the processes execute will impact the final result. In the below example, if two threads read data concurrently, then the data only gets incremented once. However, if the two threads read data sequentially, the data gets incremented by 2. \n",
    "\n",
    "![Race condition example](race_conditions.png)\n",
    "Source: https://en.wikipedia.org/wiki/Race_condition#Example\n",
    "\n",
    "#### What is MapReduce?\n",
    "MapReduce can refer to multiple concepts:\n",
    "- **Programming model:** Processes are split into a \"mapping\" phase, and a \"reducing\" phase. In the map phase, a certain function is mapped on to each value in a data set, and then in the reduce phase, the result of the map phase is aggregated. \n",
    "- **Execution framework:** This framework coordinates running processes written with the above model in mind.\n",
    "- **Software implementation:** MapReduce is the name of Google's proprietary implementation of this programming model, while Apache Hadoop is the open-source implementation.\n",
    "\n",
    "#### How does it differ from Hadoop?\n",
    "Hadoop is the open-source implementation of Google's MapReduce. Hadoop consists of two parts: distributed storage of data, and distributed processing of data. HDFS is the storage part, and MapReduce is the processing part.\n",
    "\n",
    "#### Which programming paradigm is Hadoop based on? Explain and give a simple example in code and show the code running.\n",
    "Hadoop is based on the MapReduce paradigm. The classic example of the MapReduce programming paradigm is word count. In the map phase of word count, each word in a document is assigned a count of 1. In the reduce phase, the counts for each unique word are summed to yield the final count of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP PHASE\n",
      "['hello', 1]\n",
      "['this', 1]\n",
      "['is', 1]\n",
      "['a', 1]\n",
      "['test', 1]\n",
      "['to', 1]\n",
      "['test', 1]\n",
      "['word', 1]\n",
      "['count', 1]\n",
      "['test', 1]\n",
      "['should', 1]\n",
      "['have', 1]\n",
      "['a', 1]\n",
      "['count', 1]\n",
      "['of', 1]\n",
      "['three', 1]\n",
      "\n",
      "REDUCE PHASE\n",
      "['a', 1] (intermediate step)\n",
      "['a', 2] FINAL SUM\n",
      "['count', 1] (intermediate step)\n",
      "['count', 2] FINAL SUM\n",
      "['have', 1] FINAL SUM\n",
      "['hello', 1] FINAL SUM\n",
      "['is', 1] FINAL SUM\n",
      "['of', 1] FINAL SUM\n",
      "['should', 1] FINAL SUM\n",
      "['test', 1] (intermediate step)\n",
      "['test', 2] (intermediate step)\n",
      "['test', 3] FINAL SUM\n",
      "['this', 1] FINAL SUM\n",
      "['three', 1] FINAL SUM\n",
      "['to', 1] FINAL SUM\n",
      "['word', 1] FINAL SUM\n"
     ]
    }
   ],
   "source": [
    "def hw2_0():\n",
    "    doc = \"Hello this is a test to test word count test should have a count of three\".lower()\n",
    "    key_vals = []\n",
    "    \n",
    "    print \"MAP PHASE\"\n",
    "    for word in doc.split():\n",
    "        print [word, 1]\n",
    "        key_vals.append([word, 1])\n",
    "    \n",
    "    print \"\\nREDUCE PHASE\"\n",
    "    key_vals = sorted(key_vals)\n",
    "    \n",
    "    current_word = None\n",
    "    current_count = 0\n",
    "    \n",
    "    for pair in key_vals:\n",
    "        if current_word == pair[0]:\n",
    "            print [current_word, current_count], \"(intermediate step)\"\n",
    "            current_count += pair[1]\n",
    "        else:\n",
    "            if current_word:\n",
    "                print [current_word, current_count], \"FINAL SUM\"\n",
    "            current_word = pair[0]\n",
    "            current_count = pair[1]\n",
    "\n",
    "    print [current_word, current_count], \"FINAL SUM\"\n",
    "    \n",
    "hw2_0()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.1: Sort in Hadoop MapReduce\n",
    "**Given as input: Records of the form `<integer, “NA”>`, where integer is any integer, and “NA” is just the empty string.**<br>\n",
    "**Output: Sorted key value pairs of the form `<integer, “NA”>`; what happens if you have multiple reducers? Do you need additional steps? Explain.**\n",
    "\n",
    "If there are multiple reducers, then a straightforward MapReduce process will yield outputs that are sorted within each reducer, but not sorted across all reducers. The easy, not-scalable solution is to force the job to have only one reducer.\n",
    "\n",
    "In order to output a sort across more than one reducer, an extra step needs to be implemented that will intelligently send keys to reducers so that the result from all reducers will yield a complete sort. For example, let's say our keys ranged from 0-300. If we had 3 reducers, we could send all keys in the range [0,100) to reducer 1, [100, 200) to reducer 2, and [200, 300] to reducer 3. Thus, the output of each reducer will yield documents that are completely sorted. We would need to balance the keys sent to each reducer to ensure that the load is still balanced between all reducers, which will require some calculations.\n",
    "\n",
    "In this homework, the data set is relatively small, so we can get away with specifying one reducer.\n",
    "\n",
    "Another strategy is to do a merge sort on the results of the reducer output, since each output will be sorted within itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write code to generate N  random records of the form `<integer, “NA”>`. Let N = 10,000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to need the Hadoop Streaming jar file, so let's download it here so that we know which one to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-01-21 18:37:45--  http://central.maven.org/maven2/org/apache/hadoop/hadoop-streaming/2.7.1/hadoop-streaming-2.7.1.jar\n",
      "Resolving central.maven.org... 23.235.47.209\n",
      "Connecting to central.maven.org|23.235.47.209|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 105736 (103K) [application/java-archive]\n",
      "Saving to: “hadoop-streaming-2.7.1.jar”\n",
      "\n",
      "100%[======================================>] 105,736     --.-K/s   in 0.1s    \n",
      "\n",
      "2016-01-21 18:38:00 (1.06 MB/s) - “hadoop-streaming-2.7.1.jar” saved [105736/105736]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://central.maven.org/maven2/org/apache/hadoop/hadoop-streaming/2.7.1/hadoop-streaming-2.7.1.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Generate lots of random numbers and write them to a file\n",
    "with open(\"random.txt\", \"w\") as myfile:\n",
    "    for i in range(10000):\n",
    "        myfile.write(\"{:d},{:s}\\n\".format(random.randint(0, 100000), \"NA\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write the Python Hadoop streaming map-reduce job to perform this sort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: mapper code for HW2.1\n",
    "\n",
    "import sys\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # In this case, we do not need to map the input to anything\n",
    "    key, value = line.strip().split(',')\n",
    "    print '%s\\t%s' % (int(key), value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: reducer code for HW2.1\n",
    "\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # In this case, we do not need to reduce anything\n",
    "    key, value = line.strip().split('\\t')\n",
    "    print \"%d\\t%s\" % (int(key), value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make HDFS directory and put random.txt there\n",
    "!hdfs dfs -mkdir /user/miki/week02\n",
    "!hdfs dfs -put random.txt /user/miki/week02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/miki/week02/hw2_1_output': No such file or directory\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.5.0.jar] /tmp/streamjob2545098228394721531.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "# Change permissions on mapper and reducer\n",
    "!chmod +x mapper.py\n",
    "!chmod +x reducer.py\n",
    "\n",
    "# If output folder already exists, delete it\n",
    "!hdfs dfs -rm -r /user/miki/week02/hw2_1_output\n",
    "\n",
    "# Run job\n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.reduce.tasks=2 \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.text.key.comparator.options=-n \\\n",
    "-mapper /home/cloudera/Documents/W261-Fall2016/Week02/mapper.py \\\n",
    "-reducer /home/cloudera/Documents/W261-Fall2016/Week02/reducer.py \\\n",
    "-input /user/miki/week02/random.txt \\\n",
    "-output /user/miki/week02/hw2_1_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -copyToLocal /user/miki/week02/hw2_1_output/part-*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def merge_sort(file1, file2, index_to_sort):    \n",
    "    list1 = []\n",
    "    list2 = []\n",
    "    sorted_list = []\n",
    "\n",
    "    with open(file1, 'r') as myfile:\n",
    "        for line in myfile:\n",
    "            fields = line.replace('\\n', '').split('\\t')\n",
    "            list1.append(fields)\n",
    "\n",
    "    with open(file2, 'r') as myfile:\n",
    "        for line in myfile:\n",
    "            fields = line.replace('\\n', '').split('\\t')\n",
    "            list2.append(fields)\n",
    "\n",
    "    while len(list1) > 0 and len(list2) > 0:\n",
    "        if list1[0][index_to_sort] <= list2[0][index_to_sort]:\n",
    "            sorted_list.append(list1.pop(0))\n",
    "        else:\n",
    "            sorted_list.append(list2.pop(0))\n",
    "\n",
    "    return sorted_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the 10 largest and 10 smallest numers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Largest 10 numbers:\n",
      "[['99866', 'NA'], ['99886', 'NA'], ['99914', 'NA'], ['99920', 'NA'], ['99924', 'NA'], ['99948', 'NA'], ['99972', 'NA'], ['99975', 'NA'], ['99983', 'NA'], ['99983', 'NA']]\n",
      "\n",
      "Smallest 10 numbers:\n",
      "[['19', 'NA'], ['19', 'NA'], ['24', 'NA'], ['28', 'NA'], ['28', 'NA'], ['40', 'NA'], ['41', 'NA'], ['50', 'NA'], ['63', 'NA'], ['70', 'NA']]\n"
     ]
    }
   ],
   "source": [
    "sorted_list = merge_sort('part-00000', 'part-00001', 0)\n",
    "print \"Largest 10 numbers:\"\n",
    "print sorted_list[len(sorted_list)-10:len(sorted_list)]\n",
    "print \"\\nSmallest 10 numbers:\"\n",
    "print sorted_list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.2: Using the Enron data from HW1 and Hadoop MapReduce streaming, write mapper/reducer pair that  will determine the number of occurrences of a single, user-specified word. \n",
    "\n",
    "Examine the word “assistance” and report your results. To do so, make sure that mapper.py counts all occurrences of a single word, and reducer.py collates the counts of the single word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load enronemail_1h.txt into HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -put enronemail_1h.txt /user/miki/week02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: mapper code for HW2.2\n",
    "\n",
    "import sys\n",
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    \n",
    "    # Strip white space from line, then split into fields\n",
    "    # Replace commas with spaces (we are using commas as a delimiter as well)\n",
    "    # Remove remaining punctuation from subject and body\n",
    "    # Concatenate, then split subject and body by spaces\n",
    "    # Some records are malformed -- if there is a 4th field, use it\n",
    "    fields = line.strip().split('\\t')\n",
    "    subj = fields[2].replace(',', ' ')\n",
    "    subj = subj.translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "    if len(fields) == 4:\n",
    "        body = fields[3].replace(',', ' ')\n",
    "        body = body.translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "    else:\n",
    "        body = \"\"\n",
    "    words = subj + \" \" + body\n",
    "    words = words.split()\n",
    "    \n",
    "    # Loop through words and print\n",
    "    # key = word\n",
    "    # value = 1\n",
    "    for word in words:\n",
    "        if repr(word)[1] != '\\\\':\n",
    "            print \"%s\\t%s\" % (word, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: reducer code for HW2.2\n",
    "\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "# Initialize some variables\n",
    "# We know that the words will be sorted\n",
    "# We need to keep track of state\n",
    "prev_word = None\n",
    "prev_count = 0\n",
    "word = None\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # Strip and split line from mapper\n",
    "    word, count = line.strip().split('\\t', 1)\n",
    "    \n",
    "    # If possible, turn count into an int (it's read as a string)\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # We couldn't make count into an int, so move on\n",
    "        continue\n",
    "        \n",
    "    # Since the words will be sorted, all counts for a word will be grouped\n",
    "    if prev_word == word:\n",
    "        # If prev_word is word, then we haven't changed words\n",
    "        # Just update prev_count\n",
    "        prev_count += count\n",
    "    else:\n",
    "        # We've encountered a new word!\n",
    "        # This might be the first word, though\n",
    "        if prev_word:\n",
    "            # We need to print the last word we were on\n",
    "            print \"%s\\t%s\" % (prev_word, prev_count)\n",
    "        \n",
    "        # Now we need to initialize our variables for the new word and count\n",
    "        prev_word = word\n",
    "        prev_count = count\n",
    "\n",
    "# We have reached the end of the file, so print the last word and count\n",
    "if prev_word == word:\n",
    "    print \"%s\\t%s\" % (prev_word, prev_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Hadoop streaming command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/miki/week02/hw2_2_output\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.5.0.jar] /tmp/streamjob9066873689980855879.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "# Change permissions on mapper and reducer\n",
    "!chmod +x mapper.py\n",
    "!chmod +x reducer.py\n",
    "\n",
    "# Delete output folder if it exists\n",
    "!hdfs dfs -rm -r /user/miki/week02/hw2_2_output\n",
    "\n",
    "# Run job\n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-mapper /home/cloudera/Documents/W261-Fall2016/Week02/mapper.py \\\n",
    "-reducer /home/cloudera/Documents/W261-Fall2016/Week02/reducer.py \\\n",
    "-input /user/miki/week02/enronemail_1h.txt \\\n",
    "-output /user/miki/week02/hw2_2_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine the word 'assistance'\n",
    "\n",
    "Because the next part requires that we have counted all words, this map reduce job counts all words. We need to loop through all lines to find the term 'assistance.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -copyToLocal /user/miki/week02/hw2_2_output/part-00000 hw2_2_output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance\t10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('hw2_2_output.txt', 'r') as myfile:\n",
    "    for line in myfile:\n",
    "        if line.split('\\t')[0] == \"assistance\":\n",
    "            print line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.2.1: Using Hadoop MapReduce and your wordcount job (from HW2.2) determine the top-10 occurring tokens (most frequent tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: mapper code for HW2.2.1\n",
    "\n",
    "import sys\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    fields = line.replace('\\n', '').split('\\t')\n",
    "    print '%s\\t%s' % (fields[1], fields[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: reducer code for HW2.2.1\n",
    "\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    print line.replace('\\n', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Hadoop streaming command\n",
    "\n",
    "We need to specify in this case, that we are only using one reducer, and that we want to sort the keys (mapper switches the key and value, so the key is word counts) based numerical value, in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/miki/week02/hw2_2_1_output\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.5.0.jar] /tmp/streamjob3864465894171665498.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "# Change permissions on mapper and reducer\n",
    "!chmod +x mapper.py\n",
    "!chmod +x reducer.py\n",
    "\n",
    "# Delete output folder if it exists\n",
    "!hdfs dfs -rm -r /user/miki/week02/hw2_2_1_output\n",
    "\n",
    "# Run job\n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.reduce.tasks=2 \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.text.key.comparator.options=-nr \\\n",
    "-mapper /home/cloudera/Documents/W261-Fall2016/Week02/mapper.py \\\n",
    "-reducer /home/cloudera/Documents/W261-Fall2016/Week02/reducer.py \\\n",
    "-input /user/miki/week02/hw2_2_output \\\n",
    "-output /user/miki/week02/hw2_2_1_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!rm part-*\n",
    "!hdfs dfs -copyToLocal /user/miki/week02/hw2_2_1_output/part-*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the 10 words with the highest counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Largest 10 numbers:\n",
      "['1246', 'the']\n",
      "['560', 'of']\n",
      "['427', 'you']\n",
      "['391', 'your']\n",
      "['373', 'for']\n",
      "['258', 'on']\n",
      "['241', 'i']\n",
      "['234', 'will']\n",
      "['201', 'com']\n",
      "['164', 'we']\n"
     ]
    }
   ],
   "source": [
    "sorted_list = merge_sort('part-00000', 'part-00001', 0)\n",
    "print \"Largest 10 numbers:\"\n",
    "for i in range(10):\n",
    "    print sorted_list[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.3: Multinomial NAIVE BAYES with NO Smoothing\n",
    "Using the Enron data from HW1 and Hadoop MapReduce, write  a mapper/reducer job(s) that will both learn Naive Bayes classifier and classify the Enron email messages using the learnt Naive Bayes classifier. Use all white-space delimited tokens as independent input variables (assume spaces, fullstops, commas as delimiters). \n",
    "\n",
    "Note: for multinomial Naive Bayes, the Pr(X=“assistance”|Y=SPAM) is calculated as follows:\n",
    "\n",
    "$$\n",
    "\\frac{\\text{number of times \"assistance\" occurs in SPAM labeled documents}}{\\text{the number of words in documents labeled SPAM}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapper/Reducer for fitting NB\n",
    "\n",
    "Mapper:\n",
    "- Input: training documents\n",
    "- Output: (word, 1, 0) if word was in spam, otherwise (word, 0, 1)\n",
    "  - Special words: \\*alldocs, \\*docs and \\*words\n",
    "  \n",
    "Reducer:\n",
    "- Input: (word, 1, 0) or (word, 0, 1)\n",
    "- Output: (word, spam count, ham count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: mapper code for HW2.3\n",
    "\n",
    "import sys\n",
    "import string\n",
    "import re\n",
    "\n",
    "counts = {\n",
    "    '*words':{\n",
    "        '1':0,\n",
    "        '0':0\n",
    "    },\n",
    "    '*docs':{\n",
    "        '1':0,\n",
    "        '0':0\n",
    "    }\n",
    "}\n",
    "total_docs = 0\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    \n",
    "    # Strip white space from line, then split into fields\n",
    "    # Replace commas with spaces (we are using commas as a delimiter as well)\n",
    "    # Remove remaining punctuation from subject and body\n",
    "    # Concatenate, then split subject and body by spaces\n",
    "    # Some records are malformed -- if there is a 4th field, use it\n",
    "    fields = line.strip().split('\\t')\n",
    "    \n",
    "    # Keep track of document counts\n",
    "    spam = fields[1]\n",
    "    counts['*docs'][spam] += 1\n",
    "    total_docs += 1\n",
    "    \n",
    "    subj = fields[2].replace(',', ' ')\n",
    "    subj = subj.translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "    if len(fields) == 4:\n",
    "        body = fields[3].replace(',', ' ')\n",
    "        body = body.translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "    else:\n",
    "        body = \"\"\n",
    "    words = subj + \" \" + body\n",
    "    words = words.split()\n",
    "    \n",
    "    # Loop through words\n",
    "    # If word is not trivial, write to file\n",
    "    # key = word\n",
    "    # value = 1\n",
    "    for word in words:\n",
    "        if len(word) > 0 and repr(word)[1] != '\\\\':\n",
    "            if spam == '1':\n",
    "                print \"%s\\t%s\\t%s\" % (word, 1, 0)\n",
    "            elif spam == '0':\n",
    "                print \"%s\\t%s\\t%s\" % (word, 0, 1)\n",
    "            counts['*words'][spam] += 1\n",
    "\n",
    "# At the end, output document and word counts\n",
    "for item in counts:\n",
    "    print \"%s\\t%s\\t%s\" % (item, counts[item]['1'], counts[item]['0'])\n",
    "print \"%s\\t%s\\t%s\" % ('*alldocs', total_docs, total_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: reducer code for HW2.3\n",
    "\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "import math\n",
    "\n",
    "# Initialize some variables\n",
    "# We know that the words will be sorted\n",
    "# We need to keep track of state\n",
    "prev_word = None\n",
    "prev_spam_count = 0\n",
    "prev_ham_count = 0\n",
    "word = None\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # Split line into fields\n",
    "    fields = line.strip().split('\\t')\n",
    "    word = fields[0]\n",
    "    spam_count = fields[1]\n",
    "    ham_count = fields[2]\n",
    "    \n",
    "    # If possible, turn count into an int (it's read as a string)\n",
    "    try:\n",
    "        spam_count = int(spam_count)\n",
    "        ham_count = int(ham_count)\n",
    "    except ValueError:\n",
    "        # We couldn't make count into an int, so move on\n",
    "        continue\n",
    "        \n",
    "    if prev_word == word:\n",
    "        # We have not moved to a new word\n",
    "        # Just update the count of this word\n",
    "        prev_spam_count += spam_count\n",
    "        prev_ham_count += ham_count\n",
    "        \n",
    "    else:\n",
    "        # We have encountered a new word!\n",
    "        # If this is the first word, we don't need to print anything\n",
    "        if prev_word: \n",
    "            # Write the previous word to file\n",
    "            print '%s\\t%s\\t%s' % (prev_word, prev_spam_count, prev_ham_count)\n",
    "            \n",
    "        # Now we need to initialize our variables\n",
    "        prev_word = word\n",
    "        prev_spam_count = spam_count\n",
    "        prev_ham_count = ham_count\n",
    "\n",
    "# We've reached the end of the file\n",
    "# Print the last word and counts\n",
    "print '%s\\t%s\\t%s' % (prev_word, prev_spam_count, prev_ham_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/miki/week02/hw2_3_output_fit\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.5.0.jar] /tmp/streamjob7354054932591241196.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "# Change permissions on mapper and reducer\n",
    "!chmod +x mapper.py\n",
    "!chmod +x reducer.py\n",
    "\n",
    "# Delete output folder if it exists\n",
    "!hdfs dfs -rm -r /user/miki/week02/hw2_3_output_fit\n",
    "\n",
    "# Run job\n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-input /user/miki/week02/enronemail_1h.txt \\\n",
    "-output /user/miki/week02/hw2_3_output_fit \\\n",
    "-mapper /home/cloudera/Documents/W261-Fall2016/Week02/mapper.py \\\n",
    "-reducer /home/cloudera/Documents/W261-Fall2016/Week02/reducer.py \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!hdfs dfs -cat /user/miki/week02/hw2_3_output_fit/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapper/Reducer #1 for predicting NB\n",
    "\n",
    "Mapper:\n",
    "- Input from fit: (word, spam count, ham count)\n",
    "- Output: (word, spam count, ham count)\n",
    "- Input testing document: (document ID, cat, subj, body)\n",
    "- Output: (word, cat, document ID)\n",
    "\n",
    "Reducer:\n",
    "- Input: (word, spam count, ham count)\n",
    "- Input: (word, cat, document ID)\n",
    "- Output: (document ID, cat, word, spam count, ham count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: mapper code for HW2.3\n",
    "\n",
    "import sys\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    \n",
    "    # Strip white space from line, then split into fields    \n",
    "    fields = line.strip().split('\\t')\n",
    "    \n",
    "    # If first field matches pattern of document ID, tokenize words\n",
    "    pattern = re.compile('^\\d{4}.\\d{4}-\\d{2}-\\d{2}\\.\\w+')\n",
    "    if pattern.match(fields[0]):\n",
    "        # Keep track of true document class\n",
    "        doc_id = fields[0]\n",
    "        spam = fields[1]\n",
    "\n",
    "        # We are always going to need the doc/word counts for each document\n",
    "        print '%s^%s^%s' % ('*alldocs', spam, doc_id)\n",
    "        print '%s^%s^%s' % ('*docs', spam, doc_id)\n",
    "        print '%s^%s^%s' % ('*words', spam, doc_id)\n",
    "        \n",
    "        # Replace commas with spaces (we are using commas as a delimiter as well)\n",
    "        # Remove remaining punctuation from subject and body\n",
    "        # Concatenate, then split subject and body by spaces\n",
    "        # Some records are malformed -- if there is a 4th field, use it\n",
    "        subj = fields[2].replace(',', ' ')\n",
    "        subj = subj.translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "        if len(fields) == 4:\n",
    "            body = fields[3].replace(',', ' ')\n",
    "            body = body.translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "        else:\n",
    "            body = \"\"\n",
    "        words = subj + \" \" + body\n",
    "        words = words.split()\n",
    "\n",
    "        # Loop through words\n",
    "        # If word is not trivial, write to file\n",
    "        # key = word\n",
    "        # value = doc_id\n",
    "        for word in words:\n",
    "            if len(word) > 0 and repr(word)[1] != '\\\\':\n",
    "                print '%s^%s^%s' % (word, spam, doc_id)\n",
    "    else:\n",
    "        # Now we know that the record is the\n",
    "        # output of the previous MapReduce job\n",
    "        # We just need to output this back out for use in the reducer\n",
    "        word = fields[0]\n",
    "        spam_count = fields[1]\n",
    "        ham_count = fields[2]\n",
    "        print '%s^%s^%s' % (word, '*' + spam_count, ham_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: reducer code for HW2.3\n",
    "\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "import math\n",
    "import re\n",
    "\n",
    "# Initialize some variables\n",
    "spam_count = 0\n",
    "ham_count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # Strip and split line\n",
    "    key, value = line.strip().split('\\t')\n",
    "    word, field1 = key.split('^')\n",
    "\n",
    "    # If field1 starts with a *, we know that it is the spam and ham counts\n",
    "    if field1[0] == '*':\n",
    "        # This record will be of the form (word field1=*spam_count value=ham_count)\n",
    "        # This record should always come first, so we can append the\n",
    "        # counts to each following word in the doc\n",
    "        try:\n",
    "            spam_count = int(field1.replace('*',''))\n",
    "            ham_count = int(value)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    \n",
    "    else:\n",
    "        # Now we know that it is a doc_id record\n",
    "        # This record will be of the form (word field1=cat value=doc_id)\n",
    "        # We have saved spam_count and ham_count so we can now 'join' them\n",
    "        pattern = re.compile('^\\d{4}.\\d{4}-\\d{2}-\\d{2}\\.\\w+')\n",
    "        if pattern.match(value):\n",
    "            doc_id = value\n",
    "        print '%s\\t%s\\t%s\\t%s\\t%s' % (doc_id, field1, word, spam_count, ham_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/miki/week02/hw2_3_output_predict1\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.5.0.jar] /tmp/streamjob6214512506397240495.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "# Change permissions on mapper and reducer\n",
    "!chmod +x mapper.py\n",
    "!chmod +x reducer.py\n",
    "\n",
    "# Delete output folder if it exists\n",
    "!hdfs dfs -rm -r /user/miki/week02/hw2_3_output_predict1\n",
    "\n",
    "# Run job\n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-D stream.map.output.field.separator=^ \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D mapreduce.map.output.key.field.separator=^ \\\n",
    "-D mapreduce.partition.keypartitioner.options=-k1,1 \\\n",
    "-input /user/miki/week02/enronemail_1h.txt \\\n",
    "-input /user/miki/week02/hw2_3_output_fit \\\n",
    "-output /user/miki/week02/hw2_3_output_predict1 \\\n",
    "-mapper /home/cloudera/Documents/W261-Fall2016/Week02/mapper.py \\\n",
    "-reducer /home/cloudera/Documents/W261-Fall2016/Week02/reducer.py \n",
    "\n",
    "# This job makes use of secondary sorting so that all words show up together\n",
    "# However, in the reducer we emit using the doc_id as primary key\n",
    "# so we can classify in the next job when things are sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!hdfs dfs -cat /user/miki/week02/hw2_3_output_predict1/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapper/Reducer #2 for predicting NB\n",
    "\n",
    "Mapper:\n",
    "- Input from predict1: (doc_id, cat, word, spam count, ham count)\n",
    "- Output: identity\n",
    "\n",
    "Reducer:\n",
    "- Input: (doc_id, cat, word, spam count, ham count)\n",
    "- Output: (doc_id, cat, prediction, spam log prob, ham log prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: mapper code for HW2.3\n",
    "\n",
    "import sys\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    \n",
    "    # Replace delimiter\n",
    "    line = line.replace('\\n', '')\n",
    "    fields = line.split('\\t')\n",
    "    print '%s^%s^%s^%s^%s' % (fields[0], fields[1], fields[2], fields[3], fields[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: reducer code for HW2.3\n",
    "\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "import math\n",
    "import re\n",
    "import decimal\n",
    "\n",
    "# Initialize some variables\n",
    "doc = None\n",
    "spam = None\n",
    "count = 1\n",
    "class_count = {'1':0, '0':0}\n",
    "word = None\n",
    "\n",
    "prev_doc = None\n",
    "prev_spam = None\n",
    "prev_count = 1\n",
    "prev_class_count = {'1':0, '0':0}\n",
    "prev_word = None\n",
    "\n",
    "docs_total = 0\n",
    "docs = {'1':0, '0':0}\n",
    "words_total = 0\n",
    "words = {'1':0, '0':0}\n",
    "log_prior = {'1':0, '0':0}\n",
    "log_posterior = {'1':0, '0':0}\n",
    "log_likelihood = {'1':0, '0':0}\n",
    "\n",
    "classes = {'1':'spam', '0':'ham'}\n",
    "num_errors = {'1':0, '0':0}\n",
    "num_total = 0.0\n",
    "num_correct = 0.0\n",
    "\n",
    "print_debug = False\n",
    "\n",
    "# Create a function to update the posterior\n",
    "# since we need to do it in multiple locations.\n",
    "# We don't want to duplicate code\n",
    "def update_posterior():\n",
    "    # Calculate evidence\n",
    "    if print_debug:\n",
    "        print \"times word occured:\", prev_count\n",
    "\n",
    "    for item in classes:\n",
    "        if prev_class_count[item] > 0 and log_likelihood[item] != float('-inf'):\n",
    "            log_likelihood[item] = math.log(prev_class_count[item] / words[item])\n",
    "            log_posterior[item] += prev_count * log_likelihood[item]\n",
    "            if print_debug:\n",
    "                print \"updated log posterior:\", log_posterior[item]\n",
    "\n",
    "        else:\n",
    "            if print_debug:\n",
    "                print \"zero probability, set log_post to -inf for\", classes[item]\n",
    "                print '\\n'\n",
    "            log_posterior[item] = float('-inf')\n",
    "            num_errors[item] += 1\n",
    "        \n",
    "\n",
    "# Create a function to avoid code duplication\n",
    "def make_prediction(): \n",
    "    global num_total, num_correct\n",
    "    \n",
    "    # We can compare non-normalized posterior probabilities\n",
    "    num_total += 1\n",
    "    if log_posterior['1'] > log_posterior['0']: prediction = '1'\n",
    "    else: prediction = '0'\n",
    "\n",
    "    # Count correct guesses\n",
    "    if prev_spam == prediction:\n",
    "        num_correct += 1\n",
    "        \n",
    "    # Output the log posteriors. We can normalize later.\n",
    "    print '%s\\t%s\\t%s\\t%s\\t%s\\n' % (prev_doc, prev_spam, prediction, \n",
    "                                    log_posterior['1'],\n",
    "                                    log_posterior['0'])\n",
    "    \n",
    "for line in sys.stdin:\n",
    "    # Remove end of line\n",
    "    line = line.replace('\\n', '')\n",
    "    \n",
    "    # Split words when running locally\n",
    "    #doc, spam, word, class_count['1'], class_count['0'] = line.split('\\t')\n",
    "    \n",
    "    # Split words when running in Hadoop\n",
    "    key, value = line.strip().split('\\t')\n",
    "    doc, spam, word = key.split('^')\n",
    "    class_count['1'], class_count['0'] = value.split('^')    \n",
    "\n",
    "    # Keep this in a try/except statement so we don't fail\n",
    "    try:\n",
    "        for item in classes:\n",
    "            class_count[item] = float(class_count[item])\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "    # Let's calculate some probabilities\n",
    "    if prev_doc == doc:\n",
    "        # We haven't changed documents\n",
    "        if prev_word == word:\n",
    "            # We haven't changed words, so just increment\n",
    "            prev_count += 1\n",
    "\n",
    "        else:\n",
    "            # We are at a new word\n",
    "            # We need to check if we are at a keyword\n",
    "            if print_debug: print '\\n', prev_word, '\\n'\n",
    "            if prev_word == '*alldocs': \n",
    "                # We are at a record where we need to output total docs\n",
    "                docs_total = prev_class_count['1']\n",
    "                if print_debug: print \"total docs:\", docs_total\n",
    "\n",
    "            elif prev_word == '*docs': \n",
    "                # We are at a record where we need to output unique docs per class\n",
    "                for item in classes:\n",
    "                    docs[item] = prev_class_count[item]\n",
    "                    if print_debug: print \"prior\", item, docs[item], '/', docs_total\n",
    "                    log_prior[item] = math.log(docs[item] / docs_total)\n",
    "\n",
    "                    # We will update the posterior after each word\n",
    "                    # Initialize it to the prior\n",
    "                    log_posterior[item] = log_prior[item]\n",
    "                if print_debug: \n",
    "                    print \"log prior:\", log_prior\n",
    "                    print 'log posterior initial', log_posterior\n",
    "\n",
    "            elif prev_word == '*words':\n",
    "                # We are at a record where we need to output words per class\n",
    "                for item in classes:\n",
    "                    words[item] = prev_class_count[item]\n",
    "                words_total = sum(prev_class_count.values())\n",
    "                if print_debug: print \"word class_count:\", words\n",
    "\n",
    "            elif prev_word:\n",
    "                # We are at a new normal word, and need to calculate stuff\n",
    "                update_posterior()\n",
    "\n",
    "            prev_word = word\n",
    "            prev_count = 1\n",
    "            for item in classes:\n",
    "                prev_class_count[item] = class_count[item]\n",
    "\n",
    "    else:\n",
    "        # We are done with one document. We need to: \n",
    "        # - process the last word\n",
    "        # - output our predictions\n",
    "        if prev_doc:\n",
    "            if print_debug: print '\\n', prev_word, '\\n'\n",
    "            # We are at a new normal word, and need to calculate stuff\n",
    "            update_posterior()\n",
    "\n",
    "            # Now we can calculate the prediction\n",
    "            make_prediction()\n",
    "            if print_debug: print num_correct, \"out of\", num_total\n",
    "\n",
    "        prev_doc = doc\n",
    "        prev_spam = spam\n",
    "        prev_word = word\n",
    "        for item in classes:\n",
    "            prev_class_count[item] = class_count[item]\n",
    "        log_likelihood = {'1':0, '0':0}\n",
    "        if print_debug: print \"reset log likelihood\"\n",
    "\n",
    "# Output our final prediction\n",
    "if print_debug: print '\\n', prev_word, '\\n'\n",
    "update_posterior()\n",
    "make_prediction()\n",
    "\n",
    "print \"Number of documents\\t%d\" % (num_total)\n",
    "print \"Number correct predictions\\t%d\" % (num_correct)\n",
    "print \"Error rate\\t%s\" % (100 - 100 * num_correct / num_total) + \"%\"\n",
    "print \"Number of zero probability spam\\t%d\" % (num_errors['1'])\n",
    "print \"Number of zero probability ham\\t%d\" % (num_errors['0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/miki/week02/hw2_3_output_predict2\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.5.0.jar] /tmp/streamjob7956579118514301940.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "# Change permissions on mapper and reducer\n",
    "!chmod +x mapper.py\n",
    "!chmod +x reducer.py\n",
    "\n",
    "# Delete output folder if it exists\n",
    "!hdfs dfs -rm -r /user/miki/week02/hw2_3_output_predict2\n",
    "\n",
    "# Run job\n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-D stream.map.output.field.separator=^ \\\n",
    "-D stream.num.map.output.key.fields=3 \\\n",
    "-D mapreduce.map.output.key.field.separator=^ \\\n",
    "-D mapreduce.partition.keypartitioner.options=-k1,2 \\\n",
    "-input /user/miki/week02/hw2_3_output_predict1/part* \\\n",
    "-output /user/miki/week02/hw2_3_output_predict2 \\\n",
    "-mapper /home/cloudera/Documents/W261-Fall2016/Week02/mapper.py \\\n",
    "-reducer /home/cloudera/Documents/W261-Fall2016/Week02/reducer.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!hdfs dfs -cat /user/miki/week02/hw2_3_output_predict2/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Copy files from HDFS\n",
    "!rm hw2_3_output_predict2.txt\n",
    "!hdfs dfs -copyToLocal /user/miki/week02/hw2_3_output_predict2/part-00000 \\\n",
    "hw2_3_output_predict2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pretty print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def print_results(thefile):\n",
    "    # Read in data from text file\n",
    "    with open(thefile, 'r') as myfile:\n",
    "        # Initialize\n",
    "        total = 0.0\n",
    "        incorrect = 0.0\n",
    "        \n",
    "        summary_lines = []\n",
    "        \n",
    "        # Print header\n",
    "        table = '{:28s}{:^12s}{:^12s}{:15f}{:15f}'.format\n",
    "        print '{:28s}{:^12s}{:^12s}{:>15s}{:>15s}'.format('Document', \n",
    "                                                      'Truth', \n",
    "                                                      'Predicted', \n",
    "                                                      'Log Prob Spam', \n",
    "                                                      'Log Prob Ham')\n",
    "        print '----------------------------------------------------------------------------------'\n",
    "        for line in myfile:\n",
    "            fields = line.split()\n",
    "\n",
    "            # Only look at records that have a doc_id as the first field\n",
    "            pattern = re.compile('^\\d{4}.\\d{4}-\\d{2}-\\d{2}\\.\\w+')\n",
    "            try:\n",
    "                key = fields[0]\n",
    "            except:\n",
    "                continue\n",
    "            if pattern.match(fields[0]):\n",
    "                print table(fields[0], fields[1], fields[2], float(fields[3]), float(fields[4]))\n",
    "                total += 1\n",
    "                if fields[1] != fields[2]: incorrect += 1\n",
    "            else:\n",
    "                summary_lines.append(line)\n",
    "            \n",
    "        print '----------------------------------------------------------------------------------'\n",
    "\n",
    "        for item in summary_lines:\n",
    "            fields = item.replace('\\n', '').split('\\t')\n",
    "            print '{:32s}{:>12s}'.format(fields[0], fields[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document                       Truth     Predicted    Log Prob Spam   Log Prob Ham\n",
      "----------------------------------------------------------------------------------\n",
      "0001.1999-12-10.farmer           0           0                 -inf     -45.201972\n",
      "0001.1999-12-10.kaminski         0           0                 -inf     -29.284981\n",
      "0001.2000-01-17.beck             0           0                 -inf   -3341.077344\n",
      "0001.2000-06-06.lokay            0           0                 -inf   -3526.273656\n",
      "0001.2001-02-07.kitchen          0           0                 -inf    -296.797358\n",
      "0001.2001-04-02.williams         0           0                 -inf   -1334.948366\n",
      "0002.1999-12-13.farmer           0           0                 -inf   -2684.579651\n",
      "0002.2001-02-07.kitchen          0           0                 -inf    -441.700059\n",
      "0002.2001-05-25.SA_and_HP        1           1          -580.752342           -inf\n",
      "0002.2003-12-18.GP               1           1         -1270.960024           -inf\n",
      "0002.2004-08-01.BG               1           1          -842.706686           -inf\n",
      "0003.1999-12-10.kaminski         0           0                 -inf    -386.040537\n",
      "0003.1999-12-14.farmer           0           0                 -inf     -78.152547\n",
      "0003.2000-01-17.beck             0           0                 -inf   -1207.578678\n",
      "0003.2001-02-08.kitchen          0           0                 -inf   -1206.780507\n",
      "0003.2003-12-18.GP               1           1          -850.351820           -inf\n",
      "0003.2004-08-01.BG               1           1          -777.647834           -inf\n",
      "0004.1999-12-10.kaminski         0           0                 -inf   -1090.317286\n",
      "0004.1999-12-14.farmer           0           0                 -inf    -937.767809\n",
      "0004.2001-04-02.williams         0           0                 -inf    -638.140105\n",
      "0004.2001-06-12.SA_and_HP        1           1          -877.744435           -inf\n",
      "0004.2004-08-01.BG               1           1          -763.165365           -inf\n",
      "0005.1999-12-12.kaminski         0           0                 -inf    -756.536217\n",
      "0005.1999-12-14.farmer           0           0                 -inf    -961.752679\n",
      "0005.2000-06-06.lokay            0           0                 -inf    -413.005686\n",
      "0005.2001-02-08.kitchen          0           0                 -inf    -809.806584\n",
      "0005.2001-06-23.SA_and_HP        1           1          -189.504203           -inf\n",
      "0005.2003-12-18.GP               1           1         -7502.210444           -inf\n",
      "0006.1999-12-13.kaminski         0           0                 -inf    -489.864863\n",
      "0006.2001-02-08.kitchen          0           0                 -inf   -9478.893225\n",
      "0006.2001-04-03.williams         0           0                 -inf    -299.745379\n",
      "0006.2001-06-25.SA_and_HP        1           1          -378.538145           -inf\n",
      "0006.2003-12-18.GP               1           1         -1121.775578           -inf\n",
      "0006.2004-08-01.BG               1           1         -1057.013400           -inf\n",
      "0007.1999-12-13.kaminski         0           0                 -inf   -1435.592153\n",
      "0007.1999-12-14.farmer           0           0                 -inf    -673.184638\n",
      "0007.2000-01-17.beck             0           0                 -inf   -2673.868345\n",
      "0007.2001-02-09.kitchen          0           0                 -inf   -1641.988845\n",
      "0007.2003-12-18.GP               1           1         -1235.461261           -inf\n",
      "0007.2004-08-01.BG               1           1         -1476.865340           -inf\n",
      "0008.2001-02-09.kitchen          0           0                 -inf   -3926.085101\n",
      "0008.2001-06-12.SA_and_HP        1           1          -877.744435           -inf\n",
      "0008.2001-06-25.SA_and_HP        1           1         -4209.757676           -inf\n",
      "0008.2003-12-18.GP               1           1         -1009.088985           -inf\n",
      "0008.2004-08-01.BG               1           1         -5865.653555           -inf\n",
      "0009.1999-12-13.kaminski         0           0                 -inf   -5293.259576\n",
      "0009.1999-12-14.farmer           0           0                 -inf    -485.699382\n",
      "0009.2000-06-07.lokay            0           0                 -inf   -2656.290633\n",
      "0009.2001-02-09.kitchen          0           0                 -inf   -5457.875614\n",
      "0009.2001-06-26.SA_and_HP        1           1         -1267.354338           -inf\n",
      "0009.2003-12-18.GP               1           1          -763.745092           -inf\n",
      "0010.1999-12-14.farmer           0           0                 -inf   -1198.577025\n",
      "0010.1999-12-14.kaminski         0           0                 -inf    -200.984122\n",
      "0010.2001-02-09.kitchen          0           0                 -inf   -2892.573319\n",
      "0010.2001-06-28.SA_and_HP        1           1         -3348.753849           -inf\n",
      "0010.2003-12-18.GP               1           1           -55.844271           -inf\n",
      "0010.2004-08-01.BG               1           1         -2303.713694           -inf\n",
      "0011.1999-12-14.farmer           0           0                 -inf   -1875.449588\n",
      "0011.2001-06-28.SA_and_HP        1           1         -3341.017541           -inf\n",
      "0011.2001-06-29.SA_and_HP        1           1        -15600.612139           -inf\n",
      "0011.2003-12-18.GP               1           1          -517.764478           -inf\n",
      "0011.2004-08-01.BG               1           1          -676.375628           -inf\n",
      "0012.1999-12-14.farmer           0           0                 -inf   -3038.816953\n",
      "0012.1999-12-14.kaminski         0           0                 -inf    -845.566647\n",
      "0012.2000-01-17.beck             0           0                 -inf   -2662.786350\n",
      "0012.2000-06-08.lokay            0           0                 -inf    -902.722519\n",
      "0012.2001-02-09.kitchen          0           0                 -inf    -476.959635\n",
      "0012.2003-12-19.GP               1           1          -152.639505           -inf\n",
      "0013.1999-12-14.farmer           0           0                 -inf   -1758.007755\n",
      "0013.1999-12-14.kaminski         0           0                 -inf   -1110.137081\n",
      "0013.2001-04-03.williams         0           0                 -inf    -652.272744\n",
      "0013.2001-06-30.SA_and_HP        1           1        -28156.916028           -inf\n",
      "0013.2004-08-01.BG               1           1         -1516.322536           -inf\n",
      "0014.1999-12-14.kaminski         0           0                 -inf   -1607.441023\n",
      "0014.1999-12-15.farmer           0           0                 -inf   -1041.309841\n",
      "0014.2001-02-12.kitchen          0           0                 -inf   -1298.850507\n",
      "0014.2001-07-04.SA_and_HP        1           1         -3468.402191           -inf\n",
      "0014.2003-12-19.GP               1           1          -161.514030           -inf\n",
      "0014.2004-08-01.BG               1           1          -802.301047           -inf\n",
      "0015.1999-12-14.kaminski         0           0                 -inf    -556.269139\n",
      "0015.1999-12-15.farmer           0           0                 -inf    -692.971449\n",
      "0015.2000-06-09.lokay            0           0                 -inf    -130.104971\n",
      "0015.2001-02-12.kitchen          0           0                 -inf   -4914.445404\n",
      "0015.2001-07-05.SA_and_HP        1           1          -881.529547           -inf\n",
      "0015.2003-12-19.GP               1           1         -1279.611075           -inf\n",
      "0016.1999-12-15.farmer           0           0                 -inf    -699.497159\n",
      "0016.2001-02-12.kitchen          0           0                 -inf   -1130.276565\n",
      "0016.2001-07-05.SA_and_HP        1           1          -881.529547           -inf\n",
      "0016.2001-07-06.SA_and_HP        1           1        -16461.822596           -inf\n",
      "0016.2003-12-19.GP               1           1          -785.179398           -inf\n",
      "0016.2004-08-01.BG               1           1          -656.976627           -inf\n",
      "0017.1999-12-14.kaminski         0           0                 -inf    -377.984890\n",
      "0017.2000-01-17.beck             0           0                 -inf   -2673.886861\n",
      "0017.2001-04-03.williams         0           0                 -inf    -433.805570\n",
      "0017.2003-12-18.GP               1           1          -211.328721           -inf\n",
      "0017.2004-08-01.BG               1           1          -912.600138           -inf\n",
      "0017.2004-08-02.BG               1           1         -2525.094375           -inf\n",
      "0018.1999-12-14.kaminski         0           0                 -inf    -983.657215\n",
      "0018.2001-07-13.SA_and_HP        1           1         -3280.399856           -inf\n",
      "0018.2003-12-18.GP               1           1         -3420.208484           -inf\n",
      "----------------------------------------------------------------------------------\n",
      "Number of documents                      100\n",
      "Number correct predictions               100\n",
      "Error rate                              0.0%\n",
      "Number of zero probability spam         2927\n",
      "Number of zero probability ham          3993\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "print_results('hw2_3_output_predict2.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogram of posterior probabilities\n",
    "\n",
    "Plot a histogram of the log posterior probabilities (i.e., log(Pr(Class|Doc))) for each class over the training set. Summarize what you see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtMAAAEFCAYAAAAhe2ueAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npRAFALZyilsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1140a4cf90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# Import libraries for plotting and analysis\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from decimal import *\n",
    "\n",
    "# Initialize data structures\n",
    "doc, Y, prediction, log_prob_spam, log_prob_ham = [], [], [], [], []\n",
    "classes = [\"Spam\", \"Ham\"]\n",
    "\n",
    "# Read in data from text file\n",
    "with open('hw2_3_output_predict2.txt', 'r') as myfile:\n",
    "    for line in myfile:\n",
    "        fields = line.split()\n",
    "\n",
    "        # Only look at records that have a doc_id as the first field\n",
    "        pattern = re.compile('^\\d{4}.\\d{4}-\\d{2}-\\d{2}\\.\\w+')\n",
    "        try:\n",
    "            key = fields[0]\n",
    "        except:\n",
    "            continue\n",
    "        if pattern.match(fields[0]):\n",
    "            doc.append(fields[0])\n",
    "            Y.append(fields[1])\n",
    "            prediction.append(fields[2])\n",
    "            log_prob_spam.append(fields[3])\n",
    "            log_prob_ham.append(fields[4])\n",
    "\n",
    "# Convert to numpy arrays\n",
    "doc = np.array(doc)\n",
    "Y = np.array(Y)\n",
    "prediction = np.array(prediction)\n",
    "log_prob_spam = np.array(log_prob_spam).astype(float)\n",
    "log_prob_ham = np.array(log_prob_ham).astype(float)\n",
    "log_probs = np.column_stack((log_prob_spam, log_prob_ham))\n",
    "\n",
    "# These numbers are SO SMALL we need to work with Decimal type, then convert back to float\n",
    "norm_probs = {0:[], 1:[]}\n",
    "log_probs_graph = {0:[], 1:[]}\n",
    "\n",
    "for i in log_probs:\n",
    "    spam = Decimal(i[0]).exp() / (Decimal(i[0]).exp() + Decimal(i[1]).exp())\n",
    "    ham = Decimal(i[1]).exp() / (Decimal(i[0]).exp() + Decimal(i[1]).exp())\n",
    "    norm_probs[0].append(float(spam))\n",
    "    norm_probs[1].append(float(ham))\n",
    "    if i[0] != float('-inf'): log_probs_graph[0].append(float(spam))\n",
    "    if i[1] != float('-inf'): log_probs_graph[1].append(float(ham))\n",
    "\n",
    "num_bins = 30\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot histograms\n",
    "for i in range(2):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.hist(norm_probs[i], num_bins, facecolor='green', alpha=0.5)\n",
    "    plt.xlabel(classes[i] + ' Probability')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.title(r'Histogram of ' + classes[i] + ' Probabilities')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogram summary\n",
    "\n",
    "The histogram of normalized probabilities are either 0 or 1. Because we are not using smoothing, the non-presence of a word in a document renders that prediction's probability to be zero (thus the other class's probability is 1). There are no emails where all words in the document are present in both spam and ham. Thus, for every email, one of the classes will have a zero probability. This results in 0% error rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.4: Repeat HW2.3 with the following modification: use Laplace plus-one smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapper/Reducer #1 for fitting NB\n",
    "\n",
    "Mapper:\n",
    "- Input: training documents\n",
    "- Output: (word, 1, 0) if word was in spam, otherwise (word, 0, 1)\n",
    "  - Special words: \\*alldocs, \\*docs, \\*vocab and \\*words\n",
    "  \n",
    "Reducer:\n",
    "- Input: (word, 1, 0) or (word, 0, 1)\n",
    "- Output: (word, spam count, ham count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: mapper code for HW2.4\n",
    "\n",
    "import sys\n",
    "import string\n",
    "import re\n",
    "\n",
    "counts = {\n",
    "    '*words':{\n",
    "        '1':0,\n",
    "        '0':0\n",
    "    },\n",
    "    '*docs':{\n",
    "        '1':0,\n",
    "        '0':0\n",
    "    }\n",
    "}\n",
    "total_docs = 0\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    \n",
    "    # Strip white space from line, then split into fields\n",
    "    # Replace commas with spaces (we are using commas as a delimiter as well)\n",
    "    # Remove remaining punctuation from subject and body\n",
    "    # Concatenate, then split subject and body by spaces\n",
    "    # Some records are malformed -- if there is a 4th field, use it\n",
    "    fields = line.strip().split('\\t')\n",
    "    \n",
    "    # Keep track of document counts\n",
    "    spam = fields[1]\n",
    "    counts['*docs'][spam] += 1\n",
    "    total_docs += 1\n",
    "    \n",
    "    subj = fields[2].replace(',', ' ')\n",
    "    subj = subj.translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "    if len(fields) == 4:\n",
    "        body = fields[3].replace(',', ' ')\n",
    "        body = body.translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "    else:\n",
    "        body = \"\"\n",
    "    words = subj + \" \" + body\n",
    "    words = words.split()\n",
    "    \n",
    "    # Loop through words\n",
    "    # If word is not trivial, write to file\n",
    "    # key = word\n",
    "    # value = 1\n",
    "    for word in words:\n",
    "        if len(word) > 0 and repr(word)[1] != '\\\\':\n",
    "            if spam == '1':\n",
    "                print \"%s\\t%s\\t%s\" % (word, 1, 0)\n",
    "            elif spam == '0':\n",
    "                print \"%s\\t%s\\t%s\" % (word, 0, 1)\n",
    "            counts['*words'][spam] += 1\n",
    "\n",
    "# At the end, output document and word counts\n",
    "for item in counts:\n",
    "    print \"%s\\t%s\\t%s\" % (item, counts[item]['1'], counts[item]['0'])\n",
    "print \"%s\\t%s\\t%s\" % ('*alldocs', total_docs, total_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: reducer code for HW2.4\n",
    "\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "import math\n",
    "\n",
    "# Initialize some variables\n",
    "# We know that the words will be sorted\n",
    "# We need to keep track of state\n",
    "prev_word = None\n",
    "prev_spam_count = 0\n",
    "prev_ham_count = 0\n",
    "word = None\n",
    "vocab_size = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # Split line into fields\n",
    "    fields = line.strip().split('\\t')\n",
    "    word = fields[0]\n",
    "    spam_count = fields[1]\n",
    "    ham_count = fields[2]\n",
    "    \n",
    "    # If possible, turn count into an int (it's read as a string)\n",
    "    try:\n",
    "        spam_count = int(spam_count)\n",
    "        ham_count = int(ham_count)\n",
    "    except ValueError:\n",
    "        # We couldn't make count into an int, so move on\n",
    "        continue\n",
    "        \n",
    "    if prev_word == word:\n",
    "        # We have not moved to a new word\n",
    "        # Just update the count of this word\n",
    "        prev_spam_count += spam_count\n",
    "        prev_ham_count += ham_count\n",
    "        \n",
    "    else:\n",
    "        # We have encountered a new word!\n",
    "        # If this is the first word, we don't need to print anything\n",
    "        if prev_word: \n",
    "            # Write the previous word to file\n",
    "            print '%s\\t%s\\t%s' % (prev_word, prev_spam_count, prev_ham_count)\n",
    "            if prev_word[0] != '*': vocab_size += 1\n",
    "            \n",
    "        # Now we need to initialize our variables\n",
    "        prev_word = word\n",
    "        prev_spam_count = spam_count\n",
    "        prev_ham_count = ham_count\n",
    "\n",
    "# We've reached the end of the file\n",
    "# Print the last word and counts\n",
    "print '%s\\t%s\\t%s' % (prev_word, prev_spam_count, prev_ham_count)\n",
    "if prev_word[0] != '*': vocab_size += 1\n",
    "    \n",
    "# Print the vocab size for this set of words\n",
    "print '%s\\t%s\\t%s' % ('*vocab', vocab_size, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/miki/week02/hw2_4_output_fit1\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.5.0.jar] /tmp/streamjob5743295591766583947.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "# Change permissions on mapper and reducer\n",
    "!chmod +x mapper.py\n",
    "!chmod +x reducer.py\n",
    "\n",
    "# Delete output folder if it exists\n",
    "!hdfs dfs -rm -r /user/miki/week02/hw2_4_output_fit1\n",
    "\n",
    "# Run job\n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-input /user/miki/week02/enronemail_1h.txt \\\n",
    "-output /user/miki/week02/hw2_4_output_fit1 \\\n",
    "-mapper /home/cloudera/Documents/W261-Fall2016/Week02/mapper.py \\\n",
    "-reducer /home/cloudera/Documents/W261-Fall2016/Week02/reducer.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "\n",
    "We are working with a file that is small in size, and so there is only one reducer involved. If there were multiple reducers involved, we would need to run another reducer to sum the values of \\*vocab, since we need unique counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!hdfs dfs -cat /user/miki/week02/hw2_4_output_fit1/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapper/Reducer #1 for predicting NB\n",
    "\n",
    "Mapper:\n",
    "- Input from fit: (word, spam count, ham count)\n",
    "- Output: (word, spam count, ham count)\n",
    "- Input testing document: (document ID, cat, subj, body)\n",
    "- Output: (word, cat, document ID)\n",
    "\n",
    "Reducer:\n",
    "- Input: (word, spam count, ham count)\n",
    "- Input: (word, cat, document ID)\n",
    "- Output: (document ID, cat, word, spam count, ham count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: mapper code for HW2.4\n",
    "\n",
    "import sys\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    \n",
    "    # Strip white space from line, then split into fields    \n",
    "    fields = line.strip().split('\\t')\n",
    "    \n",
    "    # If first field matches pattern of document ID, tokenize words\n",
    "    pattern = re.compile('^\\d{4}.\\d{4}-\\d{2}-\\d{2}\\.\\w+')\n",
    "    if pattern.match(fields[0]):\n",
    "        # Keep track of document counts\n",
    "        doc_id = fields[0]\n",
    "        spam = fields[1]\n",
    "\n",
    "        # We are always going to need the doc/word counts for each document\n",
    "        print '%s^%s^%s' % ('*alldocs', spam, doc_id)\n",
    "        print '%s^%s^%s' % ('*docs', spam, doc_id)\n",
    "        print '%s^%s^%s' % ('*words', spam, doc_id)\n",
    "        print '%s^%s^%s' % ('*vocab', spam, doc_id)\n",
    "        \n",
    "        # Replace commas with spaces (we are using commas as a delimiter as well)\n",
    "        # Remove remaining punctuation from subject and body\n",
    "        # Concatenate, then split subject and body by spaces\n",
    "        # Some records are malformed -- if there is a 4th field, use it\n",
    "        subj = fields[2].replace(',', ' ')\n",
    "        subj = subj.translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "        if len(fields) == 4:\n",
    "            body = fields[3].replace(',', ' ')\n",
    "            body = body.translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "        else:\n",
    "            body = \"\"\n",
    "        words = subj + \" \" + body\n",
    "        words = words.split()\n",
    "\n",
    "        # Loop through words\n",
    "        # If word is not trivial, write to file\n",
    "        # key = word\n",
    "        # value = doc_id\n",
    "        for word in words:\n",
    "            if len(word) > 0 and repr(word)[1] != '\\\\':\n",
    "                print '%s^%s^%s' % (word, spam, doc_id)\n",
    "    else:\n",
    "        # Now we know that the record is the\n",
    "        #   output of the previous MapReduce job\n",
    "        #   not the doc we are classifying\n",
    "        # We append the spam_count with a * so that it comes first\n",
    "        # We need the overall spam/ham word counts with each word in the doc\n",
    "        word = fields[0]\n",
    "        spam_count = fields[1]\n",
    "        ham_count = fields[2]\n",
    "        print '%s^%s^%s' % (word, '*' + spam_count, ham_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: reducer code for HW2.4\n",
    "\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "import math\n",
    "import re\n",
    "\n",
    "# Initialize some variables\n",
    "spam_count = 0\n",
    "ham_count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # Strip and split line\n",
    "    key, value = line.strip().split('\\t')\n",
    "    word, field1 = key.split('^')\n",
    "\n",
    "    # If field1 starts with a *, we know that it is the spam and ham counts\n",
    "    if field1[0] == '*':\n",
    "        # This record will be of the form (word field1=*spam_count value=ham_count)\n",
    "        try:\n",
    "            spam_count = int(field1.replace('*',''))\n",
    "            ham_count = int(value)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    \n",
    "    else:\n",
    "        # Now we know that it is a doc_id record\n",
    "        # This record will be of the form (word field1=cat value=doc_id)\n",
    "        pattern = re.compile('^\\d{4}.\\d{4}-\\d{2}-\\d{2}\\.\\w+')\n",
    "        if pattern.match(value):\n",
    "            doc_id = value\n",
    "        print '%s\\t%s\\t%s\\t%s\\t%s' % (doc_id, field1, word, spam_count, ham_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/miki/week02/hw2_4_output_predict1\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.5.0.jar] /tmp/streamjob4524769482412452532.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "# Change permissions on mapper and reducer\n",
    "!chmod +x mapper.py\n",
    "!chmod +x reducer.py\n",
    "\n",
    "# Delete output folder if it exists\n",
    "!hdfs dfs -rm -r /user/miki/week02/hw2_4_output_predict1\n",
    "\n",
    "# Run job\n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-D stream.map.output.field.separator=^ \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D mapreduce.map.output.key.field.separator=^ \\\n",
    "-D mapreduce.partition.keypartitioner.options=-k1,1 \\\n",
    "-input /user/miki/week02/enronemail_1h.txt \\\n",
    "-input /user/miki/week02/hw2_4_output_fit1 \\\n",
    "-output /user/miki/week02/hw2_4_output_predict1 \\\n",
    "-mapper /home/cloudera/Documents/W261-Fall2016/Week02/mapper.py \\\n",
    "-reducer /home/cloudera/Documents/W261-Fall2016/Week02/reducer.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!hdfs dfs -cat /user/miki/week02/hw2_4_output_predict1/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapper/Reducer #2 for predicting NB\n",
    "\n",
    "Mapper:\n",
    "- Input from predict1: (doc_id, cat, word, spam count, ham count)\n",
    "- Output: identity\n",
    "\n",
    "Reducer:\n",
    "- Input: (doc_id, cat, word, spam count, ham count)\n",
    "- Output: (doc_id, cat, prediction, spam log prob, ham log prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: mapper code for HW2.4\n",
    "\n",
    "import sys\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    \n",
    "    # Replace delimiter\n",
    "    line = line.replace('\\n', '')\n",
    "    fields = line.split('\\t')\n",
    "    print '%s^%s^%s^%s^%s' % (fields[0], fields[1], fields[2], fields[3], fields[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: reducer code for HW2.4\n",
    "\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "import math\n",
    "import re\n",
    "\n",
    "# Initialize some variables\n",
    "doc = None\n",
    "spam = None\n",
    "count = 1\n",
    "class_count = {'1':0, '0':0}\n",
    "word = None\n",
    "\n",
    "prev_doc = None\n",
    "prev_spam = None\n",
    "prev_count = 1\n",
    "prev_class_count = {'1':0, '0':0}\n",
    "prev_word = None\n",
    "\n",
    "vocab = 0\n",
    "docs_total = 0\n",
    "docs = {'1':0, '0':0}\n",
    "words_total = 0\n",
    "words = {'1':0, '0':0}\n",
    "log_prior = {'1':0, '0':0}\n",
    "log_posterior = {'1':0, '0':0}\n",
    "log_likelihood = {'1':0, '0':0}\n",
    "\n",
    "classes = {'1':'spam', '0':'ham'}\n",
    "num_errors = {'1':0, '0':0}\n",
    "num_total = 0.0\n",
    "num_correct = 0.0\n",
    "\n",
    "print_debug = False\n",
    "\n",
    "# Create a function to update the posterior\n",
    "# since we need to do it in multiple locations.\n",
    "# We don't want to duplicate code\n",
    "def update_posterior():\n",
    "    for item in classes:\n",
    "        # This is where we incorporate smoothing\n",
    "        log_likelihood[item] = math.log((1 + prev_class_count[item]) / (vocab + words[item]))\n",
    "        log_posterior[item] += prev_count * log_likelihood[item]\n",
    "        if print_debug:\n",
    "            print \"updated log posterior:\", log_posterior[item]\n",
    "            print '\\n'\n",
    "\n",
    "def make_prediction(): \n",
    "    global num_total, num_correct\n",
    "    \n",
    "    # We can compare non-normalized posterior probabilities\n",
    "    num_total += 1\n",
    "    if log_posterior['1'] > log_posterior['0']: prediction = '1'\n",
    "    else: prediction = '0'\n",
    "\n",
    "    # Count correct guesses\n",
    "    if prev_spam == prediction:\n",
    "        num_correct += 1\n",
    "        \n",
    "    # Output the log posteriors. We can normalize later.\n",
    "    print '%s\\t%s\\t%s\\t%s\\t%s\\n' % (prev_doc, prev_spam, prediction, \n",
    "                                    log_posterior['1'],\n",
    "                                    log_posterior['0'])\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # Strip and split line\n",
    "    # Assign variables\n",
    "    line = line.replace('\\n', '')\n",
    "    \n",
    "    # Split when testing locally\n",
    "    #doc, spam, word, count['1'], count['0'] = line.split('\\t')\n",
    "    \n",
    "    # Split when using Hadoop\n",
    "    key, value = line.strip().split('\\t')\n",
    "    doc, spam, word = key.split('^')\n",
    "    class_count['1'], class_count['0'] = value.split('^')    \n",
    "\n",
    "    # Keep this in a try/except statement so we don't fail\n",
    "    try:\n",
    "        for item in classes:\n",
    "            class_count[item] = float(class_count[item])\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "    # Let's calculate some probabilities\n",
    "    if prev_doc == doc:\n",
    "        # We haven't changed documents\n",
    "        if prev_word == word:\n",
    "            # We haven't changed words, so just increment\n",
    "            prev_count += 1\n",
    "\n",
    "        else:\n",
    "            # We are at a new word\n",
    "            # We need to check if we are at a keyword\n",
    "            if print_debug: print '\\n', prev_word, '\\n'\n",
    "            if prev_word == '*alldocs': \n",
    "                # We are at a record where we need to output total docs\n",
    "                docs_total = prev_class_count['1']\n",
    "                if print_debug: print \"total docs:\", docs_total\n",
    "\n",
    "            elif prev_word == '*docs': \n",
    "                # We are at a record where we need to output unique docs per class\n",
    "                for item in classes:\n",
    "                    docs[item] = prev_class_count[item]\n",
    "                    if print_debug: print \"prior\", item, docs[item], '/', docs_total\n",
    "                    log_prior[item] = math.log(docs[item] / docs_total)\n",
    "\n",
    "                    # We will update the posterior after each word\n",
    "                    # Initialize it to the prior\n",
    "                    log_posterior[item] = log_prior[item]\n",
    "                if print_debug: \n",
    "                    print \"log prior:\", log_prior\n",
    "                    print 'log posterior initial', log_posterior\n",
    "\n",
    "            elif prev_word == '*words':\n",
    "                # We are at a record where we need to output words per class\n",
    "                for item in classes:\n",
    "                    words[item] = prev_class_count[item]\n",
    "                words_total = sum(prev_class_count.values())\n",
    "                if print_debug: print \"word class_count:\", words\n",
    "                                \n",
    "            elif prev_word == '*vocab':\n",
    "                # We are at a record where we need to output the vocab size\n",
    "                vocab = prev_class_count['1']\n",
    "                if print_debug: print \"vocab size:\", vocab\n",
    "\n",
    "            elif prev_word:\n",
    "                # We are at a new normal word, and need to calculate stuff\n",
    "                update_posterior()\n",
    "\n",
    "            prev_word = word\n",
    "            prev_count = 1\n",
    "            for item in classes:\n",
    "                prev_class_count[item] = class_count[item]\n",
    "\n",
    "    else:\n",
    "        # We are done with one document. We need to: \n",
    "        # - process the last word\n",
    "        # - output our predictions\n",
    "        if prev_doc:\n",
    "            if print_debug: print '\\n', prev_word, '\\n'\n",
    "            # We are at a new normal word, and need to calculate stuff\n",
    "            update_posterior()\n",
    "\n",
    "            # Now we can calculate the prediction\n",
    "            make_prediction()\n",
    "            if print_debug: print num_correct, \"out of\", num_total\n",
    "\n",
    "        prev_doc = doc\n",
    "        prev_spam = spam\n",
    "        prev_word = word\n",
    "        for item in classes:\n",
    "            prev_class_count[item] = class_count[item]\n",
    "        log_likelihood = {'1':0, '0':0}\n",
    "        if print_debug: print \"reset log likelihood\"\n",
    "\n",
    "# Output our final prediction\n",
    "if print_debug: print '\\n', prev_word, '\\n'\n",
    "update_posterior()\n",
    "make_prediction()\n",
    "\n",
    "print \"Number of documents\\t%d\" % (num_total)\n",
    "print \"Number correct predictions\\t%d\" % (num_correct)\n",
    "print \"Error rate\\t%s\" % (100 - 100 * num_correct / num_total) + \"%\"\n",
    "print \"Number of zero probability spam\\t%d\" % (num_errors['1'])\n",
    "print \"Number of zero probability ham\\t%d\" % (num_errors['0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/miki/week02/hw2_4_output_predict2\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.5.0.jar] /tmp/streamjob5652509028914698936.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "# Change permissions on mapper and reducer\n",
    "!chmod +x mapper.py\n",
    "!chmod +x reducer.py\n",
    "\n",
    "# Delete output folder if it exists\n",
    "!hdfs dfs -rm -r /user/miki/week02/hw2_4_output_predict2\n",
    "\n",
    "# Run job\n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-D stream.map.output.field.separator=^ \\\n",
    "-D stream.num.map.output.key.fields=3 \\\n",
    "-D mapreduce.map.output.key.field.separator=^ \\\n",
    "-D mapreduce.partition.keypartitioner.options=-k1,2 \\\n",
    "-input /user/miki/week02/hw2_4_output_predict1/part* \\\n",
    "-output /user/miki/week02/hw2_4_output_predict2 \\\n",
    "-mapper /home/cloudera/Documents/W261-Fall2016/Week02/mapper.py \\\n",
    "-reducer /home/cloudera/Documents/W261-Fall2016/Week02/reducer.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!hdfs dfs -cat /user/miki/week02/hw2_4_output_predict2/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Copy files from HDFS\n",
    "!rm hw2_4_output_predict2.txt\n",
    "!hdfs dfs -copyToLocal /user/miki/week02/hw2_4_output_predict2/part-00000 \\\n",
    "hw2_4_output_predict2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document                       Truth     Predicted    Log Prob Spam   Log Prob Ham\n",
      "----------------------------------------------------------------------------------\n",
      "0001.1999-12-10.farmer           0           0           -47.096765     -44.164126\n",
      "0001.1999-12-10.kaminski         0           0           -32.156825     -29.786564\n",
      "0001.2000-01-17.beck             0           0         -3784.543066   -3423.100066\n",
      "0001.2000-06-06.lokay            0           0         -4047.517444   -3618.247343\n",
      "0001.2001-02-07.kitchen          0           0          -350.817691    -305.204350\n",
      "0001.2001-04-02.williams         0           0         -1412.267544   -1366.134284\n",
      "0002.1999-12-13.farmer           0           0         -3090.293822   -2744.595250\n",
      "0002.2001-02-07.kitchen          0           0          -463.594975    -449.426584\n",
      "0002.2001-05-25.SA_and_HP        1           1          -586.481375    -641.071290\n",
      "0002.2003-12-18.GP               1           1         -1275.748087   -1394.091408\n",
      "0002.2004-08-01.BG               1           1          -860.309319    -907.779546\n",
      "0003.1999-12-10.kaminski         0           0          -452.480037    -397.681776\n",
      "0003.1999-12-14.farmer           0           0           -96.936725     -79.499120\n",
      "0003.2000-01-17.beck             0           0         -1431.977521   -1252.126044\n",
      "0003.2001-02-08.kitchen          0           0         -1391.704554   -1236.584250\n",
      "0003.2003-12-18.GP               1           1          -852.797406    -902.747828\n",
      "0003.2004-08-01.BG               1           1          -775.448304    -827.257869\n",
      "0004.1999-12-10.kaminski         0           0         -1246.147942   -1111.517481\n",
      "0004.1999-12-14.farmer           0           0         -1168.620929    -963.821410\n",
      "0004.2001-04-02.williams         0           0          -691.095815    -652.266266\n",
      "0004.2001-06-12.SA_and_HP        1           1          -898.638296    -980.374079\n",
      "0004.2004-08-01.BG               1           1          -758.462183    -795.951176\n",
      "0005.1999-12-12.kaminski         0           0          -871.501155    -761.976720\n",
      "0005.1999-12-14.farmer           0           0         -1161.153698    -987.019881\n",
      "0005.2000-06-06.lokay            0           0          -438.747606    -418.195956\n",
      "0005.2001-02-08.kitchen          0           0          -902.491520    -818.309950\n",
      "0005.2001-06-23.SA_and_HP        1           1          -190.569898    -204.737506\n",
      "0005.2003-12-18.GP               1           1         -7527.282811   -8158.436311\n",
      "0006.1999-12-13.kaminski         0           0          -526.208783    -501.295549\n",
      "0006.2001-02-08.kitchen          0           0        -10503.704784   -9733.282329\n",
      "0006.2001-04-03.williams         0           0          -319.021016    -307.460195\n",
      "0006.2001-06-25.SA_and_HP        1           1          -378.248492    -400.785328\n",
      "0006.2003-12-18.GP               1           1         -1110.810083   -1171.909737\n",
      "0006.2004-08-01.BG               1           1         -1055.996294   -1120.005294\n",
      "0007.1999-12-13.kaminski         0           0         -1611.814471   -1485.356904\n",
      "0007.1999-12-14.farmer           0           0          -767.010586    -688.742967\n",
      "0007.2000-01-17.beck             0           0         -3389.086555   -2781.252237\n",
      "0007.2001-02-09.kitchen          0           0         -1844.945341   -1685.560090\n",
      "0007.2003-12-18.GP               1           1         -1230.804021   -1311.215089\n",
      "0007.2004-08-01.BG               1           1         -1485.845578   -1708.500012\n",
      "0008.2001-02-09.kitchen          0           0         -4355.581851   -3960.847446\n",
      "0008.2001-06-12.SA_and_HP        1           1          -898.638296    -980.374079\n",
      "0008.2001-06-25.SA_and_HP        1           1         -4268.105142   -4789.807228\n",
      "0008.2003-12-18.GP               1           1         -1022.156895   -1121.098367\n",
      "0008.2004-08-01.BG               1           1         -5895.448539   -6238.196192\n",
      "0009.1999-12-13.kaminski         0           0         -6707.589422   -5469.442816\n",
      "0009.1999-12-14.farmer           0           0          -556.226457    -493.593332\n",
      "0009.2000-06-07.lokay            0           0         -3047.743764   -2710.014008\n",
      "0009.2001-02-09.kitchen          0           0         -6245.594556   -5625.149880\n",
      "0009.2001-06-26.SA_and_HP        1           1         -1274.983361   -1373.878167\n",
      "0009.2003-12-18.GP               1           1          -763.278173    -827.327975\n",
      "0010.1999-12-14.farmer           0           0         -1439.705855   -1209.422935\n",
      "0010.1999-12-14.kaminski         0           0          -229.416123    -206.100618\n",
      "0010.2001-02-09.kitchen          0           0         -3385.397900   -2990.799981\n",
      "0010.2001-06-28.SA_and_HP        1           1         -3428.365713   -3787.734531\n",
      "0010.2003-12-18.GP               1           1           -54.763330     -55.892351\n",
      "0010.2004-08-01.BG               1           1         -2309.449714   -2560.674214\n",
      "0011.1999-12-14.farmer           0           0         -2108.964180   -1934.461528\n",
      "0011.2001-06-28.SA_and_HP        1           1         -3420.476004   -3777.885077\n",
      "0011.2001-06-29.SA_and_HP        1           1        -15936.841140  -17662.757436\n",
      "0011.2003-12-18.GP               1           1          -517.903783    -567.896690\n",
      "0011.2004-08-01.BG               1           1          -678.856381    -725.082640\n",
      "0012.1999-12-14.farmer           0           0         -3487.399042   -3144.038712\n",
      "0012.1999-12-14.kaminski         0           0         -1097.215774    -874.951567\n",
      "0012.2000-01-17.beck             0           0         -3376.301141   -2771.554111\n",
      "0012.2000-06-08.lokay            0           0          -945.071018    -922.095641\n",
      "0012.2001-02-09.kitchen          0           0          -523.187540    -488.020239\n",
      "0012.2003-12-19.GP               1           1          -153.515906    -167.281786\n",
      "0013.1999-12-14.farmer           0           0         -1990.463197   -1827.707479\n",
      "0013.1999-12-14.kaminski         0           0         -1417.199826   -1162.282418\n",
      "0013.2001-04-03.williams         0           0          -692.586549    -666.766301\n",
      "0013.2001-06-30.SA_and_HP        1           1        -28818.183422  -32105.395138\n",
      "0013.2004-08-01.BG               1           1         -1523.925605   -1608.254332\n",
      "0014.1999-12-14.kaminski         0           0         -1948.708306   -1674.710318\n",
      "0014.1999-12-15.farmer           0           0         -1205.033605   -1063.916039\n",
      "0014.2001-02-12.kitchen          0           0         -1459.489600   -1314.368698\n",
      "0014.2001-07-04.SA_and_HP        1           1         -3521.663602   -3825.287747\n",
      "0014.2003-12-19.GP               1           1          -161.897718    -172.075311\n",
      "0014.2004-08-01.BG               1           1          -793.790635    -832.114748\n",
      "0015.1999-12-14.kaminski         0           0          -611.024331    -562.914386\n",
      "0015.1999-12-15.farmer           0           0          -829.321220    -708.060652\n",
      "0015.2000-06-09.lokay            0           0          -139.255164    -133.122161\n",
      "0015.2001-02-12.kitchen          0           0         -5533.128403   -5066.199224\n",
      "0015.2001-07-05.SA_and_HP        1           1          -901.185494    -976.602830\n",
      "0015.2003-12-19.GP               1           1         -1291.857294   -1414.805011\n",
      "0016.1999-12-15.farmer           0           0          -773.861025    -718.206678\n",
      "0016.2001-02-12.kitchen          0           0         -1306.745075   -1149.695470\n",
      "0016.2001-07-05.SA_and_HP        1           1          -901.185494    -976.602830\n",
      "0016.2001-07-06.SA_and_HP        1           1        -16874.972196  -18993.152454\n",
      "0016.2003-12-19.GP               1           1          -783.725827    -849.343110\n",
      "0016.2004-08-01.BG               1           1          -667.666867    -732.269631\n",
      "0017.1999-12-14.kaminski         0           0          -400.837238    -387.900527\n",
      "0017.2000-01-17.beck             0           0         -3386.388074   -2781.809030\n",
      "0017.2001-04-03.williams         0           0          -459.413592    -445.240552\n",
      "0017.2003-12-18.GP               1           1          -214.070194    -227.980890\n",
      "0017.2004-08-01.BG               1           1          -879.800880    -925.945486\n",
      "0017.2004-08-02.BG               1           1         -2545.282175   -2693.269015\n",
      "0018.1999-12-14.kaminski         0           0         -1031.281775   -1015.621419\n",
      "0018.2001-07-13.SA_and_HP        1           1         -3344.611784   -3583.298677\n",
      "0018.2003-12-18.GP               1           1         -3468.281340   -3690.761405\n",
      "----------------------------------------------------------------------------------\n",
      "Number of documents                      100\n",
      "Number correct predictions               100\n",
      "Error rate                              0.0%\n",
      "Number of zero probability spam            0\n",
      "Number of zero probability ham             0\n"
     ]
    }
   ],
   "source": [
    "print_results('hw2_4_output_predict2.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare the misclassifcation error rates for 2.3 versus 2.4 and explain the differences.\n",
    "\n",
    "Both error rates for 2.3 and 2.4 are zero. In 2.3, if we run into a zero probability issue, the probability for that class's prediction will be zero, so the normalized probabilities are 0 or 1. In 2.4, we have probabilities that are between 0 and 1, not equal to 0 or 1. However, both result in a 0% error rate.\n",
    "\n",
    "Since we are training and testing on the same data set, we likely are missing any significant differences in the two models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.5: Repeat HW2.4. This time when modeling and classification ignore tokens with a frequency of less than three (3) in the training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapper/Reducer #1 for fitting NB\n",
    "\n",
    "Mapper:\n",
    "- Input: training documents\n",
    "- Output: (word, 1, 0) if word was in spam, otherwise (word, 0, 1)\n",
    "  - Special words: \\*alldocs, \\*docs, \\*vocab and \\*words\n",
    "  \n",
    "Reducer:\n",
    "- Input: (word, 1, 0) or (word, 0, 1)\n",
    "- Output: (word, spam count, ham count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: mapper code for HW2.5\n",
    "\n",
    "import sys\n",
    "import string\n",
    "import re\n",
    "\n",
    "counts = {\n",
    "    '*words':{\n",
    "        '1':0,\n",
    "        '0':0\n",
    "    },\n",
    "    '*docs':{\n",
    "        '1':0,\n",
    "        '0':0\n",
    "    }\n",
    "}\n",
    "total_docs = 0\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    \n",
    "    # Strip white space from line, then split into fields\n",
    "    # Replace commas with spaces (we are using commas as a delimiter as well)\n",
    "    # Remove remaining punctuation from subject and body\n",
    "    # Concatenate, then split subject and body by spaces\n",
    "    # Some records are malformed -- if there is a 4th field, use it\n",
    "    fields = line.strip().split('\\t')\n",
    "    \n",
    "    # Keep track of document counts\n",
    "    spam = fields[1]\n",
    "    counts['*docs'][spam] += 1\n",
    "    total_docs += 1\n",
    "    \n",
    "    subj = fields[2].replace(',', ' ')\n",
    "    subj = subj.translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "    if len(fields) == 4:\n",
    "        body = fields[3].replace(',', ' ')\n",
    "        body = body.translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "    else:\n",
    "        body = \"\"\n",
    "    words = subj + \" \" + body\n",
    "    words = words.split()\n",
    "    \n",
    "    # Loop through words\n",
    "    # If word is not trivial, write to file\n",
    "    # key = word\n",
    "    # value = 1\n",
    "    for word in words:\n",
    "        if len(word) > 0 and repr(word)[1] != '\\\\':\n",
    "            if spam == '1':\n",
    "                print \"%s\\t%s\\t%s\" % (word, 1, 0)\n",
    "            elif spam == '0':\n",
    "                print \"%s\\t%s\\t%s\" % (word, 0, 1)\n",
    "            counts['*words'][spam] += 1\n",
    "\n",
    "# At the end, output document and word counts\n",
    "for item in counts:\n",
    "    print \"%s\\t%s\\t%s\" % (item, counts[item]['1'], counts[item]['0'])\n",
    "print \"%s\\t%s\\t%s\" % ('*alldocs', total_docs, total_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: reducer code for HW2.5\n",
    "\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "import math\n",
    "\n",
    "# Initialize some variables\n",
    "# We know that the words will be sorted\n",
    "# We need to keep track of state\n",
    "prev_word = None\n",
    "prev_spam_count = 0\n",
    "prev_ham_count = 0\n",
    "word = None\n",
    "vocab_size = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # Split line into fields\n",
    "    fields = line.strip().split('\\t')\n",
    "    word = fields[0]\n",
    "    spam_count = fields[1]\n",
    "    ham_count = fields[2]\n",
    "    \n",
    "    # If possible, turn count into an int (it's read as a string)\n",
    "    try:\n",
    "        spam_count = int(spam_count)\n",
    "        ham_count = int(ham_count)\n",
    "    except ValueError:\n",
    "        # We couldn't make count into an int, so move on\n",
    "        continue\n",
    "        \n",
    "    if prev_word == word:\n",
    "        # We have not moved to a new word\n",
    "        # Just update the count of this word\n",
    "        prev_spam_count += spam_count\n",
    "        prev_ham_count += ham_count\n",
    "        \n",
    "    else:\n",
    "        # We have encountered a new word!\n",
    "        # If this is the first word, we don't need to print anything\n",
    "        # Also only print the word and increment if total occurences is >= 3\n",
    "        if prev_word and prev_spam_count + prev_ham_count >= 3: \n",
    "            # Write the previous word to file\n",
    "            print '%s\\t%s\\t%s' % (prev_word, prev_spam_count, prev_ham_count)\n",
    "            if prev_word[0] != '*': vocab_size += 1\n",
    "            \n",
    "        # Now we need to initialize our variables\n",
    "        prev_word = word\n",
    "        prev_spam_count = spam_count\n",
    "        prev_ham_count = ham_count\n",
    "\n",
    "# We've reached the end of the file\n",
    "# Print the last word and counts\n",
    "print '%s\\t%s\\t%s' % (prev_word, prev_spam_count, prev_ham_count)\n",
    "if prev_word[0] != '*': vocab_size += 1\n",
    "    \n",
    "# Print the vocab size for this set of words\n",
    "print '%s\\t%s\\t%s' % ('*vocab', vocab_size, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/miki/week02/hw2_5_output_fit1': No such file or directory\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.5.0.jar] /tmp/streamjob2782948487555149227.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "# Change permissions on mapper and reducer\n",
    "!chmod +x mapper.py\n",
    "!chmod +x reducer.py\n",
    "\n",
    "# Delete output folder if it exists\n",
    "!hdfs dfs -rm -r /user/miki/week02/hw2_5_output_fit1\n",
    "\n",
    "# Run job\n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-input /user/miki/week02/enronemail_1h.txt \\\n",
    "-output /user/miki/week02/hw2_5_output_fit1 \\\n",
    "-mapper /home/cloudera/Documents/W261-Fall2016/Week02/mapper.py \\\n",
    "-reducer /home/cloudera/Documents/W261-Fall2016/Week02/reducer.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "\n",
    "We are working with a file that is small in size, and so there is only one reducer involved. If there were multiple reducers involved, we would need to run another reducer to sum the values of \\*vocab, since we need unique counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!hdfs dfs -cat /user/miki/week02/hw2_5_output_fit1/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapper/Reducer #1 for predicting NB\n",
    "\n",
    "Mapper:\n",
    "- Input from fit: (word, spam count, ham count)\n",
    "- Output: (word, spam count, ham count)\n",
    "- Input testing document: (document ID, cat, subj, body)\n",
    "- Output: (word, cat, document ID)\n",
    "\n",
    "Reducer:\n",
    "- Input: (word, spam count, ham count)\n",
    "- Input: (word, cat, document ID)\n",
    "- Output: (document ID, cat, word, spam count, ham count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: mapper code for HW2.5\n",
    "\n",
    "import sys\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    \n",
    "    # Strip white space from line, then split into fields    \n",
    "    fields = line.strip().split('\\t')\n",
    "    \n",
    "    # If first field matches pattern of document ID, tokenize words\n",
    "    pattern = re.compile('^\\d{4}.\\d{4}-\\d{2}-\\d{2}\\.\\w+')\n",
    "    if pattern.match(fields[0]):\n",
    "        # Keep track of document counts\n",
    "        doc_id = fields[0]\n",
    "        spam = fields[1]\n",
    "\n",
    "        # We are always going to need the doc/word counts for each document\n",
    "        print '%s^%s^%s' % ('*alldocs', spam, doc_id)\n",
    "        print '%s^%s^%s' % ('*docs', spam, doc_id)\n",
    "        print '%s^%s^%s' % ('*words', spam, doc_id)\n",
    "        print '%s^%s^%s' % ('*vocab', spam, doc_id)\n",
    "        \n",
    "        # Replace commas with spaces (we are using commas as a delimiter as well)\n",
    "        # Remove remaining punctuation from subject and body\n",
    "        # Concatenate, then split subject and body by spaces\n",
    "        # Some records are malformed -- if there is a 4th field, use it\n",
    "        subj = fields[2].replace(',', ' ')\n",
    "        subj = subj.translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "        if len(fields) == 4:\n",
    "            body = fields[3].replace(',', ' ')\n",
    "            body = body.translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "        else:\n",
    "            body = \"\"\n",
    "        words = subj + \" \" + body\n",
    "        words = words.split()\n",
    "\n",
    "        # Loop through words\n",
    "        # If word is not trivial, write to file\n",
    "        # key = word\n",
    "        # value = doc_id\n",
    "        for word in words:\n",
    "            if len(word) > 0 and repr(word)[1] != '\\\\':\n",
    "                print '%s^%s^%s' % (word, spam, doc_id)\n",
    "    else:\n",
    "        # Now we know that the record is the\n",
    "        #   output of the previous MapReduce job\n",
    "        #   not the doc we are classifying\n",
    "        # We append the spam_count with a * so that it comes first\n",
    "        # We need the overall spam/ham word counts with each word in the doc\n",
    "        word = fields[0]\n",
    "        spam_count = fields[1]\n",
    "        ham_count = fields[2]\n",
    "        print '%s^%s^%s' % (word, '*' + spam_count, ham_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: reducer code for HW2.5\n",
    "\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "import math\n",
    "import re\n",
    "\n",
    "# Initialize some variables\n",
    "spam_count = 0\n",
    "ham_count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # Strip and split line\n",
    "    key, value = line.strip().split('\\t')\n",
    "    word, field1 = key.split('^')\n",
    "\n",
    "    # If field1 starts with a *, we know that it is the spam and ham counts\n",
    "    if field1[0] == '*':\n",
    "        # This record will be of the form (word field1=*spam_count value=ham_count)\n",
    "        try:\n",
    "            spam_count = int(field1.replace('*',''))\n",
    "            ham_count = int(value)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    \n",
    "    else:\n",
    "        # Now we know that it is a doc_id record\n",
    "        # This record will be of the form (word field1=cat value=doc_id)\n",
    "        pattern = re.compile('^\\d{4}.\\d{4}-\\d{2}-\\d{2}\\.\\w+')\n",
    "        if pattern.match(value):\n",
    "            doc_id = value\n",
    "        print '%s\\t%s\\t%s\\t%s\\t%s' % (doc_id, field1, word, spam_count, ham_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/miki/week02/hw2_5_output_predict1': No such file or directory\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.5.0.jar] /tmp/streamjob2643183505328283945.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "# Change permissions on mapper and reducer\n",
    "!chmod +x mapper.py\n",
    "!chmod +x reducer.py\n",
    "\n",
    "# Delete output folder if it exists\n",
    "!hdfs dfs -rm -r /user/miki/week02/hw2_5_output_predict1\n",
    "\n",
    "# Run job\n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-D stream.map.output.field.separator=^ \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D mapreduce.map.output.key.field.separator=^ \\\n",
    "-D mapreduce.partition.keypartitioner.options=-k1,1 \\\n",
    "-input /user/miki/week02/enronemail_1h.txt \\\n",
    "-input /user/miki/week02/hw2_5_output_fit1 \\\n",
    "-output /user/miki/week02/hw2_5_output_predict1 \\\n",
    "-mapper /home/cloudera/Documents/W261-Fall2016/Week02/mapper.py \\\n",
    "-reducer /home/cloudera/Documents/W261-Fall2016/Week02/reducer.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!hdfs dfs -cat /user/miki/week02/hw2_5_output_predict1/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapper/Reducer #2 for predicting NB\n",
    "\n",
    "Mapper:\n",
    "- Input from predict1: (doc_id, cat, word, spam count, ham count)\n",
    "- Output: identity\n",
    "\n",
    "Reducer:\n",
    "- Input: (doc_id, cat, word, spam count, ham count)\n",
    "- Output: (doc_id, cat, prediction, spam log prob, ham log prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: mapper code for HW2.5\n",
    "\n",
    "import sys\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    \n",
    "    # Replace delimiter\n",
    "    line = line.replace('\\n', '')\n",
    "    fields = line.split('\\t')\n",
    "    print '%s^%s^%s^%s^%s' % (fields[0], fields[1], fields[2], fields[3], fields[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: reducer code for HW2.5\n",
    "\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "import math\n",
    "import re\n",
    "\n",
    "# Initialize some variables\n",
    "doc = None\n",
    "spam = None\n",
    "count = 1\n",
    "class_count = {'1':0, '0':0}\n",
    "word = None\n",
    "\n",
    "prev_doc = None\n",
    "prev_spam = None\n",
    "prev_count = 1\n",
    "prev_class_count = {'1':0, '0':0}\n",
    "prev_word = None\n",
    "\n",
    "vocab = 0\n",
    "docs_total = 0\n",
    "docs = {'1':0, '0':0}\n",
    "words_total = 0\n",
    "words = {'1':0, '0':0}\n",
    "log_prior = {'1':0, '0':0}\n",
    "log_posterior = {'1':0, '0':0}\n",
    "log_likelihood = {'1':0, '0':0}\n",
    "\n",
    "classes = {'1':'spam', '0':'ham'}\n",
    "num_errors = {'1':0, '0':0}\n",
    "num_total = 0.0\n",
    "num_correct = 0.0\n",
    "\n",
    "print_debug = False\n",
    "\n",
    "# Create a function to update the posterior\n",
    "# since we need to do it in multiple locations.\n",
    "# We don't want to duplicate code\n",
    "def update_posterior():\n",
    "    for item in classes:\n",
    "        # This is where we incorporate smoothing\n",
    "        log_likelihood[item] = math.log((1 + prev_class_count[item]) / (vocab + words[item]))\n",
    "        log_posterior[item] += prev_count * log_likelihood[item]\n",
    "        if print_debug:\n",
    "            print \"updated log posterior:\", log_posterior[item]\n",
    "            print '\\n'\n",
    "\n",
    "def make_prediction(): \n",
    "    global num_total, num_correct\n",
    "    \n",
    "    # We can compare non-normalized posterior probabilities\n",
    "    num_total += 1\n",
    "    if log_posterior['1'] > log_posterior['0']: prediction = '1'\n",
    "    else: prediction = '0'\n",
    "\n",
    "    # Count correct guesses\n",
    "    if prev_spam == prediction:\n",
    "        num_correct += 1\n",
    "        \n",
    "    # Output log posteriors. We can normalize later.\n",
    "    print '%s\\t%s\\t%s\\t%s\\t%s\\n' % (prev_doc, prev_spam, prediction, \n",
    "                                    log_posterior['1'],\n",
    "                                    log_posterior['0'])\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.replace('\\n', '')\n",
    "    \n",
    "    #Split when testing locally\n",
    "    #doc, spam, word, count['1'], count['0'] = line.split('\\t')\n",
    "    \n",
    "    # Split when using Hadoop\n",
    "    key, value = line.strip().split('\\t')\n",
    "    doc, spam, word = key.split('^')\n",
    "    class_count['1'], class_count['0'] = value.split('^')    \n",
    "\n",
    "    # Keep this in a try/except statement so we don't fail\n",
    "    try:\n",
    "        for item in classes:\n",
    "            class_count[item] = float(class_count[item])\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "    # Let's calculate some probabilities\n",
    "    if prev_doc == doc:\n",
    "        # We haven't changed documents\n",
    "        if prev_word == word:\n",
    "            # We haven't changed words, so just increment\n",
    "            prev_count += 1\n",
    "\n",
    "        else:\n",
    "            # We are at a new word\n",
    "            # We need to check if we are at a keyword\n",
    "            if print_debug: print '\\n', prev_word, '\\n'\n",
    "            if prev_word == '*alldocs': \n",
    "                # We are at a record where we need to output total docs\n",
    "                docs_total = prev_class_count['1']\n",
    "                if print_debug: print \"total docs:\", docs_total\n",
    "\n",
    "            elif prev_word == '*docs': \n",
    "                # We are at a record where we need to output unique docs per class\n",
    "                for item in classes:\n",
    "                    docs[item] = prev_class_count[item]\n",
    "                    if print_debug: print \"prior\", item, docs[item], '/', docs_total\n",
    "                    log_prior[item] = math.log(docs[item] / docs_total)\n",
    "\n",
    "                    # We will update the posterior after each word\n",
    "                    # Initialize it to the prior\n",
    "                    log_posterior[item] = log_prior[item]\n",
    "                if print_debug: \n",
    "                    print \"log prior:\", log_prior\n",
    "                    print 'log posterior initial', log_posterior\n",
    "\n",
    "            elif prev_word == '*words':\n",
    "                # We are at a record where we need to output words per class\n",
    "                for item in classes:\n",
    "                    words[item] = prev_class_count[item]\n",
    "                words_total = sum(prev_class_count.values())\n",
    "                if print_debug: print \"word class_count:\", words\n",
    "                                \n",
    "            elif prev_word == '*vocab':\n",
    "                # We are at a record where we need to output the vocab size\n",
    "                vocab = prev_class_count['1']\n",
    "                if print_debug: print \"vocab size:\", vocab\n",
    "\n",
    "            elif prev_word:\n",
    "                # We are at a new normal word, and need to calculate stuff\n",
    "                update_posterior()\n",
    "\n",
    "            prev_word = word\n",
    "            prev_count = 1\n",
    "            for item in classes:\n",
    "                prev_class_count[item] = class_count[item]\n",
    "\n",
    "    else:\n",
    "        # We are done with one document. We need to: \n",
    "        # - process the last word\n",
    "        # - output our predictions\n",
    "        if prev_doc:\n",
    "            if print_debug: print '\\n', prev_word, '\\n'\n",
    "            # We are at a new normal word, and need to calculate stuff\n",
    "            update_posterior()\n",
    "\n",
    "            # Now we can calculate the prediction\n",
    "            make_prediction()\n",
    "            if print_debug: print num_correct, \"out of\", num_total\n",
    "\n",
    "        prev_doc = doc\n",
    "        prev_spam = spam\n",
    "        prev_word = word\n",
    "        for item in classes:\n",
    "            prev_class_count[item] = class_count[item]\n",
    "        log_likelihood = {'1':0, '0':0}\n",
    "        if print_debug: print \"reset log likelihood\"\n",
    "\n",
    "# Output our final prediction\n",
    "if print_debug: print '\\n', prev_word, '\\n'\n",
    "update_posterior()\n",
    "make_prediction()\n",
    "\n",
    "print \"Number of documents\\t%d\" % (num_total)\n",
    "print \"Number correct predictions\\t%d\" % (num_correct)\n",
    "print \"Error rate\\t%s\" % (100 - 100 * num_correct / num_total) + \"%\"\n",
    "print \"Number of zero probability spam\\t%d\" % (num_errors['1'])\n",
    "print \"Number of zero probability ham\\t%d\" % (num_errors['0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/miki/week02/hw2_5_output_predict2': No such file or directory\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.5.0.jar] /tmp/streamjob4882416490496642420.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "# Change permissions on mapper and reducer\n",
    "!chmod +x mapper.py\n",
    "!chmod +x reducer.py\n",
    "\n",
    "# Delete output folder if it exists\n",
    "!hdfs dfs -rm -r /user/miki/week02/hw2_5_output_predict2\n",
    "\n",
    "# Run job\n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-D stream.map.output.field.separator=^ \\\n",
    "-D stream.num.map.output.key.fields=3 \\\n",
    "-D mapreduce.map.output.key.field.separator=^ \\\n",
    "-D mapreduce.partition.keypartitioner.options=-k1,2 \\\n",
    "-input /user/miki/week02/hw2_5_output_predict1/part* \\\n",
    "-output /user/miki/week02/hw2_5_output_predict2 \\\n",
    "-mapper /home/cloudera/Documents/W261-Fall2016/Week02/mapper.py \\\n",
    "-reducer /home/cloudera/Documents/W261-Fall2016/Week02/reducer.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!hdfs dfs -cat /user/miki/week02/hw2_4_output_predict2/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove `hw2_5_output_predict2.txt': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "# Copy files from HDFS\n",
    "!rm hw2_5_output_predict2.txt\n",
    "!hdfs dfs -copyToLocal /user/miki/week02/hw2_5_output_predict2/part-00000 \\\n",
    "hw2_5_output_predict2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document                       Truth     Predicted    Log Prob Spam   Log Prob Ham\n",
      "----------------------------------------------------------------------------------\n",
      "0001.1999-12-10.farmer           0           0           -44.424640     -42.613368\n",
      "0001.1999-12-10.kaminski         0           0           -31.452532     -27.260891\n",
      "0001.2000-01-17.beck             0           0         -3598.936015   -3274.082617\n",
      "0001.2000-06-06.lokay            0           0         -3875.694850   -3453.212023\n",
      "0001.2001-02-07.kitchen          0           0          -335.955246    -296.184093\n",
      "0001.2001-04-02.williams         0           0         -1341.716585   -1306.951502\n",
      "0002.1999-12-13.farmer           0           0         -2920.512866   -2632.211214\n",
      "0002.2001-02-07.kitchen          0           0          -442.933630    -429.812761\n",
      "0002.2001-05-25.SA_and_HP        1           1          -557.808327    -603.814689\n",
      "0002.2003-12-18.GP               1           1         -1204.417793   -1306.522525\n",
      "0002.2004-08-01.BG               1           1          -835.613746    -859.399974\n",
      "0003.1999-12-10.kaminski         0           0          -434.300198    -382.087869\n",
      "0003.1999-12-14.farmer           0           0           -95.175993     -77.208533\n",
      "0003.2000-01-17.beck             0           0         -1379.905343   -1201.880769\n",
      "0003.2001-02-08.kitchen          0           0         -1323.000068   -1190.590384\n",
      "0003.2003-12-18.GP               1           1          -821.815748    -852.632599\n",
      "0003.2004-08-01.BG               1           1          -737.808892    -770.468459\n",
      "0004.1999-12-10.kaminski         0           0         -1160.276052   -1056.478168\n",
      "0004.1999-12-14.farmer           0           0         -1112.765453    -931.258646\n",
      "0004.2001-04-02.williams         0           0          -656.483733    -621.575841\n",
      "0004.2001-06-12.SA_and_HP        1           1          -875.415235    -938.920257\n",
      "0004.2004-08-01.BG               1           1          -710.704854    -725.537945\n",
      "0005.1999-12-12.kaminski         0           0          -815.819945    -722.819487\n",
      "0005.1999-12-14.farmer           0           0         -1125.633166    -934.918951\n",
      "0005.2000-06-06.lokay            0           0          -403.459322    -395.599011\n",
      "0005.2001-02-08.kitchen          0           0          -846.763217    -789.866214\n",
      "0005.2001-06-23.SA_and_HP        1           1          -184.955797    -190.508349\n",
      "0005.2003-12-18.GP               1           1         -7228.083454   -7613.494918\n",
      "0006.1999-12-13.kaminski         0           0          -498.333505    -475.940658\n",
      "0006.2001-02-08.kitchen          0           0        -10033.359305   -9300.740920\n",
      "0006.2001-04-03.williams         0           0          -297.426675    -296.371846\n",
      "0006.2001-06-25.SA_and_HP        1           1          -358.436560    -373.939080\n",
      "0006.2003-12-18.GP               1           1         -1050.879077   -1076.660308\n",
      "0006.2004-08-01.BG               1           1         -1004.798750   -1019.975163\n",
      "0007.1999-12-13.kaminski         0           0         -1540.752163   -1431.276776\n",
      "0007.1999-12-14.farmer           0           0          -721.798885    -660.155784\n",
      "0007.2000-01-17.beck             0           0         -3311.282016   -2682.726591\n",
      "0007.2001-02-09.kitchen          0           0         -1758.827907   -1614.740056\n",
      "0007.2003-12-18.GP               1           1         -1182.059129   -1193.319263\n",
      "0007.2004-08-01.BG               1           1         -1424.873267   -1624.217221\n",
      "0008.2001-02-09.kitchen          0           0         -4041.394059   -3764.164598\n",
      "0008.2001-06-12.SA_and_HP        1           1          -875.415235    -938.920257\n",
      "0008.2001-06-25.SA_and_HP        1           1         -4065.914107   -4535.992688\n",
      "0008.2003-12-18.GP               1           1          -965.834962   -1054.462410\n",
      "0008.2004-08-01.BG               1           1         -5613.892996   -5764.696265\n",
      "0009.1999-12-13.kaminski         0           0         -6498.349486   -5279.299185\n",
      "0009.1999-12-14.farmer           0           0          -528.331486    -469.267517\n",
      "0009.2000-06-07.lokay            0           0         -2898.471607   -2600.749354\n",
      "0009.2001-02-09.kitchen          0           0         -5962.670288   -5398.636652\n",
      "0009.2001-06-26.SA_and_HP        1           1         -1213.824073   -1287.577931\n",
      "0009.2003-12-18.GP               1           1          -730.287374    -743.059058\n",
      "0010.1999-12-14.farmer           0           0         -1335.012059   -1151.571769\n",
      "0010.1999-12-14.kaminski         0           0          -216.279935    -195.318687\n",
      "0010.2001-02-09.kitchen          0           0         -3224.382789   -2885.254294\n",
      "0010.2001-06-28.SA_and_HP        1           1         -3328.419593   -3611.098895\n",
      "0010.2003-12-18.GP               1           0           -51.116624     -50.529014\n",
      "0010.2004-08-01.BG               1           1         -2207.539609   -2411.879233\n",
      "0011.1999-12-14.farmer           0           0         -2024.401813   -1857.150668\n",
      "0011.2001-06-28.SA_and_HP        1           1         -3320.705958   -3601.478500\n",
      "0011.2001-06-29.SA_and_HP        1           1        -15328.604186  -16862.477569\n",
      "0011.2003-12-18.GP               1           1          -492.070730    -533.478831\n",
      "0011.2004-08-01.BG               1           1          -655.543123    -674.458708\n",
      "0012.1999-12-14.farmer           0           0         -3334.555392   -3008.349057\n",
      "0012.1999-12-14.kaminski         0           0         -1056.490082    -843.793004\n",
      "0012.2000-01-17.beck             0           0         -3299.876648   -2676.958826\n",
      "0012.2000-06-08.lokay            0           0          -894.996749    -877.669050\n",
      "0012.2001-02-09.kitchen          0           0          -495.757675    -466.407578\n",
      "0012.2003-12-19.GP               1           1          -150.399908    -157.325333\n",
      "0013.1999-12-14.farmer           0           0         -1903.460494   -1757.662326\n",
      "0013.1999-12-14.kaminski         0           0         -1371.297888   -1125.764257\n",
      "0013.2001-04-03.williams         0           0          -654.372571    -639.232189\n",
      "0013.2001-06-30.SA_and_HP        1           1        -27830.855180  -30674.764534\n",
      "0013.2004-08-01.BG               1           1         -1452.708756   -1497.310172\n",
      "0014.1999-12-14.kaminski         0           0         -1883.609416   -1609.104483\n",
      "0014.1999-12-15.farmer           0           0         -1145.060046   -1037.009348\n",
      "0014.2001-02-12.kitchen          0           0         -1365.168002   -1252.619726\n",
      "0014.2001-07-04.SA_and_HP        1           1         -3389.997853   -3622.513565\n",
      "0014.2003-12-19.GP               1           1          -155.268988    -160.070641\n",
      "0014.2004-08-01.BG               1           1          -750.181112    -754.083771\n",
      "0015.1999-12-14.kaminski         0           0          -570.957454    -524.335361\n",
      "0015.1999-12-15.farmer           0           0          -792.414609    -697.051165\n",
      "0015.2000-06-09.lokay            0           0          -133.201724    -127.517282\n",
      "0015.2001-02-12.kitchen          0           0         -5299.632245   -4851.380603\n",
      "0015.2001-07-05.SA_and_HP        1           1          -873.814252    -942.811729\n",
      "0015.2003-12-19.GP               1           1         -1242.483622   -1328.421822\n",
      "0016.1999-12-15.farmer           0           0          -743.657300    -690.008458\n",
      "0016.2001-02-12.kitchen          0           0         -1237.519292   -1104.917834\n",
      "0016.2001-07-05.SA_and_HP        1           1          -873.814252    -942.811729\n",
      "0016.2001-07-06.SA_and_HP        1           1        -16330.267756  -18189.205604\n",
      "0016.2003-12-19.GP               1           1          -753.579285    -761.725704\n",
      "0016.2004-08-01.BG               1           1          -652.008959    -691.922618\n",
      "0017.1999-12-14.kaminski         0           0          -385.138107    -372.492804\n",
      "0017.2000-01-17.beck             0           0         -3310.886121   -2684.787462\n",
      "0017.2001-04-03.williams         0           0          -437.647386    -427.120973\n",
      "0017.2003-12-18.GP               1           1          -201.116384    -215.062958\n",
      "0017.2004-08-01.BG               1           0          -830.829440    -802.159866\n",
      "0017.2004-08-02.BG               1           1         -2450.745605   -2536.370610\n",
      "0018.1999-12-14.kaminski         0           0          -978.129008    -976.182721\n",
      "0018.2001-07-13.SA_and_HP        1           1         -3220.289039   -3388.497833\n",
      "0018.2003-12-18.GP               1           1         -3347.252559   -3474.140743\n",
      "----------------------------------------------------------------------------------\n",
      "Number of documents                      100\n",
      "Number correct predictions                98\n",
      "Error rate                              2.0%\n",
      "Number of zero probability spam            0\n",
      "Number of zero probability ham             0\n"
     ]
    }
   ],
   "source": [
    "print_results('hw2_5_output_predict2.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does it affect the misclassifcation error of learnt naive multinomial Bayesian Classifier on the training dataset?\n",
    "\n",
    "Excluding words that occur fewer than three times increases our error rate to 2%. This is because we are excluding information from our model, so there is more chance for the model to make a mistake. However, including all of the information can be considered a form of overfitting, so removing these rarely-occurring words may make the model more robust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.6: Benchmark your code with the Python SciKit-Learn implementation of the multinomial Naive Bayes algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run the Multinomial Naive Bayes algorithm (using default settings) from SciKit-Learn over the same training data used in HW2.5 and report the misclassification error (please note some data preparation might be needed to get the Multinomial Naive Bayes algorithm from SkiKit-Learn to run over this dataset)\n",
    "- Prepare a table to present your results, where rows correspond to approach used (SciKit-Learn versus your Hadoop implementation) and the column presents the training misclassification error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# General libraries\n",
    "from __future__ import division\n",
    "\n",
    "# SK-learn libraries for learning\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,) (100,)\n"
     ]
    }
   ],
   "source": [
    "# Read data in and create data and label arrays\n",
    "ids, X, Y = [], [], []\n",
    "\n",
    "with open('enronemail_1h.txt', 'r') as myfile:\n",
    "    for line in myfile:\n",
    "        fields = line.split(\"\\t\")\n",
    "        \n",
    "        # Some records are malformed, so make sure that we take the right fields\n",
    "        subj, body = \"\", \"\"\n",
    "        \n",
    "        if len(fields) >= 3:\n",
    "            subj = fields[2]\n",
    "        if len(fields) >= 4:\n",
    "            body = fields[3]\n",
    "\n",
    "        text = subj + \" \" + body\n",
    "        text = text.replace(\"\\n\", \"\")\n",
    "        X.append(text)\n",
    "        Y.append(fields[1])\n",
    "        ids.append(fields[0])\n",
    "\n",
    "# Convert these to numpy arrays\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "# Check that the shapes look correct\n",
    "print X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method                    Error\n",
      "--------------------------------\n",
      "SK-learn with alpha=1        0%\n",
      "SK-learn with min_df=3       4%\n",
      "HW2.3 (no smoothing)         0%\n",
      "HW2.4 (+1 smoothing)         0%\n",
      "HW2.5 (drop words <3)        2%\n"
     ]
    }
   ],
   "source": [
    "def hw2_6():\n",
    "    train_errors = []\n",
    "    \n",
    "    ##### MULTINOMIAL NB\n",
    "    # Create Pipeline to get feature vectors and train\n",
    "    # Use CountVectorizer to get feature arrays\n",
    "    # Classify using Multinomial NB\n",
    "    mnb_pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                         ('clf', MultinomialNB()),\n",
    "                        ])\n",
    "\n",
    "    # Fit training data and labels\n",
    "    mnb_pipe.fit(X, Y)\n",
    "\n",
    "    # Print training error\n",
    "    mnb_predictions = mnb_pipe.predict(X)\n",
    "    train_errors.append([\"SK-learn with alpha=1\",sum(mnb_predictions != Y) / Y.size])\n",
    "    \n",
    "    ##### MULTINOMIAL NB WITH MIN_DF\n",
    "    # This should exclude words that do not meet a minimum frequency\n",
    "    \n",
    "    mnb_pipe.set_params(vect__min_df=3, clf__fit_prior=True)\n",
    "    \n",
    "    # Fit training data and labels\n",
    "    mnb_pipe.fit(X, Y)\n",
    "    \n",
    "    # Print training error\n",
    "    mnb_predictions = mnb_pipe.predict(X)\n",
    "    train_errors.append([\"SK-learn with min_df=3\",sum(mnb_predictions != Y) / Y.size])\n",
    "    \n",
    "    \n",
    "    ##### CLASSIFIER in HW2.3\n",
    "    # Read output from results\n",
    "    \n",
    "    incorrect = 0.0\n",
    "    total = 0.0\n",
    "\n",
    "    # Read in data from text file\n",
    "    with open('hw2_3_output_predict2.txt', 'r') as myfile:\n",
    "        for line in myfile:\n",
    "            fields = line.split()\n",
    "\n",
    "            # Only look at records that have a doc_id as the first field\n",
    "            pattern = re.compile('^\\d{4}.\\d{4}-\\d{2}-\\d{2}\\.\\w+')\n",
    "            try:\n",
    "                key = fields[0]\n",
    "            except:\n",
    "                continue\n",
    "            if pattern.match(fields[0]):\n",
    "                total += 1\n",
    "                if fields[1] != fields[2]: incorrect += 1\n",
    "    \n",
    "    train_errors.append([\"HW2.3 (no smoothing)\",incorrect/total])\n",
    "    \n",
    "    \n",
    "    ##### CLASSIFIER in HW2.4\n",
    "    # Read output from results\n",
    "    \n",
    "    incorrect = 0.0\n",
    "    total = 0.0\n",
    "\n",
    "    # Read in data from text file\n",
    "    with open('hw2_4_output_predict2.txt', 'r') as myfile:\n",
    "        for line in myfile:\n",
    "            fields = line.split()\n",
    "\n",
    "            # Only look at records that have a doc_id as the first field\n",
    "            pattern = re.compile('^\\d{4}.\\d{4}-\\d{2}-\\d{2}\\.\\w+')\n",
    "            try:\n",
    "                key = fields[0]\n",
    "            except:\n",
    "                continue\n",
    "            if pattern.match(fields[0]):\n",
    "                total += 1\n",
    "                if fields[1] != fields[2]: incorrect += 1\n",
    "    \n",
    "    train_errors.append([\"HW2.4 (+1 smoothing)\",incorrect/total])\n",
    "    \n",
    "    ##### CLASSIFIER in HW2.5\n",
    "    # Read output from results\n",
    "    \n",
    "    incorrect = 0.0\n",
    "    total = 0.0\n",
    "\n",
    "    # Read in data from text file\n",
    "    with open('hw2_5_output_predict2.txt', 'r') as myfile:\n",
    "        for line in myfile:\n",
    "            fields = line.split()\n",
    "\n",
    "            # Only look at records that have a doc_id as the first field\n",
    "            pattern = re.compile('^\\d{4}.\\d{4}-\\d{2}-\\d{2}\\.\\w+')\n",
    "            try:\n",
    "                key = fields[0]\n",
    "            except:\n",
    "                continue\n",
    "            if pattern.match(fields[0]):\n",
    "                total += 1\n",
    "                if fields[1] != fields[2]: incorrect += 1\n",
    "    \n",
    "    train_errors.append([\"HW2.5 (drop words <3)\",incorrect/total])\n",
    "    \n",
    "    ##### TABLE OF TRAINING ERRORS\n",
    "    print \"{:<25s}{:>6s}\".format(\"Method\", \"Error\")\n",
    "    print \"--------------------------------\"\n",
    "    for method in train_errors:\n",
    "        print \"{:<25s}{:>6.0%}\".format(method[0], method[1])\n",
    "    \n",
    "hw2_6()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explain/justify any differences in terms of training error rates over the dataset in HW2.5 between your Multinomial Naive Bayes implementation (in Map Reduce) versus the Multinomial Naive Bayes implementation in SciKit-Learn\n",
    "\n",
    "We can compare the default SK-learn MultinomialNB classifier with alpha=1 to ours in question 2.4 (Laplace +1 smoothing). In both of these classifiers, the error rate is 0%.\n",
    "\n",
    "We can also compare the SK-learn classifier with min_df=3 to our classifier in question 2.5. In this case, the SK-learn classifier has a 4% error rate, while ours has a 2% error rate. There may be difference in the way that the words are tokenized (in SK-learn, I used CountVectorizer with min_df=3). \n",
    "\n",
    "By default, SK-learn does not fit a prior probability, but instead uses a uniform prior probability. This might result in a small difference, however, when I used the option to fit a prior in SK-learn, the resulting error rate was no different than using a uniform prior (the default)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
