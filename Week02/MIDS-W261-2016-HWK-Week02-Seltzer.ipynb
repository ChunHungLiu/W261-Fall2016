{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASCI W261: Machine Learning at Scale\n",
    "## Assignment Week 2\n",
    "Miki Seltzer (miki.seltzer@berkeley.edu)<br>\n",
    "W261-2, Spring 2016<br>\n",
    "Submission: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### HW2.0:\n",
    "\n",
    "#### What is a race condition in the context of parallel computation? Give an example.\n",
    "A race condition is when a section of code is executed by multiple processes, and the order in which the processes execute will impact the final result.\n",
    "\n",
    "![Race condition example](race_conditions.png)\n",
    "Source: https://en.wikipedia.org/wiki/Race_condition#Example\n",
    "\n",
    "#### What is MapReduce?\n",
    "MapReduce can refer to multiple concepts:\n",
    "- **Programming model:** Processes are split into a \"mapping\" phase, and a \"reducing\" phase. In the map phase, a certain function is mapped on to each value in a data set, and then in the reduce phase, the result of the map phase is aggregated. \n",
    "- **Execution framework:** This framework coordinates running processes written with the above model in mind.\n",
    "- **Software implementation:** MapReduce is the name of Google's proprietary implementation of this programming model, while Apache Hadoop is the open-source implementation.\n",
    "\n",
    "#### How does it differ from Hadoop?\n",
    "Hadoop is the open-source implementation of Google's MapReduce. Hadoop consists of two parts: distributed storage of data, and distributed processing of data. HDFS is the storage part, and MapReduce is the processing part.\n",
    "\n",
    "#### Which programming paradigm is Hadoop based on? Explain and give a simple example in code and show the code running.\n",
    "Hadoop is based on the MapReduce paradigm. The classic example of the MapReduce programming paradigm is word count. In the map phase of word count, each word in a document is assigned a count of 1. In the reduce phase, the counts for each unique word are summed to yield the final count of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP PHASE\n",
      "['hello', 1]\n",
      "['this', 1]\n",
      "['is', 1]\n",
      "['a', 1]\n",
      "['test', 1]\n",
      "['to', 1]\n",
      "['test', 1]\n",
      "['word', 1]\n",
      "['count', 1]\n",
      "['test', 1]\n",
      "['should', 1]\n",
      "['have', 1]\n",
      "['a', 1]\n",
      "['count', 1]\n",
      "['of', 1]\n",
      "['three', 1]\n",
      "\n",
      "REDUCE PHASE\n",
      "['a', 1] (intermediate step)\n",
      "['a', 2] FINAL SUM\n",
      "['count', 1] (intermediate step)\n",
      "['count', 2] FINAL SUM\n",
      "['have', 1] FINAL SUM\n",
      "['hello', 1] FINAL SUM\n",
      "['is', 1] FINAL SUM\n",
      "['of', 1] FINAL SUM\n",
      "['should', 1] FINAL SUM\n",
      "['test', 1] (intermediate step)\n",
      "['test', 2] (intermediate step)\n",
      "['test', 3] FINAL SUM\n",
      "['this', 1] FINAL SUM\n",
      "['three', 1] FINAL SUM\n",
      "['to', 1] FINAL SUM\n",
      "['word', 1] FINAL SUM\n"
     ]
    }
   ],
   "source": [
    "def hw2_0():\n",
    "    doc = \"Hello this is a test to test word count test should have a count of three\".lower()\n",
    "    key_vals = []\n",
    "    \n",
    "    print \"MAP PHASE\"\n",
    "    for word in doc.split():\n",
    "        print [word, 1]\n",
    "        key_vals.append([word, 1])\n",
    "    \n",
    "    print \"\\nREDUCE PHASE\"\n",
    "    key_vals = sorted(key_vals)\n",
    "    \n",
    "    current_word = None\n",
    "    current_count = 0\n",
    "    \n",
    "    for pair in key_vals:\n",
    "        if current_word == pair[0]:\n",
    "            print [current_word, current_count], \"(intermediate step)\"\n",
    "            current_count += pair[1]\n",
    "        else:\n",
    "            if current_word:\n",
    "                print [current_word, current_count], \"FINAL SUM\"\n",
    "            current_word = pair[0]\n",
    "            current_count = pair[1]\n",
    "\n",
    "    print [current_word, current_count], \"FINAL SUM\"\n",
    "    \n",
    "hw2_0()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.1: Sort in Hadoop MapReduce\n",
    "**Given as input: Records of the form `<integer, “NA”>`, where integer is any integer, and “NA” is just the empty string.**<br>\n",
    "**Output: Sorted key value pairs of the form `<integer, “NA”>`; what happens if you have multiple reducers? Do you need additional steps? Explain.**\n",
    "\n",
    "If there are multiple reducers, then a straightforward MapReduce process will yield outputs that are sorted within each reducer, but not sorted across all reducers. In order to output a sort across all reducers, an extra step needs to be implemented that will intelligently send keys to reducers so that the result from all reducers will yield a complete sort. For example, let's say our keys ranged from 0-300. If we had 3 reducers, we could send all keys in the range [0,100) to reducer 1, [100, 200) to reducer 2, and [200, 300] to reducer 3. Thus, the output of each reducer will yield documents that are completely sorted. We would need to balance the keys sent to each reducer to ensure that the load is still balanced between all reducers, which will require some calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write code to generate N  random records of the form `<integer, “NA”>`. Let N = 10,000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to need the Hadoop Streaming jar file, so let's download it here so that we know which one to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-01-21 18:37:45--  http://central.maven.org/maven2/org/apache/hadoop/hadoop-streaming/2.7.1/hadoop-streaming-2.7.1.jar\n",
      "Resolving central.maven.org... 23.235.47.209\n",
      "Connecting to central.maven.org|23.235.47.209|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 105736 (103K) [application/java-archive]\n",
      "Saving to: “hadoop-streaming-2.7.1.jar”\n",
      "\n",
      "100%[======================================>] 105,736     --.-K/s   in 0.1s    \n",
      "\n",
      "2016-01-21 18:38:00 (1.06 MB/s) - “hadoop-streaming-2.7.1.jar” saved [105736/105736]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://central.maven.org/maven2/org/apache/hadoop/hadoop-streaming/2.7.1/hadoop-streaming-2.7.1.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "with open(\"random.txt\", \"w\") as myfile:\n",
    "    for i in range(10000):\n",
    "        myfile.write(\"{:d},{:s}\\n\".format(random.randint(0, 100000), \"NA\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write the Python Hadoop streaming map-reduce job to perform this sort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: mapper code for HW2.1\n",
    "\n",
    "import sys\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # In this case, we do not need to map the input to anything\n",
    "    print line.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: reducer code for HW2.1\n",
    "\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # In this case, we do not need to reduce anything\n",
    "    print line.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod +x mapper.py\n",
    "!chmod +x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How do I specify the partitioner?\n",
    "# Since MapReduce will automatically sort between partitions,\n",
    "# I need a way to make sure that the reducers are given chunks\n",
    "# of data that will result in a total sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.2: Using the Enron data from HW1 and Hadoop MapReduce streaming, write mapper/reducer pair that  will determine the number of occurrences of a single, user-specified word. \n",
    "\n",
    "Examine the word “assistance” and report your results. To do so, make sure that mapper.py counts all occurrences of a single word, and reducer.py collates the counts of the single word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: mapper code for HW2.2\n",
    "\n",
    "import sys\n",
    "\n",
    "# Get the user-specified word\n",
    "keyword = sys.argv[1]\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    \n",
    "    # Strip white space from line, then split\n",
    "    \n",
    "    ######## THIS NEEDS TO BE SPLIT INTO FIELDS, THEN LOOK AT BODY AND SUBJ\n",
    "    words = line.strip().split()\n",
    "    \n",
    "    # Loop through words\n",
    "    # If it matches the keyword, write it to file\n",
    "    # key = word\n",
    "    # value = 1\n",
    "    for word in words:\n",
    "        if word == keyword:\n",
    "            print \"%s\\t%s\" % (word, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: reducer code for HW2.2\n",
    "\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "# Initialize some variables\n",
    "# We know that the words will be sorted\n",
    "# We need to keep track of state\n",
    "prev_word = None\n",
    "prev_count = 0\n",
    "word = None\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # Strip and split line from mapper\n",
    "    word, count = line.strip().split('\\t', 1)\n",
    "    \n",
    "    # If possible, turn count into an int (it's read as a string)\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # We couldn't make count into an int, so move on\n",
    "        continue\n",
    "        \n",
    "    # Since the words will be sorted, all counts for a word will be grouped\n",
    "    if prev_word == word:\n",
    "        # If prev_word is word, then we haven't changed words\n",
    "        # Just update prev_count\n",
    "        prev_count += count\n",
    "    else:\n",
    "        # We've encountered a new word!\n",
    "        # This might be the first word, though\n",
    "        if prev_word:\n",
    "            # We need to print the last word we were on\n",
    "            print \"%s\\t%s\" % (prev_word, prev_count)\n",
    "        \n",
    "        # Now we need to initialize our variables for the new word and count\n",
    "        prev_word = word\n",
    "        prev_count = count\n",
    "\n",
    "# We have reached the end of the file, so print the last word and count\n",
    "if prev_word == word:\n",
    "    print \"%s\\t%s\" % (prev_word, prev_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x mapper.py\n",
    "!chmod +x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo\t3\r\n"
     ]
    }
   ],
   "source": [
    "!echo \"foo foo quux labs foo bar quux\" | python mapper.py 'foo' | sort -k1,1 | python reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up Hadoop folder and load enronemail_1h.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir /user/miki/week02\n",
    "!hdfs dfs -put enronemail_1h.txt /user/miki/week02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Hadoop streaming command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.5.0.jar] /tmp/streamjob5852430093459052275.jar tmpDir=null\n",
      "16/01/21 18:38:54 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/21 18:38:54 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/21 18:38:55 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/21 18:38:55 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/01/21 18:38:55 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1453405632837_0011\n",
      "16/01/21 18:38:55 INFO impl.YarnClientImpl: Submitted application application_1453405632837_0011\n",
      "16/01/21 18:38:55 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1453405632837_0011/\n",
      "16/01/21 18:38:55 INFO mapreduce.Job: Running job: job_1453405632837_0011\n",
      "16/01/21 18:39:01 INFO mapreduce.Job: Job job_1453405632837_0011 running in uber mode : false\n",
      "16/01/21 18:39:01 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/21 18:39:10 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/21 18:39:16 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/21 18:39:16 INFO mapreduce.Job: Job job_1453405632837_0011 completed successfully\n",
      "16/01/21 18:39:16 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=96\n",
      "\t\tFILE: Number of bytes written=341486\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=216505\n",
      "\t\tHDFS: Number of bytes written=13\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=10363\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4035\n",
      "\t\tTotal time spent by all map tasks (ms)=10363\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4035\n",
      "\t\tTotal vcore-seconds taken by all map tasks=10363\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=4035\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=10611712\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=4131840\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=101\n",
      "\t\tMap output records=6\n",
      "\t\tMap output bytes=78\n",
      "\t\tMap output materialized bytes=102\n",
      "\t\tInput split bytes=236\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce shuffle bytes=102\n",
      "\t\tReduce input records=6\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=12\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=145\n",
      "\t\tCPU time spent (ms)=2140\n",
      "\t\tPhysical memory (bytes) snapshot=720977920\n",
      "\t\tVirtual memory (bytes) snapshot=4683366400\n",
      "\t\tTotal committed heap usage (bytes)=693633024\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=216269\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=13\n",
      "16/01/21 18:39:16 INFO streaming.StreamJob: Output directory: /user/miki/week02/hw2_2_output\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-mapper '/home/cloudera/Documents/W261-Fall2016/Week02/mapper.py assistance' \\\n",
    "-reducer /home/cloudera/Documents/W261-Fall2016/Week02/reducer.py \\\n",
    "-input /user/miki/week02/enronemail_1h.txt \\\n",
    "-output /user/miki/week02/hw2_2_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at output of job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance\t6\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/miki/week02/hw2_2_output/part-00000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
