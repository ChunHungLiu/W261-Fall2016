{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASCI W261: Machine Learning at Scale\n",
    "## Assignment Week 2\n",
    "Miki Seltzer (miki.seltzer@berkeley.edu)<br>\n",
    "W261-2, Spring 2016<br>\n",
    "Submission: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### HW2.0:\n",
    "\n",
    "#### What is a race condition in the context of parallel computation? Give an example.\n",
    "A race condition is when a section of code is executed by multiple processes, and the order in which the processes execute will impact the final result.\n",
    "\n",
    "![Race condition example](race_conditions.png)\n",
    "Source: https://en.wikipedia.org/wiki/Race_condition#Example\n",
    "\n",
    "#### What is MapReduce?\n",
    "MapReduce can refer to multiple concepts:\n",
    "- **Programming model:** Processes are split into a \"mapping\" phase, and a \"reducing\" phase. In the map phase, a certain function is mapped on to each value in a data set, and then in the reduce phase, the result of the map phase is aggregated. \n",
    "- **Execution framework:** This framework coordinates running processes written with the above model in mind.\n",
    "- **Software implementation:** MapReduce is the name of Google's proprietary implementation of this programming model, while Apache Hadoop is the open-source implementation.\n",
    "\n",
    "#### How does it differ from Hadoop?\n",
    "Hadoop is the open-source implementation of Google's MapReduce. Hadoop consists of two parts: distributed storage of data, and distributed processing of data. HDFS is the storage part, and MapReduce is the processing part.\n",
    "\n",
    "#### Which programming paradigm is Hadoop based on? Explain and give a simple example in code and show the code running.\n",
    "Hadoop is based on the MapReduce paradigm. The classic example of the MapReduce programming paradigm is word count. In the map phase of word count, each word in a document is assigned a count of 1. In the reduce phase, the counts for each unique word are summed to yield the final count of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP PHASE\n",
      "['hello', 1]\n",
      "['this', 1]\n",
      "['is', 1]\n",
      "['a', 1]\n",
      "['test', 1]\n",
      "['to', 1]\n",
      "['test', 1]\n",
      "['word', 1]\n",
      "['count', 1]\n",
      "['test', 1]\n",
      "['should', 1]\n",
      "['have', 1]\n",
      "['a', 1]\n",
      "['count', 1]\n",
      "['of', 1]\n",
      "['three', 1]\n",
      "\n",
      "REDUCE PHASE\n",
      "['a', 1] (intermediate step)\n",
      "['a', 2] FINAL SUM\n",
      "['count', 1] (intermediate step)\n",
      "['count', 2] FINAL SUM\n",
      "['have', 1] FINAL SUM\n",
      "['hello', 1] FINAL SUM\n",
      "['is', 1] FINAL SUM\n",
      "['of', 1] FINAL SUM\n",
      "['should', 1] FINAL SUM\n",
      "['test', 1] (intermediate step)\n",
      "['test', 2] (intermediate step)\n",
      "['test', 3] FINAL SUM\n",
      "['this', 1] FINAL SUM\n",
      "['three', 1] FINAL SUM\n",
      "['to', 1] FINAL SUM\n",
      "['word', 1] FINAL SUM\n"
     ]
    }
   ],
   "source": [
    "def hw2_0():\n",
    "    doc = \"Hello this is a test to test word count test should have a count of three\".lower()\n",
    "    key_vals = []\n",
    "    \n",
    "    print \"MAP PHASE\"\n",
    "    for word in doc.split():\n",
    "        print [word, 1]\n",
    "        key_vals.append([word, 1])\n",
    "    \n",
    "    print \"\\nREDUCE PHASE\"\n",
    "    key_vals = sorted(key_vals)\n",
    "    \n",
    "    current_word = None\n",
    "    current_count = 0\n",
    "    \n",
    "    for pair in key_vals:\n",
    "        if current_word == pair[0]:\n",
    "            print [current_word, current_count], \"(intermediate step)\"\n",
    "            current_count += pair[1]\n",
    "        else:\n",
    "            if current_word:\n",
    "                print [current_word, current_count], \"FINAL SUM\"\n",
    "            current_word = pair[0]\n",
    "            current_count = pair[1]\n",
    "\n",
    "    print [current_word, current_count], \"FINAL SUM\"\n",
    "    \n",
    "hw2_0()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.1: Sort in Hadoop MapReduce\n",
    "**Given as input: Records of the form `<integer, “NA”>`, where integer is any integer, and “NA” is just the empty string.**<br>\n",
    "**Output: Sorted key value pairs of the form `<integer, “NA”>`; what happens if you have multiple reducers? Do you need additional steps? Explain.**\n",
    "\n",
    "If there are multiple reducers, then a straightforward MapReduce process will yield outputs that are sorted within each reducer, but not sorted across all reducers. In order to output a sort across all reducers, an extra step needs to be implemented that will intelligently send keys to reducers so that the result from all reducers will yield a complete sort. For example, let's say our keys ranged from 0-300. If we had 3 reducers, we could send all keys in the range [0,100) to reducer 1, [100, 200) to reducer 2, and [200, 300] to reducer 3. Thus, the output of each reducer will yield documents that are completely sorted. We would need to balance the keys sent to each reducer to ensure that the load is still balanced between all reducers, which will require some calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write code to generate N  random records of the form `<integer, “NA”>`. Let N = 10,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "with open(\"random.txt\", \"w\") as myfile:\n",
    "    for i in range(10000):\n",
    "        myfile.write(\"{:d},{:s}\\n\".format(random.randint(0, 100000), \"NA\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write the Python Hadoop streaming map-reduce job to perform this sort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: mapper code for HW2.1\n",
    "\n",
    "import sys\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # In this case, we do not need to map the input to anything\n",
    "    print line.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: reducer code for HW2.1\n",
    "\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # In this case, we do not need to reduce anything\n",
    "    print line.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'chmod' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'chmod' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!chmod +x mapper.py\n",
    "!chmod +x reducer.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
