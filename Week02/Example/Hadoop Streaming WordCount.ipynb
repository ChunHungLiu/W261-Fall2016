{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASCI W261: Machine Learning at Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook shows a Hadoop MapReduce job of WordCount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wordcount.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile wordcount.txt\n",
    "hello hi hi hallo\n",
    "bonjour hola hi ciao\n",
    "nihao konnichiwa ola\n",
    "hola nihao hello"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # split the line into words\n",
    "    words = line.split()\n",
    "    # increase counters\n",
    "    for word in words:\n",
    "        # write the results to STDOUT (standard output);\n",
    "        # what we output here will be the input for the\n",
    "        # Reduce step, i.e. the input for reducer.py\n",
    "        #\n",
    "        # tab-delimited; the trivial word count is 1\n",
    "        print '%s\\t%s' % (word, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count = line.split('\\t', 1)\n",
    "\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # write result to STDOUT\n",
    "            print '%s\\t%s' % (current_word, current_count)\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "\n",
    "# do not forget to output the last word if needed!\n",
    "if current_word == word:\n",
    "    print '%s\\t%s' % (current_word, current_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x mapper.py\n",
    "!chmod +x reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bar\t1\r\n",
      "foo\t3\r\n",
      "labs\t1\r\n",
      "quux\t2\r\n"
     ]
    }
   ],
   "source": [
    "!echo \"foo foo quux labs foo bar quux\" | python mapper.py | sort -k1,1 | python reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Run wordcount in hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start yarn and hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: /usr/local/Cellar/hadoop/2.6.0/sbin/start-yarn.sh: No such file or directory\n",
      "/bin/sh: /usr/local/Cellar/hadoop/2.6.0/sbin/start-dfs.sh: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/Cellar/hadoop/2.6.0/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.6.0/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create a folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir -p /user/miki/week02example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### upload wordcount.txt to hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -put wordcount.txt /user/miki/week02example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadoop streaming command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "hadoop jar hadoopstreamingjarfile \\\n",
    "    -D stream.num.map.output.key.fields=n \\\n",
    "    -mapper mapperfile \\\n",
    "    -reducer reducerfile \\\n",
    "    -input inputfile \\\n",
    "    -output outputfile</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hadoop streaming jar file can be found in your hadoop folder or downloaded from\n",
    "http://mvnrepository.com/artifact/org.apache.hadoop/hadoop-streaming/2.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.5.0.jar] /tmp/streamjob5598277463891770878.jar tmpDir=null\n",
      "16/01/21 13:36:51 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/21 13:36:51 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/21 13:36:51 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/21 13:36:51 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/01/21 13:36:52 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1453405632837_0010\n",
      "16/01/21 13:36:52 INFO impl.YarnClientImpl: Submitted application application_1453405632837_0010\n",
      "16/01/21 13:36:52 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1453405632837_0010/\n",
      "16/01/21 13:36:52 INFO mapreduce.Job: Running job: job_1453405632837_0010\n",
      "16/01/21 13:36:59 INFO mapreduce.Job: Job job_1453405632837_0010 running in uber mode : false\n",
      "16/01/21 13:36:59 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/21 13:37:06 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/01/21 13:37:07 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/21 13:37:13 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/21 13:37:13 INFO mapreduce.Job: Job job_1453405632837_0010 completed successfully\n",
      "16/01/21 13:37:13 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=139\n",
      "\t\tFILE: Number of bytes written=341638\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=356\n",
      "\t\tHDFS: Number of bytes written=72\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=10138\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4216\n",
      "\t\tTotal time spent by all map tasks (ms)=10138\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4216\n",
      "\t\tTotal vcore-seconds taken by all map tasks=10138\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=4216\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=10381312\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=4317184\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=4\n",
      "\t\tMap output records=14\n",
      "\t\tMap output bytes=105\n",
      "\t\tMap output materialized bytes=145\n",
      "\t\tInput split bytes=242\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=9\n",
      "\t\tReduce shuffle bytes=145\n",
      "\t\tReduce input records=14\n",
      "\t\tReduce output records=9\n",
      "\t\tSpilled Records=28\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=128\n",
      "\t\tCPU time spent (ms)=1980\n",
      "\t\tPhysical memory (bytes) snapshot=768061440\n",
      "\t\tVirtual memory (bytes) snapshot=4699054080\n",
      "\t\tTotal committed heap usage (bytes)=645398528\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=114\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=72\n",
      "16/01/21 13:37:13 INFO streaming.StreamJob: Output directory: /user/miki/week02example/wordcountOutput\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar hadoop-streaming-2.7.1.jar -mapper /home/cloudera/Documents/W261-Fall2016/Week02/Example/mapper.py -reducer /home/cloudera/Documents/W261-Fall2016/Week02/Example/reducer.py -input /user/miki/week02example/wordcount.txt -output /user/miki/week02example/wordcountOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### show the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bonjour\t1\r\n",
      "ciao\t1\r\n",
      "hallo\t1\r\n",
      "hello\t2\r\n",
      "hi\t3\r\n",
      "hola\t2\r\n",
      "konnichiwa\t1\r\n",
      "nihao\t2\r\n",
      "ola\t1\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/miki/week02example/wordcountOutput/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stop yarn and hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "no proxyserver to stop\n",
      "15/08/21 06:28:48 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "15/08/21 06:29:09 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/Cellar/hadoop/2.6.0/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.6.0/sbin/stop-dfs.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
