{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASCI W261: Machine Learning at Scale\n",
    "## Assignment Week 2\n",
    "Miki Seltzer (miki.seltzer@berkeley.edu)<br>\n",
    "W261-2, Spring 2016<br>\n",
    "Submission: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### HW2.0:\n",
    "\n",
    "#### What is a race condition in the context of parallel computation? Give an example.\n",
    "A race condition is when a section of code is executed by multiple processes, and the order in which the processes execute will impact the final result.\n",
    "\n",
    "![Race condition example](race_conditions.png)\n",
    "Source: https://en.wikipedia.org/wiki/Race_condition#Example\n",
    "\n",
    "#### What is MapReduce?\n",
    "MapReduce can refer to multiple concepts:\n",
    "- **Programming model:** Processes are split into a \"mapping\" phase, and a \"reducing\" phase. In the map phase, a certain function is mapped on to each value in a data set, and then in the reduce phase, the result of the map phase is aggregated. \n",
    "- **Execution framework:** This framework coordinates running processes written with the above model in mind.\n",
    "- **Software implementation:** MapReduce is the name of Google's proprietary implementation of this programming model, while Apache Hadoop is the open-source implementation.\n",
    "\n",
    "#### How does it differ from Hadoop?\n",
    "Hadoop is the open-source implementation of Google's MapReduce. Hadoop consists of two parts: distributed storage of data, and distributed processing of data. HDFS is the storage part, and MapReduce is the processing part.\n",
    "\n",
    "#### Which programming paradigm is Hadoop based on? Explain and give a simple example in code and show the code running.\n",
    "Hadoop is based on the MapReduce paradigm. The classic example of the MapReduce programming paradigm is word count. In the map phase of word count, each word in a document is assigned a count of 1. In the reduce phase, the counts for each unique word are summed to yield the final count of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP PHASE\n",
      "['hello', 1]\n",
      "['this', 1]\n",
      "['is', 1]\n",
      "['a', 1]\n",
      "['test', 1]\n",
      "['to', 1]\n",
      "['test', 1]\n",
      "['word', 1]\n",
      "['count', 1]\n",
      "['test', 1]\n",
      "['should', 1]\n",
      "['have', 1]\n",
      "['a', 1]\n",
      "['count', 1]\n",
      "['of', 1]\n",
      "['three', 1]\n",
      "\n",
      "REDUCE PHASE\n",
      "['a', 1] (intermediate step)\n",
      "['a', 2] FINAL SUM\n",
      "['count', 1] (intermediate step)\n",
      "['count', 2] FINAL SUM\n",
      "['have', 1] FINAL SUM\n",
      "['hello', 1] FINAL SUM\n",
      "['is', 1] FINAL SUM\n",
      "['of', 1] FINAL SUM\n",
      "['should', 1] FINAL SUM\n",
      "['test', 1] (intermediate step)\n",
      "['test', 2] (intermediate step)\n",
      "['test', 3] FINAL SUM\n",
      "['this', 1] FINAL SUM\n",
      "['three', 1] FINAL SUM\n",
      "['to', 1] FINAL SUM\n",
      "['word', 1] FINAL SUM\n"
     ]
    }
   ],
   "source": [
    "def hw2_0():\n",
    "    doc = \"Hello this is a test to test word count test should have a count of three\".lower()\n",
    "    key_vals = []\n",
    "    \n",
    "    print \"MAP PHASE\"\n",
    "    for word in doc.split():\n",
    "        print [word, 1]\n",
    "        key_vals.append([word, 1])\n",
    "    \n",
    "    print \"\\nREDUCE PHASE\"\n",
    "    key_vals = sorted(key_vals)\n",
    "    \n",
    "    current_word = None\n",
    "    current_count = 0\n",
    "    \n",
    "    for pair in key_vals:\n",
    "        if current_word == pair[0]:\n",
    "            print [current_word, current_count], \"(intermediate step)\"\n",
    "            current_count += pair[1]\n",
    "        else:\n",
    "            if current_word:\n",
    "                print [current_word, current_count], \"FINAL SUM\"\n",
    "            current_word = pair[0]\n",
    "            current_count = pair[1]\n",
    "\n",
    "    print [current_word, current_count], \"FINAL SUM\"\n",
    "    \n",
    "hw2_0()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.1: Sort in Hadoop MapReduce\n",
    "**Given as input: Records of the form `<integer, “NA”>`, where integer is any integer, and “NA” is just the empty string.**<br>\n",
    "**Output: Sorted key value pairs of the form `<integer, “NA”>`; what happens if you have multiple reducers? Do you need additional steps? Explain.**\n",
    "\n",
    "If there are multiple reducers, then a straightforward MapReduce process will yield outputs that are sorted within each reducer, but not sorted across all reducers. In order to output a sort across all reducers, an extra step needs to be implemented that will intelligently send keys to reducers so that the result from all reducers will yield a complete sort. For example, let's say our keys ranged from 0-300. If we had 3 reducers, we could send all keys in the range [0,100) to reducer 1, [100, 200) to reducer 2, and [200, 300] to reducer 3. Thus, the output of each reducer will yield documents that are completely sorted. We would need to balance the keys sent to each reducer to ensure that the load is still balanced between all reducers, which will require some calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write code to generate N  random records of the form `<integer, “NA”>`. Let N = 10,000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to need the Hadoop Streaming jar file, so let's download it here so that we know which one to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-01-21 18:37:45--  http://central.maven.org/maven2/org/apache/hadoop/hadoop-streaming/2.7.1/hadoop-streaming-2.7.1.jar\n",
      "Resolving central.maven.org... 23.235.47.209\n",
      "Connecting to central.maven.org|23.235.47.209|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 105736 (103K) [application/java-archive]\n",
      "Saving to: “hadoop-streaming-2.7.1.jar”\n",
      "\n",
      "100%[======================================>] 105,736     --.-K/s   in 0.1s    \n",
      "\n",
      "2016-01-21 18:38:00 (1.06 MB/s) - “hadoop-streaming-2.7.1.jar” saved [105736/105736]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://central.maven.org/maven2/org/apache/hadoop/hadoop-streaming/2.7.1/hadoop-streaming-2.7.1.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "with open(\"random.txt\", \"w\") as myfile:\n",
    "    for i in range(10000):\n",
    "        myfile.write(\"{:d},{:s}\\n\".format(random.randint(0, 100000), \"NA\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write the Python Hadoop streaming map-reduce job to perform this sort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: mapper code for HW2.1\n",
    "\n",
    "import sys\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # In this case, we do not need to map the input to anything\n",
    "    key, value = line.strip().split(',')\n",
    "    print \"%010d\\t%s\" % (int(key), value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: reducer code for HW2.1\n",
    "\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # In this case, we do not need to reduce anything\n",
    "    key, value = line.strip().split('\\t')\n",
    "    print \"%d\\t%s\" % (int(key), value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make HDFS directory and put random.txt there\n",
    "!hdfs dfs -mkdir /user/miki/week02\n",
    "!hdfs dfs -put random.txt /user/miki/week02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/22 12:59:13 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/miki/week02/hw2_1_output\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.5.0.jar] /tmp/streamjob4129142604574616452.jar tmpDir=null\n",
      "16/01/22 12:59:16 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/22 12:59:17 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/22 12:59:17 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/22 12:59:17 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/01/22 12:59:17 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/01/22 12:59:17 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1453405632837_0022\n",
      "16/01/22 12:59:18 INFO impl.YarnClientImpl: Submitted application application_1453405632837_0022\n",
      "16/01/22 12:59:18 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1453405632837_0022/\n",
      "16/01/22 12:59:18 INFO mapreduce.Job: Running job: job_1453405632837_0022\n",
      "16/01/22 12:59:25 INFO mapreduce.Job: Job job_1453405632837_0022 running in uber mode : false\n",
      "16/01/22 12:59:25 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/22 12:59:32 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/01/22 12:59:33 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/22 12:59:44 INFO mapreduce.Job:  map 100% reduce 25%\n",
      "16/01/22 12:59:46 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "16/01/22 12:59:48 INFO mapreduce.Job:  map 100% reduce 75%\n",
      "16/01/22 12:59:49 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/22 12:59:49 INFO mapreduce.Job: Job job_1453405632837_0022 completed successfully\n",
      "16/01/22 12:59:49 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=160024\n",
      "\t\tFILE: Number of bytes written=1002568\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=93193\n",
      "\t\tHDFS: Number of bytes written=88875\n",
      "\t\tHDFS: Number of read operations=18\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=8\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=4\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=11193\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=43568\n",
      "\t\tTotal time spent by all map tasks (ms)=11193\n",
      "\t\tTotal time spent by all reduce tasks (ms)=43568\n",
      "\t\tTotal vcore-seconds taken by all map tasks=11193\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=43568\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=11461632\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=44613632\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=10000\n",
      "\t\tMap output records=10000\n",
      "\t\tMap output bytes=140000\n",
      "\t\tMap output materialized bytes=160048\n",
      "\t\tInput split bytes=222\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=9533\n",
      "\t\tReduce shuffle bytes=160048\n",
      "\t\tReduce input records=10000\n",
      "\t\tReduce output records=10000\n",
      "\t\tSpilled Records=20000\n",
      "\t\tShuffled Maps =8\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=8\n",
      "\t\tGC time elapsed (ms)=441\n",
      "\t\tCPU time spent (ms)=7330\n",
      "\t\tPhysical memory (bytes) snapshot=1214963712\n",
      "\t\tVirtual memory (bytes) snapshot=9382010880\n",
      "\t\tTotal committed heap usage (bytes)=1192230912\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=92971\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=88875\n",
      "16/01/22 12:59:49 INFO streaming.StreamJob: Output directory: /user/miki/week02/hw2_1_output\n"
     ]
    }
   ],
   "source": [
    "# Change permissions on mapper and reducer\n",
    "!chmod +x mapper.py\n",
    "!chmod +x reducer.py\n",
    "\n",
    "# If output folder already exists, delete it\n",
    "!hdfs dfs -rm -r /user/miki/week02/hw2_1_output\n",
    "\n",
    "# Run job\n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.reduce.tasks=4 \\\n",
    "-mapper /home/cloudera/Documents/W261-Fall2016/Week02/mapper.py \\\n",
    "-reducer /home/cloudera/Documents/W261-Fall2016/Week02/reducer.py \\\n",
    "-input /user/miki/week02/random.txt \\\n",
    "-output /user/miki/week02/hw2_1_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -cat /user/miki/week02/hw2_1_output/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.2: Using the Enron data from HW1 and Hadoop MapReduce streaming, write mapper/reducer pair that  will determine the number of occurrences of a single, user-specified word. \n",
    "\n",
    "Examine the word “assistance” and report your results. To do so, make sure that mapper.py counts all occurrences of a single word, and reducer.py collates the counts of the single word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load enronemail_1h.txt into HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -put enronemail_1h.txt /user/miki/week02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: mapper code for HW2.2\n",
    "\n",
    "import sys\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Keyword from user input\n",
    "keyword = sys.argv[1]\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    \n",
    "    # Strip white space from line, then split into fields\n",
    "    # Replace commas with spaces (we are using commas as a delimiter as well)\n",
    "    # Remove remaining punctuation from subject and body\n",
    "    # Concatenate, then split subject and body by spaces\n",
    "    # Some records are malformed -- if there is a 4th field, use it\n",
    "    fields = line.strip().split('\\t')\n",
    "    subj = fields[2].replace(',', ' ')\n",
    "    subj = subj.translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "    if len(fields) == 4:\n",
    "        body = fields[3].replace(',', ' ')\n",
    "        body = body.translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "    else:\n",
    "        body = \"\"\n",
    "    words = subj + \" \" + body\n",
    "    words = words.split()\n",
    "    \n",
    "    # Loop through words\n",
    "    # If word is keyword, write to file\n",
    "    # key = word\n",
    "    # value = 1\n",
    "    for word in words:\n",
    "        if keyword == word:\n",
    "            print \"%s\\t%s\" % (word, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: reducer code for HW2.2\n",
    "\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "# Initialize some variables\n",
    "# We know that the words will be sorted\n",
    "# We need to keep track of state\n",
    "prev_word = None\n",
    "prev_count = 0\n",
    "word = None\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # Strip and split line from mapper\n",
    "    word, count = line.strip().split('\\t', 1)\n",
    "    \n",
    "    # If possible, turn count into an int (it's read as a string)\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # We couldn't make count into an int, so move on\n",
    "        continue\n",
    "        \n",
    "    # Since the words will be sorted, all counts for a word will be grouped\n",
    "    if prev_word == word:\n",
    "        # If prev_word is word, then we haven't changed words\n",
    "        # Just update prev_count\n",
    "        prev_count += count\n",
    "    else:\n",
    "        # We've encountered a new word!\n",
    "        # This might be the first word, though\n",
    "        if prev_word:\n",
    "            # We need to print the last word we were on\n",
    "            print \"%s\\t%s\" % (prev_word, prev_count)\n",
    "        \n",
    "        # Now we need to initialize our variables for the new word and count\n",
    "        prev_word = word\n",
    "        prev_count = count\n",
    "\n",
    "# We have reached the end of the file, so print the last word and count\n",
    "if prev_word == word:\n",
    "    print \"%s\\t%s\" % (prev_word, prev_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Hadoop streaming command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/22 13:59:07 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/miki/week02/hw2_2_output\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.5.0.jar] /tmp/streamjob2515332195244567054.jar tmpDir=null\n",
      "16/01/22 13:59:10 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/22 13:59:10 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/22 13:59:10 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/22 13:59:10 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/01/22 13:59:11 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1453405632837_0027\n",
      "16/01/22 13:59:11 INFO impl.YarnClientImpl: Submitted application application_1453405632837_0027\n",
      "16/01/22 13:59:11 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1453405632837_0027/\n",
      "16/01/22 13:59:11 INFO mapreduce.Job: Running job: job_1453405632837_0027\n",
      "16/01/22 13:59:18 INFO mapreduce.Job: Job job_1453405632837_0027 running in uber mode : false\n",
      "16/01/22 13:59:18 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/22 13:59:25 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/01/22 13:59:26 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/22 13:59:32 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/22 13:59:32 INFO mapreduce.Job: Job job_1453405632837_0027 completed successfully\n",
      "16/01/22 13:59:32 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=156\n",
      "\t\tFILE: Number of bytes written=341606\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=216503\n",
      "\t\tHDFS: Number of bytes written=14\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=10277\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4092\n",
      "\t\tTotal time spent by all map tasks (ms)=10277\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4092\n",
      "\t\tTotal vcore-seconds taken by all map tasks=10277\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=4092\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=10523648\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=4190208\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=10\n",
      "\t\tMap output bytes=130\n",
      "\t\tMap output materialized bytes=162\n",
      "\t\tInput split bytes=236\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce shuffle bytes=162\n",
      "\t\tReduce input records=10\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=20\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=135\n",
      "\t\tCPU time spent (ms)=2100\n",
      "\t\tPhysical memory (bytes) snapshot=723128320\n",
      "\t\tVirtual memory (bytes) snapshot=4702228480\n",
      "\t\tTotal committed heap usage (bytes)=693633024\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=216267\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=14\n",
      "16/01/22 13:59:32 INFO streaming.StreamJob: Output directory: /user/miki/week02/hw2_2_output\n"
     ]
    }
   ],
   "source": [
    "# Change permissions on mapper and reducer\n",
    "!chmod +x mapper.py\n",
    "!chmod +x reducer.py\n",
    "\n",
    "# Delete output folder if it exists\n",
    "!hdfs dfs -rm -r /user/miki/week02/hw2_2_output\n",
    "\n",
    "# Run job\n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-mapper '/home/cloudera/Documents/W261-Fall2016/Week02/mapper.py assistance' \\\n",
    "-reducer /home/cloudera/Documents/W261-Fall2016/Week02/reducer.py \\\n",
    "-input /user/miki/week02/enronemail_1h.txt \\\n",
    "-output /user/miki/week02/hw2_2_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at output of job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance\t10\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/miki/week02/hw2_2_output/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.2.1: Using Hadoop MapReduce and your wordcount job (from HW2.2) determine the top-10 occurring tokens (most frequent tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################\n",
    "#\n",
    "#    DO THIS!!!!!!!\n",
    "#\n",
    "##############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.3: Multinomial NAIVE BAYES with NO Smoothing\n",
    "Using the Enron data from HW1 and Hadoop MapReduce, write  a mapper/reducer job(s) that will both learn Naive Bayes classifier and classify the Enron email messages using the learnt Naive Bayes classifier. Use all white-space delimited tokens as independent input variables (assume spaces, fullstops, commas as delimiters). \n",
    "\n",
    "Note: for multinomial Naive Bayes, the Pr(X=“assistance”|Y=SPAM) is calculated as follows:\n",
    "\n",
    "$$\n",
    "\\frac{\\text{number of times \"assistance\" occurs in SPAM labeled documents}}{\\text{the number of words in documents labeled SPAM}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapper/Reducer for fitting NB\n",
    "\n",
    "Mapper:\n",
    "- Input: training documents\n",
    "- Output: (word, 1, 0) if word was in spam, otherwise (word, 0, 1)\n",
    "  - Special words: \\*alldocs, \\*docs and \\*words\n",
    "  \n",
    "Reducer:\n",
    "- Input: (word, 1, 0) or (word, 0, 1)\n",
    "- Output: (word, spam count, ham count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: mapper code for HW2.3\n",
    "\n",
    "import sys\n",
    "import string\n",
    "import re\n",
    "\n",
    "counts = {\n",
    "    '*words':{\n",
    "        '1':0,\n",
    "        '0':0\n",
    "    },\n",
    "    '*docs':{\n",
    "        '1':0,\n",
    "        '0':0\n",
    "    }\n",
    "}\n",
    "total_docs = 0\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    \n",
    "    # Strip white space from line, then split into fields\n",
    "    # Replace commas with spaces (we are using commas as a delimiter as well)\n",
    "    # Remove remaining punctuation from subject and body\n",
    "    # Concatenate, then split subject and body by spaces\n",
    "    # Some records are malformed -- if there is a 4th field, use it\n",
    "    fields = line.strip().split('\\t')\n",
    "    \n",
    "    # Keep track of document counts\n",
    "    spam = fields[1]\n",
    "    counts['*docs'][spam] += 1\n",
    "    total_docs += 1\n",
    "    \n",
    "    subj = fields[2].replace(',', ' ')\n",
    "    subj = subj.translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "    if len(fields) == 4:\n",
    "        body = fields[3].replace(',', ' ')\n",
    "        body = body.translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "    else:\n",
    "        body = \"\"\n",
    "    words = subj + \" \" + body\n",
    "    words = words.split()\n",
    "    \n",
    "    # Loop through words\n",
    "    # If word is not trivial, write to file\n",
    "    # key = word\n",
    "    # value = 1\n",
    "    for word in words:\n",
    "        if len(word) > 0 and repr(word)[1] != '\\\\':\n",
    "            if spam == '1':\n",
    "                print \"%s\\t%s\\t%s\" % (word, 1, 0)\n",
    "            elif spam == '0':\n",
    "                print \"%s\\t%s\\t%s\" % (word, 0, 1)\n",
    "            counts['*words'][spam] += 1\n",
    "\n",
    "# At the end, output document and word counts\n",
    "for item in counts:\n",
    "    print \"%s\\t%s\\t%s\" % (item, counts[item]['1'], counts[item]['0'])\n",
    "print \"%s\\t%s\\t%s\" % ('*alldocs', total_docs, total_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: reducer code for HW2.3\n",
    "\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "import math\n",
    "\n",
    "# Initialize some variables\n",
    "# We know that the words will be sorted\n",
    "# We need to keep track of state\n",
    "prev_word = None\n",
    "prev_spam_count = 0\n",
    "prev_ham_count = 0\n",
    "word = None\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # Split line into fields\n",
    "    fields = line.strip().split('\\t')\n",
    "    word = fields[0]\n",
    "    spam_count = fields[1]\n",
    "    ham_count = fields[2]\n",
    "    \n",
    "    # If possible, turn count into an int (it's read as a string)\n",
    "    try:\n",
    "        spam_count = int(spam_count)\n",
    "        ham_count = int(ham_count)\n",
    "    except ValueError:\n",
    "        # We couldn't make count into an int, so move on\n",
    "        continue\n",
    "        \n",
    "    if prev_word == word:\n",
    "        # We have not moved to a new word\n",
    "        # Just update the count of this word\n",
    "        prev_spam_count += spam_count\n",
    "        prev_ham_count += ham_count\n",
    "        \n",
    "    else:\n",
    "        # We have encountered a new word!\n",
    "        # If this is the first word, we don't need to print anything\n",
    "        if prev_word: \n",
    "            # Write the previous word to file\n",
    "            print '%s\\t%s\\t%s' % (prev_word, prev_spam_count, prev_ham_count)\n",
    "            \n",
    "        # Now we need to initialize our variables\n",
    "        prev_word = word\n",
    "        prev_spam_count = spam_count\n",
    "        prev_ham_count = ham_count\n",
    "\n",
    "# We've reached the end of the file\n",
    "# Print the last word and counts\n",
    "print '%s\\t%s\\t%s' % (prev_word, prev_spam_count, prev_ham_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 02:24:29 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/miki/week02/hw2_3_output_fit\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.5.0.jar] /tmp/streamjob1294460146102016869.jar tmpDir=null\n",
      "16/01/23 02:24:33 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/23 02:24:33 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/23 02:24:33 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/23 02:24:33 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/01/23 02:24:34 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1453405632837_0143\n",
      "16/01/23 02:24:34 INFO impl.YarnClientImpl: Submitted application application_1453405632837_0143\n",
      "16/01/23 02:24:34 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1453405632837_0143/\n",
      "16/01/23 02:24:34 INFO mapreduce.Job: Running job: job_1453405632837_0143\n",
      "16/01/23 02:24:41 INFO mapreduce.Job: Job job_1453405632837_0143 running in uber mode : false\n",
      "16/01/23 02:24:41 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/23 02:24:49 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/01/23 02:24:50 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/23 02:24:57 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/23 02:24:57 INFO mapreduce.Job: Job job_1453405632837_0143 completed successfully\n",
      "16/01/23 02:24:57 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=373606\n",
      "\t\tFILE: Number of bytes written=1088485\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=216503\n",
      "\t\tHDFS: Number of bytes written=68226\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=11715\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4813\n",
      "\t\tTotal time spent by all map tasks (ms)=11715\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4813\n",
      "\t\tTotal vcore-seconds taken by all map tasks=11715\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=4813\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=11996160\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=4928512\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=31567\n",
      "\t\tMap output bytes=310466\n",
      "\t\tMap output materialized bytes=373612\n",
      "\t\tInput split bytes=236\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5710\n",
      "\t\tReduce shuffle bytes=373612\n",
      "\t\tReduce input records=31567\n",
      "\t\tReduce output records=5710\n",
      "\t\tSpilled Records=63134\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=149\n",
      "\t\tCPU time spent (ms)=3890\n",
      "\t\tPhysical memory (bytes) snapshot=737927168\n",
      "\t\tVirtual memory (bytes) snapshot=4685791232\n",
      "\t\tTotal committed heap usage (bytes)=693633024\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=216267\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=68226\n",
      "16/01/23 02:24:57 INFO streaming.StreamJob: Output directory: /user/miki/week02/hw2_3_output_fit\n"
     ]
    }
   ],
   "source": [
    "# Change permissions on mapper and reducer\n",
    "!chmod +x mapper.py\n",
    "!chmod +x reducer.py\n",
    "\n",
    "# Delete output folder if it exists\n",
    "!hdfs dfs -rm -r /user/miki/week02/hw2_3_output_fit\n",
    "\n",
    "# Run job\n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-input /user/miki/week02/enronemail_1h.txt \\\n",
    "-output /user/miki/week02/hw2_3_output_fit \\\n",
    "-mapper /home/cloudera/Documents/W261-Fall2016/Week02/mapper.py \\\n",
    "-reducer /home/cloudera/Documents/W261-Fall2016/Week02/reducer.py \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!hdfs dfs -cat /user/miki/week02/hw2_3_output_fit/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapper/Reducer #1 for predicting NB\n",
    "\n",
    "Mapper:\n",
    "- Input from fit: (word, spam count, ham count)\n",
    "- Output: (word, spam count, ham count)\n",
    "- Input testing document: (document ID, cat, subj, body)\n",
    "- Output: (word, cat, document ID)\n",
    "\n",
    "Reducer:\n",
    "- Input: (word, spam count, ham count)\n",
    "- Input: (word, cat, document ID)\n",
    "- Output: (document ID, cat, word, spam count, ham count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: mapper code for HW2.3\n",
    "\n",
    "import sys\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    \n",
    "    # Strip white space from line, then split into fields    \n",
    "    fields = line.strip().split('\\t')\n",
    "    \n",
    "    # If first field matches pattern of document ID, tokenize words\n",
    "    pattern = re.compile('^\\d{4}.\\d{4}-\\d{2}-\\d{2}\\.\\w+')\n",
    "    if pattern.match(fields[0]):\n",
    "        # Keep track of document counts\n",
    "        doc_id = fields[0]\n",
    "        spam = fields[1]\n",
    "\n",
    "        # We are always going to need the doc/word counts for each document\n",
    "        print '%s^%s^%s' % ('*alldocs', spam, doc_id)\n",
    "        print '%s^%s^%s' % ('*docs', spam, doc_id)\n",
    "        print '%s^%s^%s' % ('*words', spam, doc_id)\n",
    "        \n",
    "        # Replace commas with spaces (we are using commas as a delimiter as well)\n",
    "        # Remove remaining punctuation from subject and body\n",
    "        # Concatenate, then split subject and body by spaces\n",
    "        # Some records are malformed -- if there is a 4th field, use it\n",
    "        subj = fields[2].replace(',', ' ')\n",
    "        subj = subj.translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "        if len(fields) == 4:\n",
    "            body = fields[3].replace(',', ' ')\n",
    "            body = body.translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "        else:\n",
    "            body = \"\"\n",
    "        words = subj + \" \" + body\n",
    "        words = words.split()\n",
    "\n",
    "        # Loop through words\n",
    "        # If word is not trivial, write to file\n",
    "        # key = word\n",
    "        # value = doc_id\n",
    "        for word in words:\n",
    "            if len(word) > 0 and repr(word)[1] != '\\\\':\n",
    "                print '%s^%s^%s' % (word, spam, doc_id)\n",
    "    else:\n",
    "        # Now we know that the record is the\n",
    "        # output of the previous MapReduce job\n",
    "        word = fields[0]\n",
    "        spam_count = fields[1]\n",
    "        ham_count = fields[2]\n",
    "        print '%s^%s^%s' % (word, '*' + spam_count, ham_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: reducer code for HW2.3\n",
    "\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "import math\n",
    "import re\n",
    "\n",
    "# Initialize some variables\n",
    "spam_count = 0\n",
    "ham_count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # Strip and split line\n",
    "    key, value = line.strip().split('\\t')\n",
    "    word, field1 = key.split('^')\n",
    "\n",
    "    # If field1 starts with a *, we know that it is the spam and ham counts\n",
    "    if field1[0] == '*':\n",
    "        # This record will be of the form (word field1=*spam_count value=ham_count)\n",
    "        try:\n",
    "            spam_count = int(field1.replace('*',''))\n",
    "            ham_count = int(value)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    \n",
    "    else:\n",
    "        # Now we know that it is a doc_id record\n",
    "        # This record will be of the form (word field1=cat value=doc_id)\n",
    "        pattern = re.compile('^\\d{4}.\\d{4}-\\d{2}-\\d{2}\\.\\w+')\n",
    "        if pattern.match(value):\n",
    "            doc_id = value\n",
    "        print '%s\\t%s\\t%s\\t%s\\t%s' % (doc_id, field1, word, spam_count, ham_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 09:16:47 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/miki/week02/hw2_3_output_predict1\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.5.0.jar] /tmp/streamjob8552654983851536121.jar tmpDir=null\n",
      "16/01/23 09:16:50 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/23 09:16:50 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/23 09:16:51 INFO mapred.FileInputFormat: Total input paths to process : 2\n",
      "16/01/23 09:16:51 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "16/01/23 09:16:51 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1453405632837_0175\n",
      "16/01/23 09:16:51 INFO impl.YarnClientImpl: Submitted application application_1453405632837_0175\n",
      "16/01/23 09:16:51 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1453405632837_0175/\n",
      "16/01/23 09:16:51 INFO mapreduce.Job: Running job: job_1453405632837_0175\n",
      "16/01/23 09:16:58 INFO mapreduce.Job: Job job_1453405632837_0175 running in uber mode : false\n",
      "16/01/23 09:16:58 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/23 09:17:09 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/01/23 09:17:11 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/23 09:17:16 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/23 09:17:16 INFO mapreduce.Job: Job job_1453405632837_0175 completed successfully\n",
      "16/01/23 09:17:16 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1153594\n",
      "\t\tFILE: Number of bytes written=2764889\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=293049\n",
      "\t\tHDFS: Number of bytes written=1180670\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=3\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=21841\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4621\n",
      "\t\tTotal time spent by all map tasks (ms)=21841\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4621\n",
      "\t\tTotal vcore-seconds taken by all map tasks=21841\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=4621\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=22365184\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=4731904\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=5810\n",
      "\t\tMap output records=37571\n",
      "\t\tMap output bytes=1078446\n",
      "\t\tMap output materialized bytes=1153606\n",
      "\t\tInput split bytes=364\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=12441\n",
      "\t\tReduce shuffle bytes=1153606\n",
      "\t\tReduce input records=37571\n",
      "\t\tReduce output records=31861\n",
      "\t\tSpilled Records=75142\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=186\n",
      "\t\tCPU time spent (ms)=4870\n",
      "\t\tPhysical memory (bytes) snapshot=991035392\n",
      "\t\tVirtual memory (bytes) snapshot=6254219264\n",
      "\t\tTotal committed heap usage (bytes)=828375040\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=292685\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1180670\n",
      "16/01/23 09:17:16 INFO streaming.StreamJob: Output directory: /user/miki/week02/hw2_3_output_predict1\n"
     ]
    }
   ],
   "source": [
    "# Change permissions on mapper and reducer\n",
    "!chmod +x mapper.py\n",
    "!chmod +x reducer.py\n",
    "\n",
    "# Delete output folder if it exists\n",
    "!hdfs dfs -rm -r /user/miki/week02/hw2_3_output_predict1\n",
    "\n",
    "# Run job\n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-D stream.map.output.field.separator=^ \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D mapreduce.map.output.key.field.separator=^ \\\n",
    "-D mapreduce.partition.keypartitioner.options=-k1,1 \\\n",
    "-input /user/miki/week02/enronemail_1h.txt \\\n",
    "-input /user/miki/week02/hw2_3_output_fit \\\n",
    "-output /user/miki/week02/hw2_3_output_predict1 \\\n",
    "-mapper /home/cloudera/Documents/W261-Fall2016/Week02/mapper.py \\\n",
    "-reducer /home/cloudera/Documents/W261-Fall2016/Week02/reducer.py \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!hdfs dfs -cat /user/miki/week02/hw2_3_output_predict1/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapper/Reducer #2 for predicting NB\n",
    "\n",
    "Mapper:\n",
    "- Input from predict1: (doc_id, cat, word, spam count, ham count)\n",
    "- Output: identity\n",
    "\n",
    "Reducer:\n",
    "- Input: (doc_id, cat, word, spam count, ham count)\n",
    "- Output: (doc_id, cat, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: mapper code for HW2.3\n",
    "\n",
    "import sys\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    \n",
    "    # Replace delimiter\n",
    "    line = line.replace('\\n', '')\n",
    "    fields = line.split('\\t')\n",
    "    print '%s^%s^%s^%s^%s' % (fields[0], fields[1], fields[2], fields[3], fields[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: reducer code for HW2.3\n",
    "\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "import math\n",
    "import re\n",
    "\n",
    "# Initialize some variables\n",
    "prev_doc = None\n",
    "prev_spam = None\n",
    "prev_spam_count = 0\n",
    "prev_ham_count = 0\n",
    "prev_word = None\n",
    "\n",
    "docs_total = 0\n",
    "docs = {'1':0, '0':0}\n",
    "words_total = 0\n",
    "words = {'1':0, '0':0}\n",
    "log_prior = {'1':0, '0':0}\n",
    "log_posterior = {'1':0, '0':0}\n",
    "log_likelihood = {'1':0, '0':0}\n",
    "log_evidence = 0\n",
    "\n",
    "##### MAKE NUM_ERRORS A DICTIONARY\n",
    "# Need to keep track of errors for each class\n",
    "num_errors = 0.0\n",
    "num_total = 0.0\n",
    "num_correct = 0.0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # Strip and split line\n",
    "    # Assign variables\n",
    "    line = line.replace('\\n', '')\n",
    "    key, value = line.strip().split('\\t')\n",
    "    doc, spam, word = key.split('^')\n",
    "    spam_count, ham_count = value.split('^')\n",
    "    \n",
    "    try:\n",
    "        spam_count = float(spam_count)\n",
    "        ham_count = float(ham_count)\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "    # Let's calculate some log_probs!\n",
    "    if prev_doc == doc:\n",
    "        # We haven't changed documents\n",
    "        if prev_word == word:\n",
    "            # We haven't changed words, so just increment\n",
    "            prev_spam_count += spam_count\n",
    "            prev_ham_count += ham_count\n",
    "            #print \"update counts\"\n",
    "            \n",
    "        else:\n",
    "            # We are at a new word\n",
    "            # We need to check if we are at a keyword\n",
    "            if prev_word == '*alldocs': \n",
    "                # We are at a record where we need to output total docs\n",
    "                docs_total = prev_spam_count\n",
    "                #print \"total docs:\", docs_total\n",
    "            \n",
    "            elif prev_word == '*docs': \n",
    "                # We are at a record where we need to output unique docs per class\n",
    "                docs['1'] = prev_spam_count\n",
    "                docs['0'] = prev_ham_count\n",
    "                for item in log_prior:\n",
    "                    log_prior[item] = math.log(docs[item] / docs_total)\n",
    "                    \n",
    "                    # We will update the posterior after each word\n",
    "                    # Initialize it to the prior\n",
    "                    log_posterior[item] = math.log(docs[item] / docs_total)\n",
    "                #print \"log prior:\", log_prior\n",
    "\n",
    "            elif prev_word == '*words':\n",
    "                # We are at a record where we need to output words per class\n",
    "                words['1'] = prev_spam_count\n",
    "                words['0'] = prev_ham_count\n",
    "                words_total = prev_spam_count + prev_ham_count\n",
    "                #print \"word count:\", words\n",
    "\n",
    "            elif prev_word:\n",
    "                # We are at a new normal word, and need to calculate stuff\n",
    "                try:\n",
    "                    log_likelihood['1'] = math.log(prev_spam_count / words['1'])\n",
    "                    #print word, \"log likelihood spam for\", word, log_likelihood['1']\n",
    "                    \n",
    "                except:\n",
    "                    #print \"error calculating log likelihood spam for\", word\n",
    "                    num_errors += 1\n",
    "                    \n",
    "                try:\n",
    "                    log_likelihood['0'] = math.log(prev_ham_count / words['0'])\n",
    "                    #print word, \"log likelihood ham for\", word, log_likelihood['0']\n",
    "                    \n",
    "                except:\n",
    "                    #print \"error calculating log likelihood ham for\", word\n",
    "                    num_errors += 1\n",
    "                    \n",
    "                # Calculate evidence\n",
    "                log_evidence = math.log((prev_spam_count + prev_ham_count)/words_total)\n",
    "                for item in log_posterior:\n",
    "                    log_posterior[item] += log_likelihood[item] - log_evidence\n",
    "                #print \"updated log posterior:\", log_posterior\n",
    "                \n",
    "                \n",
    "            prev_word = word\n",
    "            prev_spam_count = spam_count\n",
    "            prev_ham_count = ham_count\n",
    "            \n",
    "    else:\n",
    "        # We are done with one document, and need to output our predictions\n",
    "        if prev_doc:\n",
    "            # We know that we're not on the very first record\n",
    "            if log_posterior['1'] > log_posterior['0']: prediction = '1'\n",
    "            else: prediction = '0'\n",
    "            num_total += 1\n",
    "            if prev_spam == prediction: num_correct += 1\n",
    "            print '%s\\t%s\\t%s\\t%s\\t%s' % (prev_doc, prev_spam, prediction, \n",
    "                                          log_posterior['1'],\n",
    "                                          log_posterior['0'])\n",
    "        \n",
    "        prev_doc = doc\n",
    "        prev_spam = spam\n",
    "        prev_word = word\n",
    "        prev_spam_count = spam_count\n",
    "        prev_ham_count = ham_count\n",
    "\n",
    "# Output our final prediction\n",
    "if log_posterior['1'] > log_posterior['0']: prediction = '1'\n",
    "else: prediction = '0'\n",
    "num_total += 1\n",
    "if prev_spam == prediction: num_correct += 1\n",
    "print '%s\\t%s\\t%s\\t%s\\t%s' % (prev_doc, prev_spam, prediction, \n",
    "                              log_posterior['1'],\n",
    "                              log_posterior['0'])\n",
    "print '\\n'\n",
    "print \"Number of documents:\", num_total\n",
    "print \"Number correct predictions:\", num_correct\n",
    "print \"Error rate:\", 1 - num_correct / num_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 12:00:16 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/miki/week02/hw2_3_output_predict2\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.5.0.jar] /tmp/streamjob2504384993510058722.jar tmpDir=null\n",
      "16/01/23 12:00:20 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/23 12:00:20 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/23 12:00:21 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/23 12:00:21 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/01/23 12:00:21 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1453405632837_0201\n",
      "16/01/23 12:00:22 INFO impl.YarnClientImpl: Submitted application application_1453405632837_0201\n",
      "16/01/23 12:00:22 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1453405632837_0201/\n",
      "16/01/23 12:00:22 INFO mapreduce.Job: Running job: job_1453405632837_0201\n",
      "16/01/23 12:00:28 INFO mapreduce.Job: Job job_1453405632837_0201 running in uber mode : false\n",
      "16/01/23 12:00:28 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/23 12:00:36 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/01/23 12:00:37 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/23 12:00:43 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/23 12:00:44 INFO mapreduce.Job: Job job_1453405632837_0201 completed successfully\n",
      "16/01/23 12:00:44 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1244398\n",
      "\t\tFILE: Number of bytes written=2831887\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1185032\n",
      "\t\tHDFS: Number of bytes written=5541\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=11673\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4516\n",
      "\t\tTotal time spent by all map tasks (ms)=11673\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4516\n",
      "\t\tTotal vcore-seconds taken by all map tasks=11673\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=4516\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=11953152\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=4624384\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31861\n",
      "\t\tMap output records=31861\n",
      "\t\tMap output bytes=1180670\n",
      "\t\tMap output materialized bytes=1244404\n",
      "\t\tInput split bytes=266\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=15638\n",
      "\t\tReduce shuffle bytes=1244404\n",
      "\t\tReduce input records=31861\n",
      "\t\tReduce output records=105\n",
      "\t\tSpilled Records=63722\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=157\n",
      "\t\tCPU time spent (ms)=4090\n",
      "\t\tPhysical memory (bytes) snapshot=712200192\n",
      "\t\tVirtual memory (bytes) snapshot=4683845632\n",
      "\t\tTotal committed heap usage (bytes)=645398528\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1184766\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=5541\n",
      "16/01/23 12:00:44 INFO streaming.StreamJob: Output directory: /user/miki/week02/hw2_3_output_predict2\n"
     ]
    }
   ],
   "source": [
    "# Change permissions on mapper and reducer\n",
    "!chmod +x mapper.py\n",
    "!chmod +x reducer.py\n",
    "\n",
    "# Delete output folder if it exists\n",
    "!hdfs dfs -rm -r /user/miki/week02/hw2_3_output_predict2\n",
    "\n",
    "# Run job\n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-D stream.map.output.field.separator=^ \\\n",
    "-D stream.num.map.output.key.fields=3 \\\n",
    "-D mapreduce.map.output.key.field.separator=^ \\\n",
    "-D mapreduce.partition.keypartitioner.options=-k1,2 \\\n",
    "-input /user/miki/week02/hw2_3_output_predict1/part* \\\n",
    "-output /user/miki/week02/hw2_3_output_predict2 \\\n",
    "-mapper /home/cloudera/Documents/W261-Fall2016/Week02/mapper.py \\\n",
    "-reducer /home/cloudera/Documents/W261-Fall2016/Week02/reducer.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!hdfs dfs -cat /user/miki/week02/hw2_3_output_predict2/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copy files from HDFS\n",
    "#!hdfs dfs -put FILEONHDFS workingdirectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pretty print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_results(thefile):\n",
    "    # Read in data from text file\n",
    "    with open(thefile, 'r') as myfile:\n",
    "        # Initialize\n",
    "        total = 0.0\n",
    "        incorrect = 0.0\n",
    "        \n",
    "        # Print header\n",
    "        table = '{:28s}{:^12s}{:^12s}{:15f}{:15f}'.format\n",
    "        print '{:28s}{:^12s}{:^12s}{:>15s}{:>15s}'.format('Document', \n",
    "                                                      'Truth', \n",
    "                                                      'Predicted', \n",
    "                                                      'Log Prob Spam', \n",
    "                                                      'Log Prob Ham')\n",
    "        print '----------------------------------------------------------------------------------'\n",
    "        for line in myfile:\n",
    "            fields = line.split()\n",
    "\n",
    "            # Only look at records that have a doc_id as the first field\n",
    "            pattern = re.compile('^\\d{4}.\\d{4}-\\d{2}-\\d{2}\\.\\w+')\n",
    "            try:\n",
    "                key = fields[0]\n",
    "            except:\n",
    "                continue\n",
    "            if pattern.match(fields[0]):\n",
    "                print table(fields[0], fields[1], fields[2], float(fields[3]), float(fields[4]))\n",
    "                total += 1\n",
    "                if fields[1] != fields[2]: incorrect += 1\n",
    "            \n",
    "        print '----------------------------------------------------------------------------------'\n",
    "        print 'Total Documents: {:.0f}'.format(total)\n",
    "        print 'Error rate:      {:.0%}'.format(incorrect / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document                       Truth     Predicted    Log Prob Spam   Log Prob Ham\n",
      "----------------------------------------------------------------------------------\n",
      "0001.1999-12-10.farmer           0           1             0.297533      -0.571138\n",
      "0001.1999-12-10.kaminski         0           0             0.113204       1.263854\n",
      "0001.2000-01-17.beck             0           1           169.543130      90.312397\n",
      "0001.2000-06-06.lokay            0           1            78.647446      63.775913\n",
      "0001.2001-02-07.kitchen          0           0            -4.911744      19.358109\n",
      "0001.2001-04-02.williams         0           1            92.726948      25.197349\n",
      "0002.1999-12-13.farmer           0           1           165.428879      79.788727\n",
      "0002.2001-02-07.kitchen          0           1            34.682100       9.458434\n",
      "0002.2001-05-25.SA_and_HP        1           0            16.525785      44.258024\n",
      "0002.2003-12-18.GP               1           0            37.218578     113.654014\n",
      "0002.2004-08-01.BG               1           0            10.359233      40.708809\n",
      "0003.1999-12-10.kaminski         0           0             9.594514      15.038510\n",
      "0003.1999-12-14.farmer           0           0            -2.072302       2.346136\n",
      "0003.2000-01-17.beck             0           0             5.753512      45.493647\n",
      "0003.2001-02-08.kitchen          0           0            47.026771      51.044210\n",
      "0003.2003-12-18.GP               1           0            12.222788     101.829090\n",
      "0003.2004-08-01.BG               1           0            21.929859      83.862356\n",
      "0004.1999-12-10.kaminski         0           1            98.706189      46.228732\n",
      "0004.1999-12-14.farmer           0           1            99.267376      39.216443\n",
      "0004.2001-04-02.williams         0           1            36.650696       9.170073\n",
      "0004.2001-06-12.SA_and_HP        1           1            10.714340       6.612371\n",
      "0004.2004-08-01.BG               1           0            19.698144     117.314735\n",
      "0005.1999-12-12.kaminski         0           1            97.884492      40.040452\n",
      "0005.1999-12-14.farmer           0           1           106.527054      37.852514\n",
      "0005.2000-06-06.lokay            0           1            65.435980      15.876519\n",
      "0005.2001-02-08.kitchen          0           1            75.405690      37.755108\n",
      "0005.2001-06-23.SA_and_HP        1           0             5.675386      17.169814\n",
      "0005.2003-12-18.GP               1           0           137.811244     585.083912\n",
      "0006.1999-12-13.kaminski         0           1            50.796916      12.396519\n",
      "0006.2001-02-08.kitchen          0           1           307.973750     208.350288\n",
      "0006.2001-04-03.williams         0           1            11.741806       5.576597\n",
      "0006.2001-06-25.SA_and_HP        1           0             5.206772      40.793749\n",
      "0006.2003-12-18.GP               1           0            29.683275     182.588413\n",
      "0006.2004-08-01.BG               1           0            29.484037     157.852749\n",
      "0007.1999-12-13.kaminski         0           1            42.375500      33.748469\n",
      "0007.1999-12-14.farmer           0           1            50.949368      23.468937\n",
      "0007.2000-01-17.beck             0           0            47.585593     111.094115\n",
      "0007.2001-02-09.kitchen          0           0            54.995573      59.485634\n",
      "0007.2003-12-18.GP               1           0            41.350935     186.887074\n",
      "0007.2004-08-01.BG               1           0            35.895096      43.467413\n",
      "0008.2001-02-09.kitchen          0           1           430.726317     178.002115\n",
      "0008.2001-06-12.SA_and_HP        1           1            10.714340       6.612371\n",
      "0008.2001-06-25.SA_and_HP        1           0            98.922135     257.145056\n",
      "0008.2003-12-18.GP               1           0            23.825689      95.580787\n",
      "0008.2004-08-01.BG               1           0           131.756706     623.022493\n",
      "0009.1999-12-13.kaminski         0           1           183.403687      94.085382\n",
      "0009.1999-12-14.farmer           0           0            21.073379      21.480520\n",
      "0009.2000-06-07.lokay            0           1           251.479199      99.605024\n",
      "0009.2001-02-09.kitchen          0           1           227.173434     179.309532\n",
      "0009.2001-06-26.SA_and_HP        1           0            27.689723      62.565594\n",
      "0009.2003-12-18.GP               1           0            30.065567     140.522387\n",
      "0010.1999-12-14.farmer           0           1           161.833022      67.713970\n",
      "0010.1999-12-14.kaminski         0           1            31.032440       8.703255\n",
      "0010.2001-02-09.kitchen          0           0            87.666375     125.937308\n",
      "0010.2001-06-28.SA_and_HP        1           0            83.751099     151.941823\n",
      "0010.2003-12-18.GP               1           0            -0.221600       4.553930\n",
      "0010.2004-08-01.BG               1           0            59.758481     239.214923\n",
      "0011.1999-12-14.farmer           0           1            92.476555      39.585890\n",
      "0011.2001-06-28.SA_and_HP        1           0            83.751099     153.200284\n",
      "0011.2001-06-29.SA_and_HP        1           0           220.064909     665.257365\n",
      "0011.2003-12-18.GP               1           0            13.412127      47.827917\n",
      "0011.2004-08-01.BG               1           0            19.509863      84.053123\n",
      "0012.1999-12-14.farmer           0           1           107.203682      93.089153\n",
      "0012.1999-12-14.kaminski         0           1            46.178881      30.500406\n",
      "0012.2000-01-17.beck             0           0            46.242140     108.777824\n",
      "0012.2000-06-08.lokay            0           1            75.739098      10.266182\n",
      "0012.2001-02-09.kitchen          0           1            59.091855      17.145792\n",
      "0012.2003-12-19.GP               1           0             4.873335       5.138224\n",
      "0013.1999-12-14.farmer           0           0            28.400256      52.515739\n",
      "0013.1999-12-14.kaminski         0           0            -1.030738      27.415913\n",
      "0013.2001-04-03.williams         0           1            39.639722      17.610258\n",
      "0013.2001-06-30.SA_and_HP        1           0           322.877334    1044.352043\n",
      "0013.2004-08-01.BG               1           0            32.304849     191.594256\n",
      "0014.1999-12-14.kaminski         0           1            36.927928      31.354386\n",
      "0014.1999-12-15.farmer           0           1            78.348405      43.391680\n",
      "0014.2001-02-12.kitchen          0           1           129.507016      57.273421\n",
      "0014.2001-07-04.SA_and_HP        1           0            57.765348     221.253192\n",
      "0014.2003-12-19.GP               1           0             1.816043      17.807242\n",
      "0014.2004-08-01.BG               1           0            22.009261     128.520068\n",
      "0015.1999-12-14.kaminski         0           1            91.120479      26.442760\n",
      "0015.1999-12-15.farmer           0           1            53.239082      32.474802\n",
      "0015.2000-06-09.lokay            0           1            10.507099       3.939048\n",
      "0015.2001-02-12.kitchen          0           1           132.372831     113.198895\n",
      "0015.2001-07-05.SA_and_HP        1           0            15.943050      37.204861\n",
      "0015.2003-12-19.GP               1           0            29.575397      79.441687\n",
      "0016.1999-12-15.farmer           0           0            10.011971      16.009952\n",
      "0016.2001-02-12.kitchen          0           1            69.427852      47.251661\n",
      "0016.2001-07-05.SA_and_HP        1           0            15.943050      37.204861\n",
      "0016.2001-07-06.SA_and_HP        1           0           229.412500     479.196990\n",
      "0016.2003-12-19.GP               1           0            29.888072     180.170192\n",
      "0016.2004-08-01.BG               1           0            11.557660      19.830415\n",
      "0017.1999-12-14.kaminski         0           1            32.005039       4.462523\n",
      "0017.2000-01-17.beck             0           0            59.925473     109.646428\n",
      "0017.2001-04-03.williams         0           1            18.990595       8.863565\n",
      "0017.2003-12-18.GP               1           0             4.690239      20.735471\n",
      "0017.2004-08-01.BG               1           0            45.880404     320.673291\n",
      "0017.2004-08-02.BG               1           0            36.261569     179.747119\n",
      "0018.1999-12-14.kaminski         0           1            50.450996      12.128321\n",
      "0018.2001-07-13.SA_and_HP        1           0            52.297081     169.461804\n",
      "0018.2003-12-18.GP               1           0            51.729651     266.331989\n",
      "----------------------------------------------------------------------------------\n",
      "Total Documents: 100\n",
      "Error rate:      83%\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "print_results('predict2_output.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogram of posterior probabilities\n",
    "\n",
    "Plot a histogram of the log posterior probabilities (i.e., log(Pr(Class|Doc))) for each class over the training set. Summarize what you see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0xcd85a90>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuYAAAEZCAYAAADbteACAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA3bMJUmSpDFgx1ySJEkaA/8/G0KJ/BDC+IsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xbfa8208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# Import libraries for plotting and analysis\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Initialize data structures\n",
    "doc, Y, prediction, log_prob_spam, log_prob_ham = [], [], [], [], []\n",
    "\n",
    "# Read in data from text file\n",
    "with open('predict2_output.txt', 'r') as myfile:\n",
    "    for line in myfile:\n",
    "        fields = line.split()\n",
    "\n",
    "        # Only look at records that have a doc_id as the first field\n",
    "        pattern = re.compile('^\\d{4}.\\d{4}-\\d{2}-\\d{2}\\.\\w+')\n",
    "        try:\n",
    "            key = fields[0]\n",
    "        except:\n",
    "            continue\n",
    "        if pattern.match(fields[0]):\n",
    "            doc.append(fields[0])\n",
    "            Y.append(fields[1])\n",
    "            prediction.append(fields[2])\n",
    "            log_prob_spam.append(fields[3])\n",
    "            log_prob_ham.append(fields[4])\n",
    "\n",
    "# Convert to numpy arrays\n",
    "doc = np.array(doc)\n",
    "Y = np.array(Y)\n",
    "prediction = np.array(prediction)\n",
    "log_prob_spam = np.array(log_prob_spam).astype(np.float)\n",
    "log_prob_ham = np.array(log_prob_ham).astype(np.float)\n",
    "\n",
    "num_bins = 30\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Print spam histogram\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(log_prob_spam, num_bins, normed=1, facecolor='green', alpha=0.5)\n",
    "plt.xlabel('Spam Probability')\n",
    "plt.ylabel('Probability')\n",
    "plt.title(r'Histogram of Spam Probabilities')\n",
    "\n",
    "# Print ham histogram\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(log_prob_ham, num_bins, normed=1, facecolor='green', alpha=0.5)\n",
    "plt.xlabel('Ham Probability')\n",
    "plt.ylabel('Probability')\n",
    "plt.title(r'Histogram of Ham Probabilities')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.4: Repeat HW2.3 with the following modification: use Laplace plus-one smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare the misclassifcation error rates for 2.3 versus 2.4 and explain the differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.5: Repeat HW2.4. This time when modeling and classification ignore tokens with a frequency of less than three (3) in the training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does it affect the misclassifcation error of learnt naive multinomial Bayesian Classifier on the training dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.6: Benchmark your code with the Python SciKit-Learn implementation of the multinomial Naive Bayes algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run the Multinomial Naive Bayes algorithm (using default settings) from SciKit-Learn over the same training data used in HW2.5 and report the misclassification error (please note some data preparation might be needed to get the Multinomial Naive Bayes algorithm from SkiKit-Learn to run over this dataset)\n",
    "- Prepare a table to present your results, where rows correspond to approach used (SkiKit-Learn versus your Hadoop implementation) and the column presents the training misclassification error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# General libraries\n",
    "from __future__ import division\n",
    "\n",
    "# SK-learn libraries for learning\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100L,) (100L,)\n"
     ]
    }
   ],
   "source": [
    "# Read data in and create data and label arrays\n",
    "ids, X, Y = [], [], []\n",
    "\n",
    "with open('enronemail_1h.txt', 'r') as myfile:\n",
    "    for line in myfile:\n",
    "        fields = line.split(\"\\t\")\n",
    "        \n",
    "        # Some records are malformed, so make sure that we take the right fields\n",
    "        subj, body = \"\", \"\"\n",
    "        \n",
    "        if len(fields) >= 3:\n",
    "            subj = fields[2]\n",
    "        if len(fields) >= 4:\n",
    "            body = fields[3]\n",
    "\n",
    "        text = subj + \" \" + body\n",
    "        text = text.replace(\"\\n\", \"\")\n",
    "        X.append(text)\n",
    "        Y.append(fields[1])\n",
    "        ids.append(fields[0])\n",
    "\n",
    "# Convert these to numpy arrays\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "# Check that the shapes look correct\n",
    "print X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method                    Error\n",
      "--------------------------------\n",
      "SK-learn MultinomialNB       0%\n",
      "HW2.4 (+1 smoothing)        83%\n",
      "HW2.5 (drop words <3)       83%\n",
      "HW2.3 (no smoothing)        83%\n"
     ]
    }
   ],
   "source": [
    "def hw2_6():\n",
    "    train_errors = {}\n",
    "    \n",
    "    ##### MULTINOMIAL NB\n",
    "    # Create Pipeline to get feature vectors and train\n",
    "    # Use CountVectorizer to get feature arrays\n",
    "    # Classify using Multinomial NB\n",
    "    mnb_pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                         ('clf', MultinomialNB()),\n",
    "                        ])\n",
    "\n",
    "    # Fit training data and labels\n",
    "    mnb_pipe.fit(X, Y)\n",
    "\n",
    "    # Print training error\n",
    "    mnb_predictions = mnb_pipe.predict(X)\n",
    "    train_errors[\"SK-learn MultinomialNB\"] = sum(mnb_predictions != Y) / Y.size\n",
    "    \n",
    "    \n",
    "    ##### CLASSIFIER in HW2.3\n",
    "    # Read output from results\n",
    "    \n",
    "    incorrect = 0.0\n",
    "    total = 0.0\n",
    "\n",
    "    # Read in data from text file\n",
    "    with open('predict2_output.txt', 'r') as myfile:\n",
    "        for line in myfile:\n",
    "            fields = line.split()\n",
    "\n",
    "            # Only look at records that have a doc_id as the first field\n",
    "            pattern = re.compile('^\\d{4}.\\d{4}-\\d{2}-\\d{2}\\.\\w+')\n",
    "            try:\n",
    "                key = fields[0]\n",
    "            except:\n",
    "                continue\n",
    "            if pattern.match(fields[0]):\n",
    "                total += 1\n",
    "                if fields[1] != fields[2]: incorrect += 1\n",
    "    \n",
    "    train_errors[\"HW2.3 (no smoothing)\"] = incorrect/total\n",
    "    \n",
    "    \n",
    "    ##### CLASSIFIER in HW2.4\n",
    "    # Read output from results\n",
    "    \n",
    "    incorrect = 0.0\n",
    "    total = 0.0\n",
    "\n",
    "    # Read in data from text file\n",
    "    with open('', 'r') as myfile:\n",
    "        for line in myfile:\n",
    "            fields = line.split()\n",
    "\n",
    "            # Only look at records that have a doc_id as the first field\n",
    "            pattern = re.compile('^\\d{4}.\\d{4}-\\d{2}-\\d{2}\\.\\w+')\n",
    "            try:\n",
    "                key = fields[0]\n",
    "            except:\n",
    "                continue\n",
    "            if pattern.match(fields[0]):\n",
    "                total += 1\n",
    "                if fields[1] != fields[2]: incorrect += 1\n",
    "    \n",
    "    train_errors[\"HW2.4 (+1 smoothing)\"] = incorrect/total\n",
    "    \n",
    "    ##### CLASSIFIER in HW2.5\n",
    "    # Read output from results\n",
    "    \n",
    "    incorrect = 0.0\n",
    "    total = 0.0\n",
    "\n",
    "    # Read in data from text file\n",
    "    with open('', 'r') as myfile:\n",
    "        for line in myfile:\n",
    "            fields = line.split()\n",
    "\n",
    "            # Only look at records that have a doc_id as the first field\n",
    "            pattern = re.compile('^\\d{4}.\\d{4}-\\d{2}-\\d{2}\\.\\w+')\n",
    "            try:\n",
    "                key = fields[0]\n",
    "            except:\n",
    "                continue\n",
    "            if pattern.match(fields[0]):\n",
    "                total += 1\n",
    "                if fields[1] != fields[2]: incorrect += 1\n",
    "    \n",
    "    train_errors[\"HW2.5 (drop words <3)\"] = incorrect/total\n",
    "    \n",
    "    ##### TABLE OF TRAINING ERRORS\n",
    "    print \"{:<25s}{:>6s}\".format(\"Method\", \"Error\")\n",
    "    print \"--------------------------------\"\n",
    "    for method in train_errors:\n",
    "        print \"{:<25s}{:>6.0%}\".format(method, train_errors[method])\n",
    "    \n",
    "hw2_6()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explain/justify any differences in terms of training error rates over the dataset in HW2.5 between your Multinomial Naive Bayes implementation (in Map Reduce) versus the Multinomial Naive Bayes implementation in SciKit-Learn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
