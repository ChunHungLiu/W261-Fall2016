{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASCI W261: Machine Learning at Scale\n",
    "## Assignment Week 2\n",
    "Miki Seltzer (miki.seltzer@berkeley.edu)<br>\n",
    "W261-2, Spring 2016<br>\n",
    "Submission: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### HW2.0:\n",
    "\n",
    "#### What is a race condition in the context of parallel computation? Give an example.\n",
    "A race condition is when a section of code is executed by multiple processes, and the order in which the processes execute will impact the final result.\n",
    "\n",
    "![Race condition example](race_conditions.png)\n",
    "Source: https://en.wikipedia.org/wiki/Race_condition#Example\n",
    "\n",
    "#### What is MapReduce?\n",
    "MapReduce can refer to multiple concepts:\n",
    "- **Programming model:** Processes are split into a \"mapping\" phase, and a \"reducing\" phase. In the map phase, a certain function is mapped on to each value in a data set, and then in the reduce phase, the result of the map phase is aggregated. \n",
    "- **Execution framework:** This framework coordinates running processes written with the above model in mind.\n",
    "- **Software implementation:** MapReduce is the name of Google's proprietary implementation of this programming model, while Apache Hadoop is the open-source implementation.\n",
    "\n",
    "#### How does it differ from Hadoop?\n",
    "Hadoop is the open-source implementation of Google's MapReduce. Hadoop consists of two parts: distributed storage of data, and distributed processing of data. HDFS is the storage part, and MapReduce is the processing part.\n",
    "\n",
    "#### Which programming paradigm is Hadoop based on? Explain and give a simple example in code and show the code running.\n",
    "Hadoop is based on the MapReduce paradigm. The classic example of the MapReduce programming paradigm is word count. In the map phase of word count, each word in a document is assigned a count of 1. In the reduce phase, the counts for each unique word are summed to yield the final count of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP PHASE\n",
      "['hello', 1]\n",
      "['this', 1]\n",
      "['is', 1]\n",
      "['a', 1]\n",
      "['test', 1]\n",
      "['to', 1]\n",
      "['test', 1]\n",
      "['word', 1]\n",
      "['count', 1]\n",
      "['test', 1]\n",
      "['should', 1]\n",
      "['have', 1]\n",
      "['a', 1]\n",
      "['count', 1]\n",
      "['of', 1]\n",
      "['three', 1]\n",
      "\n",
      "REDUCE PHASE\n",
      "['a', 1] (intermediate step)\n",
      "['a', 2] FINAL SUM\n",
      "['count', 1] (intermediate step)\n",
      "['count', 2] FINAL SUM\n",
      "['have', 1] FINAL SUM\n",
      "['hello', 1] FINAL SUM\n",
      "['is', 1] FINAL SUM\n",
      "['of', 1] FINAL SUM\n",
      "['should', 1] FINAL SUM\n",
      "['test', 1] (intermediate step)\n",
      "['test', 2] (intermediate step)\n",
      "['test', 3] FINAL SUM\n",
      "['this', 1] FINAL SUM\n",
      "['three', 1] FINAL SUM\n",
      "['to', 1] FINAL SUM\n",
      "['word', 1] FINAL SUM\n"
     ]
    }
   ],
   "source": [
    "def hw2_0():\n",
    "    doc = \"Hello this is a test to test word count test should have a count of three\".lower()\n",
    "    key_vals = []\n",
    "    \n",
    "    print \"MAP PHASE\"\n",
    "    for word in doc.split():\n",
    "        print [word, 1]\n",
    "        key_vals.append([word, 1])\n",
    "    \n",
    "    print \"\\nREDUCE PHASE\"\n",
    "    key_vals = sorted(key_vals)\n",
    "    \n",
    "    current_word = None\n",
    "    current_count = 0\n",
    "    \n",
    "    for pair in key_vals:\n",
    "        if current_word == pair[0]:\n",
    "            print [current_word, current_count], \"(intermediate step)\"\n",
    "            current_count += pair[1]\n",
    "        else:\n",
    "            if current_word:\n",
    "                print [current_word, current_count], \"FINAL SUM\"\n",
    "            current_word = pair[0]\n",
    "            current_count = pair[1]\n",
    "\n",
    "    print [current_word, current_count], \"FINAL SUM\"\n",
    "    \n",
    "hw2_0()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.1: Sort in Hadoop MapReduce\n",
    "**Given as input: Records of the form `<integer, “NA”>`, where integer is any integer, and “NA” is just the empty string.**<br>\n",
    "**Output: Sorted key value pairs of the form `<integer, “NA”>`; what happens if you have multiple reducers? Do you need additional steps? Explain.**\n",
    "\n",
    "If there are multiple reducers, then a straightforward MapReduce process will yield outputs that are sorted within each reducer, but not sorted across all reducers. In order to output a sort across all reducers, an extra step needs to be implemented that will intelligently send keys to reducers so that the result from all reducers will yield a complete sort. For example, let's say our keys ranged from 0-300. If we had 3 reducers, we could send all keys in the range [0,100) to reducer 1, [100, 200) to reducer 2, and [200, 300] to reducer 3. Thus, the output of each reducer will yield documents that are completely sorted. We would need to balance the keys sent to each reducer to ensure that the load is still balanced between all reducers, which will require some calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write code to generate N  random records of the form `<integer, “NA”>`. Let N = 10,000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to need the Hadoop Streaming jar file, so let's download it here so that we know which one to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-01-21 18:37:45--  http://central.maven.org/maven2/org/apache/hadoop/hadoop-streaming/2.7.1/hadoop-streaming-2.7.1.jar\n",
      "Resolving central.maven.org... 23.235.47.209\n",
      "Connecting to central.maven.org|23.235.47.209|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 105736 (103K) [application/java-archive]\n",
      "Saving to: “hadoop-streaming-2.7.1.jar”\n",
      "\n",
      "100%[======================================>] 105,736     --.-K/s   in 0.1s    \n",
      "\n",
      "2016-01-21 18:38:00 (1.06 MB/s) - “hadoop-streaming-2.7.1.jar” saved [105736/105736]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://central.maven.org/maven2/org/apache/hadoop/hadoop-streaming/2.7.1/hadoop-streaming-2.7.1.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "with open(\"random.txt\", \"w\") as myfile:\n",
    "    for i in range(10000):\n",
    "        myfile.write(\"{:d},{:s}\\n\".format(random.randint(0, 100000), \"NA\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write the Python Hadoop streaming map-reduce job to perform this sort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: mapper code for HW2.1\n",
    "\n",
    "import sys\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # In this case, we do not need to map the input to anything\n",
    "    key, value = line.strip().split(',')\n",
    "    print \"%010d\\t%s\" % (int(key), value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: reducer code for HW2.1\n",
    "\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # In this case, we do not need to reduce anything\n",
    "    key, value = line.strip().split('\\t')\n",
    "    print \"%d\\t%s\" % (int(key), value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make HDFS directory and put random.txt there\n",
    "!hdfs dfs -mkdir /user/miki/week02\n",
    "!hdfs dfs -put random.txt /user/miki/week02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/22 12:59:13 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/miki/week02/hw2_1_output\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.5.0.jar] /tmp/streamjob4129142604574616452.jar tmpDir=null\n",
      "16/01/22 12:59:16 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/22 12:59:17 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/22 12:59:17 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/22 12:59:17 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/01/22 12:59:17 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/01/22 12:59:17 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1453405632837_0022\n",
      "16/01/22 12:59:18 INFO impl.YarnClientImpl: Submitted application application_1453405632837_0022\n",
      "16/01/22 12:59:18 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1453405632837_0022/\n",
      "16/01/22 12:59:18 INFO mapreduce.Job: Running job: job_1453405632837_0022\n",
      "16/01/22 12:59:25 INFO mapreduce.Job: Job job_1453405632837_0022 running in uber mode : false\n",
      "16/01/22 12:59:25 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/22 12:59:32 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/01/22 12:59:33 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/22 12:59:44 INFO mapreduce.Job:  map 100% reduce 25%\n",
      "16/01/22 12:59:46 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "16/01/22 12:59:48 INFO mapreduce.Job:  map 100% reduce 75%\n",
      "16/01/22 12:59:49 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/22 12:59:49 INFO mapreduce.Job: Job job_1453405632837_0022 completed successfully\n",
      "16/01/22 12:59:49 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=160024\n",
      "\t\tFILE: Number of bytes written=1002568\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=93193\n",
      "\t\tHDFS: Number of bytes written=88875\n",
      "\t\tHDFS: Number of read operations=18\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=8\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=4\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=11193\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=43568\n",
      "\t\tTotal time spent by all map tasks (ms)=11193\n",
      "\t\tTotal time spent by all reduce tasks (ms)=43568\n",
      "\t\tTotal vcore-seconds taken by all map tasks=11193\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=43568\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=11461632\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=44613632\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=10000\n",
      "\t\tMap output records=10000\n",
      "\t\tMap output bytes=140000\n",
      "\t\tMap output materialized bytes=160048\n",
      "\t\tInput split bytes=222\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=9533\n",
      "\t\tReduce shuffle bytes=160048\n",
      "\t\tReduce input records=10000\n",
      "\t\tReduce output records=10000\n",
      "\t\tSpilled Records=20000\n",
      "\t\tShuffled Maps =8\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=8\n",
      "\t\tGC time elapsed (ms)=441\n",
      "\t\tCPU time spent (ms)=7330\n",
      "\t\tPhysical memory (bytes) snapshot=1214963712\n",
      "\t\tVirtual memory (bytes) snapshot=9382010880\n",
      "\t\tTotal committed heap usage (bytes)=1192230912\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=92971\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=88875\n",
      "16/01/22 12:59:49 INFO streaming.StreamJob: Output directory: /user/miki/week02/hw2_1_output\n"
     ]
    }
   ],
   "source": [
    "# Change permissions on mapper and reducer\n",
    "!chmod +x mapper.py\n",
    "!chmod +x reducer.py\n",
    "\n",
    "# If output folder already exists, delete it\n",
    "!hdfs dfs -rm -r /user/miki/week02/hw2_1_output\n",
    "\n",
    "# Run job\n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.reduce.tasks=4 \\\n",
    "-mapper /home/cloudera/Documents/W261-Fall2016/Week02/mapper.py \\\n",
    "-reducer /home/cloudera/Documents/W261-Fall2016/Week02/reducer.py \\\n",
    "-input /user/miki/week02/random.txt \\\n",
    "-output /user/miki/week02/hw2_1_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -cat /user/miki/week02/hw2_1_output/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.2: Using the Enron data from HW1 and Hadoop MapReduce streaming, write mapper/reducer pair that  will determine the number of occurrences of a single, user-specified word. \n",
    "\n",
    "Examine the word “assistance” and report your results. To do so, make sure that mapper.py counts all occurrences of a single word, and reducer.py collates the counts of the single word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load enronemail_1h.txt into HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -put enronemail_1h.txt /user/miki/week02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: mapper code for HW2.2\n",
    "\n",
    "import sys\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Keyword from user input\n",
    "keyword = sys.argv[1]\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    \n",
    "    # Strip white space from line, then split into fields\n",
    "    # Replace commas with spaces (we are using commas as a delimiter as well)\n",
    "    # Remove remaining punctuation from subject and body\n",
    "    # Concatenate, then split subject and body by spaces\n",
    "    # Some records are malformed -- if there is a 4th field, use it\n",
    "    fields = line.strip().split('\\t')\n",
    "    subj = fields[2].replace(',', ' ')\n",
    "    subj = subj.translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "    if len(fields) == 4:\n",
    "        body = fields[3].replace(',', ' ')\n",
    "        body = body.translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "    else:\n",
    "        body = \"\"\n",
    "    words = subj + \" \" + body\n",
    "    words = words.split()\n",
    "    \n",
    "    # Loop through words\n",
    "    # If word is keyword, write to file\n",
    "    # key = word\n",
    "    # value = 1\n",
    "    for word in words:\n",
    "        if keyword == word:\n",
    "            print \"%s\\t%s\" % (word, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: reducer code for HW2.2\n",
    "\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "# Initialize some variables\n",
    "# We know that the words will be sorted\n",
    "# We need to keep track of state\n",
    "prev_word = None\n",
    "prev_count = 0\n",
    "word = None\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # Strip and split line from mapper\n",
    "    word, count = line.strip().split('\\t', 1)\n",
    "    \n",
    "    # If possible, turn count into an int (it's read as a string)\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # We couldn't make count into an int, so move on\n",
    "        continue\n",
    "        \n",
    "    # Since the words will be sorted, all counts for a word will be grouped\n",
    "    if prev_word == word:\n",
    "        # If prev_word is word, then we haven't changed words\n",
    "        # Just update prev_count\n",
    "        prev_count += count\n",
    "    else:\n",
    "        # We've encountered a new word!\n",
    "        # This might be the first word, though\n",
    "        if prev_word:\n",
    "            # We need to print the last word we were on\n",
    "            print \"%s\\t%s\" % (prev_word, prev_count)\n",
    "        \n",
    "        # Now we need to initialize our variables for the new word and count\n",
    "        prev_word = word\n",
    "        prev_count = count\n",
    "\n",
    "# We have reached the end of the file, so print the last word and count\n",
    "if prev_word == word:\n",
    "    print \"%s\\t%s\" % (prev_word, prev_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Hadoop streaming command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/22 13:59:07 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/miki/week02/hw2_2_output\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.5.0.jar] /tmp/streamjob2515332195244567054.jar tmpDir=null\n",
      "16/01/22 13:59:10 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/22 13:59:10 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/22 13:59:10 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/22 13:59:10 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/01/22 13:59:11 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1453405632837_0027\n",
      "16/01/22 13:59:11 INFO impl.YarnClientImpl: Submitted application application_1453405632837_0027\n",
      "16/01/22 13:59:11 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1453405632837_0027/\n",
      "16/01/22 13:59:11 INFO mapreduce.Job: Running job: job_1453405632837_0027\n",
      "16/01/22 13:59:18 INFO mapreduce.Job: Job job_1453405632837_0027 running in uber mode : false\n",
      "16/01/22 13:59:18 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/22 13:59:25 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/01/22 13:59:26 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/22 13:59:32 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/22 13:59:32 INFO mapreduce.Job: Job job_1453405632837_0027 completed successfully\n",
      "16/01/22 13:59:32 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=156\n",
      "\t\tFILE: Number of bytes written=341606\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=216503\n",
      "\t\tHDFS: Number of bytes written=14\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=10277\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4092\n",
      "\t\tTotal time spent by all map tasks (ms)=10277\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4092\n",
      "\t\tTotal vcore-seconds taken by all map tasks=10277\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=4092\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=10523648\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=4190208\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=10\n",
      "\t\tMap output bytes=130\n",
      "\t\tMap output materialized bytes=162\n",
      "\t\tInput split bytes=236\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce shuffle bytes=162\n",
      "\t\tReduce input records=10\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=20\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=135\n",
      "\t\tCPU time spent (ms)=2100\n",
      "\t\tPhysical memory (bytes) snapshot=723128320\n",
      "\t\tVirtual memory (bytes) snapshot=4702228480\n",
      "\t\tTotal committed heap usage (bytes)=693633024\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=216267\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=14\n",
      "16/01/22 13:59:32 INFO streaming.StreamJob: Output directory: /user/miki/week02/hw2_2_output\n"
     ]
    }
   ],
   "source": [
    "# Change permissions on mapper and reducer\n",
    "!chmod +x mapper.py\n",
    "!chmod +x reducer.py\n",
    "\n",
    "# Delete output folder if it exists\n",
    "!hdfs dfs -rm -r /user/miki/week02/hw2_2_output\n",
    "\n",
    "# Run job\n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-mapper '/home/cloudera/Documents/W261-Fall2016/Week02/mapper.py assistance' \\\n",
    "-reducer /home/cloudera/Documents/W261-Fall2016/Week02/reducer.py \\\n",
    "-input /user/miki/week02/enronemail_1h.txt \\\n",
    "-output /user/miki/week02/hw2_2_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at output of job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance\t10\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/miki/week02/hw2_2_output/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.2.1: Using Hadoop MapReduce and your wordcount job (from HW2.2) determine the top-10 occurring tokens (most frequent tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################\n",
    "#\n",
    "#    DO THIS!!!!!!!\n",
    "#\n",
    "##############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.3: Multinomial NAIVE BAYES with NO Smoothing\n",
    "Using the Enron data from HW1 and Hadoop MapReduce, write  a mapper/reducer job(s) that will both learn Naive Bayes classifier and classify the Enron email messages using the learnt Naive Bayes classifier. Use all white-space delimited tokens as independent input variables (assume spaces, fullstops, commas as delimiters). \n",
    "\n",
    "Note: for multinomial Naive Bayes, the Pr(X=“assistance”|Y=SPAM) is calculated as follows:\n",
    "\n",
    "$$\n",
    "\\frac{\\text{number of times \"assistance\" occurs in SPAM labeled documents}}{\\text{the number of words in documents labeled SPAM}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapper/Reducer for fitting NB\n",
    "\n",
    "Mapper:\n",
    "- Input: training documents\n",
    "- Output: (word, 1, 0) if word was in spam, otherwise (word, 0, 1)\n",
    "  - Special words: \\*alldocs, \\*docs and \\*words\n",
    "  \n",
    "Reducer:\n",
    "- Input: (word, 1, 0) or (word, 0, 1)\n",
    "- Output: (word, spam count, ham count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: mapper code for HW2.3\n",
    "\n",
    "import sys\n",
    "import string\n",
    "import re\n",
    "\n",
    "counts = {\n",
    "    '*words':{\n",
    "        '1':0,\n",
    "        '0':0\n",
    "    },\n",
    "    '*docs':{\n",
    "        '1':0,\n",
    "        '0':0\n",
    "    }\n",
    "}\n",
    "total_docs = 0\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    \n",
    "    # Strip white space from line, then split into fields\n",
    "    # Replace commas with spaces (we are using commas as a delimiter as well)\n",
    "    # Remove remaining punctuation from subject and body\n",
    "    # Concatenate, then split subject and body by spaces\n",
    "    # Some records are malformed -- if there is a 4th field, use it\n",
    "    fields = line.strip().split('\\t')\n",
    "    \n",
    "    # Keep track of document counts\n",
    "    spam = fields[1]\n",
    "    counts['*docs'][spam] += 1\n",
    "    total_docs += 1\n",
    "    \n",
    "    subj = fields[2].replace(',', ' ')\n",
    "    subj = subj.translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "    if len(fields) == 4:\n",
    "        body = fields[3].replace(',', ' ')\n",
    "        body = body.translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "    else:\n",
    "        body = \"\"\n",
    "    words = subj + \" \" + body\n",
    "    words = words.split()\n",
    "    \n",
    "    # Loop through words\n",
    "    # If word is not trivial, write to file\n",
    "    # key = word\n",
    "    # value = 1\n",
    "    for word in words:\n",
    "        if len(word) > 0 and repr(word)[1] != '\\\\':\n",
    "            if spam == '1':\n",
    "                print \"%s\\t%s\\t%s\" % (word, 1, 0)\n",
    "            elif spam == '0':\n",
    "                print \"%s\\t%s\\t%s\" % (word, 0, 1)\n",
    "            counts['*words'][spam] += 1\n",
    "\n",
    "# At the end, output document and word counts\n",
    "for item in counts:\n",
    "    print \"%s\\t%s\\t%s\" % (item, counts[item]['1'], counts[item]['0'])\n",
    "print \"%s\\t%s\\t%s\" % ('*alldocs', total_docs, total_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: reducer code for HW2.3\n",
    "\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "import math\n",
    "\n",
    "# Initialize some variables\n",
    "# We know that the words will be sorted\n",
    "# We need to keep track of state\n",
    "prev_word = None\n",
    "prev_spam_count = 0\n",
    "prev_ham_count = 0\n",
    "word = None\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # Split line into fields\n",
    "    fields = line.strip().split('\\t')\n",
    "    word = fields[0]\n",
    "    spam_count = fields[1]\n",
    "    ham_count = fields[2]\n",
    "    \n",
    "    # If possible, turn count into an int (it's read as a string)\n",
    "    try:\n",
    "        spam_count = int(spam_count)\n",
    "        ham_count = int(ham_count)\n",
    "    except ValueError:\n",
    "        # We couldn't make count into an int, so move on\n",
    "        continue\n",
    "        \n",
    "    if prev_word == word:\n",
    "        # We have not moved to a new word\n",
    "        # Just update the count of this word\n",
    "        prev_spam_count += spam_count\n",
    "        prev_ham_count += ham_count\n",
    "        \n",
    "    else:\n",
    "        # We have encountered a new word!\n",
    "        # If this is the first word, we don't need to print anything\n",
    "        if prev_word: \n",
    "            # Write the previous word to file\n",
    "            print '%s\\t%s\\t%s' % (prev_word, prev_spam_count, prev_ham_count)\n",
    "            \n",
    "        # Now we need to initialize our variables\n",
    "        prev_word = word\n",
    "        prev_spam_count = spam_count\n",
    "        prev_ham_count = ham_count\n",
    "\n",
    "# We've reached the end of the file\n",
    "# Print the last word and counts\n",
    "print '%s\\t%s\\t%s' % (prev_word, prev_spam_count, prev_ham_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 02:24:29 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/miki/week02/hw2_3_output_fit\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.5.0.jar] /tmp/streamjob1294460146102016869.jar tmpDir=null\n",
      "16/01/23 02:24:33 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/23 02:24:33 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/23 02:24:33 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/23 02:24:33 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/01/23 02:24:34 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1453405632837_0143\n",
      "16/01/23 02:24:34 INFO impl.YarnClientImpl: Submitted application application_1453405632837_0143\n",
      "16/01/23 02:24:34 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1453405632837_0143/\n",
      "16/01/23 02:24:34 INFO mapreduce.Job: Running job: job_1453405632837_0143\n",
      "16/01/23 02:24:41 INFO mapreduce.Job: Job job_1453405632837_0143 running in uber mode : false\n",
      "16/01/23 02:24:41 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/23 02:24:49 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/01/23 02:24:50 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/23 02:24:57 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/23 02:24:57 INFO mapreduce.Job: Job job_1453405632837_0143 completed successfully\n",
      "16/01/23 02:24:57 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=373606\n",
      "\t\tFILE: Number of bytes written=1088485\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=216503\n",
      "\t\tHDFS: Number of bytes written=68226\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=11715\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4813\n",
      "\t\tTotal time spent by all map tasks (ms)=11715\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4813\n",
      "\t\tTotal vcore-seconds taken by all map tasks=11715\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=4813\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=11996160\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=4928512\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=31567\n",
      "\t\tMap output bytes=310466\n",
      "\t\tMap output materialized bytes=373612\n",
      "\t\tInput split bytes=236\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5710\n",
      "\t\tReduce shuffle bytes=373612\n",
      "\t\tReduce input records=31567\n",
      "\t\tReduce output records=5710\n",
      "\t\tSpilled Records=63134\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=149\n",
      "\t\tCPU time spent (ms)=3890\n",
      "\t\tPhysical memory (bytes) snapshot=737927168\n",
      "\t\tVirtual memory (bytes) snapshot=4685791232\n",
      "\t\tTotal committed heap usage (bytes)=693633024\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=216267\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=68226\n",
      "16/01/23 02:24:57 INFO streaming.StreamJob: Output directory: /user/miki/week02/hw2_3_output_fit\n"
     ]
    }
   ],
   "source": [
    "# Change permissions on mapper and reducer\n",
    "!chmod +x mapper.py\n",
    "!chmod +x reducer.py\n",
    "\n",
    "# Delete output folder if it exists\n",
    "!hdfs dfs -rm -r /user/miki/week02/hw2_3_output_fit\n",
    "\n",
    "# Run job\n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-input /user/miki/week02/enronemail_1h.txt \\\n",
    "-output /user/miki/week02/hw2_3_output_fit \\\n",
    "-mapper /home/cloudera/Documents/W261-Fall2016/Week02/mapper.py \\\n",
    "-reducer /home/cloudera/Documents/W261-Fall2016/Week02/reducer.py \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!hdfs dfs -cat /user/miki/week02/hw2_3_output_fit/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapper/Reducer #1 for predicting NB\n",
    "\n",
    "Mapper:\n",
    "- Input from fit: (word, spam count, ham count)\n",
    "- Output: (word, spam count, ham count)\n",
    "- Input testing document: (document ID, cat, subj, body)\n",
    "- Output: (word, cat, document ID)\n",
    "\n",
    "Reducer:\n",
    "- Input: (word, spam count, ham count)\n",
    "- Input: (word, cat, document ID)\n",
    "- Output: (document ID, cat, word, spam count, ham count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: mapper code for HW2.3\n",
    "\n",
    "import sys\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    \n",
    "    # Strip white space from line, then split into fields    \n",
    "    fields = line.strip().split('\\t')\n",
    "    \n",
    "    # If first field matches pattern of document ID, tokenize words\n",
    "    pattern = re.compile('^\\d{4}.\\d{4}-\\d{2}-\\d{2}\\.\\w+')\n",
    "    if pattern.match(fields[0]):\n",
    "        # Keep track of document counts\n",
    "        doc_id = fields[0]\n",
    "        spam = fields[1]\n",
    "\n",
    "        # We are always going to need the doc/word counts for each document\n",
    "        print '%s^%s^%s' % ('*alldocs', spam, doc_id)\n",
    "        print '%s^%s^%s' % ('*docs', spam, doc_id)\n",
    "        print '%s^%s^%s' % ('*words', spam, doc_id)\n",
    "        \n",
    "        # Replace commas with spaces (we are using commas as a delimiter as well)\n",
    "        # Remove remaining punctuation from subject and body\n",
    "        # Concatenate, then split subject and body by spaces\n",
    "        # Some records are malformed -- if there is a 4th field, use it\n",
    "        subj = fields[2].replace(',', ' ')\n",
    "        subj = subj.translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "        if len(fields) == 4:\n",
    "            body = fields[3].replace(',', ' ')\n",
    "            body = body.translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "        else:\n",
    "            body = \"\"\n",
    "        words = subj + \" \" + body\n",
    "        words = words.split()\n",
    "\n",
    "        # Loop through words\n",
    "        # If word is not trivial, write to file\n",
    "        # key = word\n",
    "        # value = doc_id\n",
    "        for word in words:\n",
    "            if len(word) > 0 and repr(word)[1] != '\\\\':\n",
    "                print '%s^%s^%s' % (word, spam, doc_id)\n",
    "    else:\n",
    "        # Now we know that the record is the\n",
    "        # output of the previous MapReduce job\n",
    "        word = fields[0]\n",
    "        spam_count = fields[1]\n",
    "        ham_count = fields[2]\n",
    "        print '%s^%s^%s' % (word, '*' + spam_count, ham_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: reducer code for HW2.3\n",
    "\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "import math\n",
    "import re\n",
    "\n",
    "# Initialize some variables\n",
    "spam_count = 0\n",
    "ham_count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # Strip and split line\n",
    "    key, value = line.strip().split('\\t')\n",
    "    word, field1 = key.split('^')\n",
    "\n",
    "    # If field1 starts with a *, we know that it is the spam and ham counts\n",
    "    if field1[0] == '*':\n",
    "        # This record will be of the form (word field1=*spam_count value=ham_count)\n",
    "        try:\n",
    "            spam_count = int(field1.replace('*',''))\n",
    "            ham_count = int(value)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    \n",
    "    else:\n",
    "        # Now we know that it is a doc_id record\n",
    "        # This record will be of the form (word field1=cat value=doc_id)\n",
    "        pattern = re.compile('^\\d{4}.\\d{4}-\\d{2}-\\d{2}\\.\\w+')\n",
    "        if pattern.match(value):\n",
    "            doc_id = value\n",
    "        print '%s\\t%s\\t%s\\t%s\\t%s' % (doc_id, field1, word, spam_count, ham_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 09:16:47 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/miki/week02/hw2_3_output_predict1\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.5.0.jar] /tmp/streamjob8552654983851536121.jar tmpDir=null\n",
      "16/01/23 09:16:50 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/23 09:16:50 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/23 09:16:51 INFO mapred.FileInputFormat: Total input paths to process : 2\n",
      "16/01/23 09:16:51 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "16/01/23 09:16:51 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1453405632837_0175\n",
      "16/01/23 09:16:51 INFO impl.YarnClientImpl: Submitted application application_1453405632837_0175\n",
      "16/01/23 09:16:51 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1453405632837_0175/\n",
      "16/01/23 09:16:51 INFO mapreduce.Job: Running job: job_1453405632837_0175\n",
      "16/01/23 09:16:58 INFO mapreduce.Job: Job job_1453405632837_0175 running in uber mode : false\n",
      "16/01/23 09:16:58 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/23 09:17:09 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/01/23 09:17:11 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/23 09:17:16 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/23 09:17:16 INFO mapreduce.Job: Job job_1453405632837_0175 completed successfully\n",
      "16/01/23 09:17:16 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1153594\n",
      "\t\tFILE: Number of bytes written=2764889\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=293049\n",
      "\t\tHDFS: Number of bytes written=1180670\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=3\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=21841\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4621\n",
      "\t\tTotal time spent by all map tasks (ms)=21841\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4621\n",
      "\t\tTotal vcore-seconds taken by all map tasks=21841\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=4621\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=22365184\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=4731904\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=5810\n",
      "\t\tMap output records=37571\n",
      "\t\tMap output bytes=1078446\n",
      "\t\tMap output materialized bytes=1153606\n",
      "\t\tInput split bytes=364\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=12441\n",
      "\t\tReduce shuffle bytes=1153606\n",
      "\t\tReduce input records=37571\n",
      "\t\tReduce output records=31861\n",
      "\t\tSpilled Records=75142\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=186\n",
      "\t\tCPU time spent (ms)=4870\n",
      "\t\tPhysical memory (bytes) snapshot=991035392\n",
      "\t\tVirtual memory (bytes) snapshot=6254219264\n",
      "\t\tTotal committed heap usage (bytes)=828375040\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=292685\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1180670\n",
      "16/01/23 09:17:16 INFO streaming.StreamJob: Output directory: /user/miki/week02/hw2_3_output_predict1\n"
     ]
    }
   ],
   "source": [
    "# Change permissions on mapper and reducer\n",
    "!chmod +x mapper.py\n",
    "!chmod +x reducer.py\n",
    "\n",
    "# Delete output folder if it exists\n",
    "!hdfs dfs -rm -r /user/miki/week02/hw2_3_output_predict1\n",
    "\n",
    "# Run job\n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-D stream.map.output.field.separator=^ \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D mapreduce.map.output.key.field.separator=^ \\\n",
    "-D mapreduce.partition.keypartitioner.options=-k1,1 \\\n",
    "-input /user/miki/week02/enronemail_1h.txt \\\n",
    "-input /user/miki/week02/hw2_3_output_fit \\\n",
    "-output /user/miki/week02/hw2_3_output_predict1 \\\n",
    "-mapper /home/cloudera/Documents/W261-Fall2016/Week02/mapper.py \\\n",
    "-reducer /home/cloudera/Documents/W261-Fall2016/Week02/reducer.py \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!hdfs dfs -cat /user/miki/week02/hw2_3_output_predict1/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapper/Reducer #2 for predicting NB\n",
    "\n",
    "Mapper:\n",
    "- Input from predict1: (doc_id, cat, word, spam count, ham count)\n",
    "- Output: identity\n",
    "\n",
    "Reducer:\n",
    "- Input: (doc_id, cat, word, spam count, ham count)\n",
    "- Output: (doc_id, cat, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: mapper code for HW2.3\n",
    "\n",
    "import sys\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    \n",
    "    # Replace delimiter\n",
    "    line = line.replace('\\n', '')\n",
    "    fields = line.split('\\t')\n",
    "    print '%s^%s^%s^%s^%s' % (fields[0], fields[1], fields[2], fields[3], fields[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: reducer code for HW2.3\n",
    "\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "import math\n",
    "import re\n",
    "\n",
    "# Initialize some variables\n",
    "prev_doc = None\n",
    "prev_spam = None\n",
    "prev_spam_count = 0\n",
    "prev_ham_count = 0\n",
    "prev_word = None\n",
    "\n",
    "docs_total = 0\n",
    "docs = {'1':0, '0':0}\n",
    "words_total = 0\n",
    "words = {'1':0, '0':0}\n",
    "log_prior = {'1':0, '0':0}\n",
    "log_posterior = {'1':0, '0':0}\n",
    "log_likelihood = {'1':0, '0':0}\n",
    "log_evidence = 0\n",
    "\n",
    "num_errors = 0.0\n",
    "num_total = 0.0\n",
    "num_correct = 0.0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # Strip and split line\n",
    "    # Assign variables\n",
    "    line = line.replace('\\n', '')\n",
    "    key, value = line.strip().split('\\t')\n",
    "    doc, spam, word = key.split('^')\n",
    "    spam_count, ham_count = value.split('^')\n",
    "    \n",
    "    try:\n",
    "        spam_count = float(spam_count)\n",
    "        ham_count = float(ham_count)\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "    # Let's calculate some log_probs!\n",
    "    if prev_doc == doc:\n",
    "        # We haven't changed documents\n",
    "        if prev_word == word:\n",
    "            # We haven't changed words, so just increment\n",
    "            prev_spam_count += spam_count\n",
    "            prev_ham_count += ham_count\n",
    "            #print \"update counts\"\n",
    "            \n",
    "        else:\n",
    "            # We are at a new word\n",
    "            # We need to check if we are at a keyword\n",
    "            if prev_word == '*alldocs': \n",
    "                # We are at a record where we need to output total docs\n",
    "                docs_total = prev_spam_count\n",
    "                #print \"total docs:\", docs_total\n",
    "            \n",
    "            elif prev_word == '*docs': \n",
    "                # We are at a record where we need to output unique docs per class\n",
    "                docs['1'] = prev_spam_count\n",
    "                docs['0'] = prev_ham_count\n",
    "                for item in log_prior:\n",
    "                    log_prior[item] = math.log(docs[item] / docs_total)\n",
    "                    \n",
    "                    # We will update the posterior after each word\n",
    "                    # Initialize it to the prior\n",
    "                    log_posterior[item] = math.log(docs[item] / docs_total)\n",
    "                #print \"log prior:\", log_prior\n",
    "\n",
    "            elif prev_word == '*words':\n",
    "                # We are at a record where we need to output words per class\n",
    "                words['1'] = prev_spam_count\n",
    "                words['0'] = prev_ham_count\n",
    "                words_total = prev_spam_count + prev_ham_count\n",
    "                #print \"word count:\", words\n",
    "\n",
    "            elif prev_word:\n",
    "                # We are at a new normal word, and need to calculate stuff\n",
    "                try:\n",
    "                    log_likelihood['1'] = math.log(prev_spam_count / words['1'])\n",
    "                    #print word, \"log likelihood spam for\", word, log_likelihood['1']\n",
    "                    \n",
    "                except:\n",
    "                    #print \"error calculating log likelihood spam for\", word\n",
    "                    num_errors += 1\n",
    "                    \n",
    "                try:\n",
    "                    log_likelihood['0'] = math.log(prev_ham_count / words['0'])\n",
    "                    #print word, \"log likelihood ham for\", word, log_likelihood['0']\n",
    "                    \n",
    "                except:\n",
    "                    #print \"error calculating log likelihood ham for\", word\n",
    "                    num_errors += 1\n",
    "                    \n",
    "                # Calculate evidence\n",
    "                log_evidence = math.log((prev_spam_count + prev_ham_count)/words_total)\n",
    "                for item in log_posterior:\n",
    "                    log_posterior[item] += log_likelihood[item] - log_evidence\n",
    "                #print \"updated log posterior:\", log_posterior\n",
    "                \n",
    "                \n",
    "            prev_word = word\n",
    "            prev_spam_count = spam_count\n",
    "            prev_ham_count = ham_count\n",
    "            \n",
    "    else:\n",
    "        # We are done with one document, and need to output our predictions\n",
    "        if prev_doc:\n",
    "            # We know that we're not on the very first record\n",
    "            if log_posterior['1'] > log_posterior['0']: prediction = '1'\n",
    "            else: prediction = '0'\n",
    "            num_total += 1\n",
    "            if prev_spam == prediction: num_correct += 1\n",
    "            print '%s\\t%s\\t%s\\t%s\\t%s' % (prev_doc, prev_spam, prediction, \n",
    "                                          log_posterior['1'],\n",
    "                                          log_posterior['0'])\n",
    "        \n",
    "        prev_doc = doc\n",
    "        prev_spam = spam\n",
    "        prev_word = word\n",
    "        prev_spam_count = spam_count\n",
    "        prev_ham_count = ham_count\n",
    "\n",
    "# Output our final prediction\n",
    "if log_posterior['1'] > log_posterior['0']: prediction = '1'\n",
    "else: prediction = '0'\n",
    "num_total += 1\n",
    "if prev_spam == prediction: num_correct += 1\n",
    "print '%s\\t%s\\t%s\\t%s\\t%s' % (prev_doc, prev_spam, prediction, \n",
    "                              log_posterior['1'],\n",
    "                              log_posterior['0'])\n",
    "print '\\n'\n",
    "print \"Number of documents:\", num_total\n",
    "print \"Number correct predictions:\", num_correct\n",
    "print \"Accuracy:\", num_correct / num_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 12:00:16 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/miki/week02/hw2_3_output_predict2\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.5.0.jar] /tmp/streamjob2504384993510058722.jar tmpDir=null\n",
      "16/01/23 12:00:20 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/23 12:00:20 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/23 12:00:21 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/23 12:00:21 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/01/23 12:00:21 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1453405632837_0201\n",
      "16/01/23 12:00:22 INFO impl.YarnClientImpl: Submitted application application_1453405632837_0201\n",
      "16/01/23 12:00:22 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1453405632837_0201/\n",
      "16/01/23 12:00:22 INFO mapreduce.Job: Running job: job_1453405632837_0201\n",
      "16/01/23 12:00:28 INFO mapreduce.Job: Job job_1453405632837_0201 running in uber mode : false\n",
      "16/01/23 12:00:28 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/23 12:00:36 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/01/23 12:00:37 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/23 12:00:43 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/23 12:00:44 INFO mapreduce.Job: Job job_1453405632837_0201 completed successfully\n",
      "16/01/23 12:00:44 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1244398\n",
      "\t\tFILE: Number of bytes written=2831887\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1185032\n",
      "\t\tHDFS: Number of bytes written=5541\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=11673\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4516\n",
      "\t\tTotal time spent by all map tasks (ms)=11673\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4516\n",
      "\t\tTotal vcore-seconds taken by all map tasks=11673\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=4516\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=11953152\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=4624384\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31861\n",
      "\t\tMap output records=31861\n",
      "\t\tMap output bytes=1180670\n",
      "\t\tMap output materialized bytes=1244404\n",
      "\t\tInput split bytes=266\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=15638\n",
      "\t\tReduce shuffle bytes=1244404\n",
      "\t\tReduce input records=31861\n",
      "\t\tReduce output records=105\n",
      "\t\tSpilled Records=63722\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=157\n",
      "\t\tCPU time spent (ms)=4090\n",
      "\t\tPhysical memory (bytes) snapshot=712200192\n",
      "\t\tVirtual memory (bytes) snapshot=4683845632\n",
      "\t\tTotal committed heap usage (bytes)=645398528\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1184766\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=5541\n",
      "16/01/23 12:00:44 INFO streaming.StreamJob: Output directory: /user/miki/week02/hw2_3_output_predict2\n"
     ]
    }
   ],
   "source": [
    "# Change permissions on mapper and reducer\n",
    "!chmod +x mapper.py\n",
    "!chmod +x reducer.py\n",
    "\n",
    "# Delete output folder if it exists\n",
    "!hdfs dfs -rm -r /user/miki/week02/hw2_3_output_predict2\n",
    "\n",
    "# Run job\n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-D stream.map.output.field.separator=^ \\\n",
    "-D stream.num.map.output.key.fields=3 \\\n",
    "-D mapreduce.map.output.key.field.separator=^ \\\n",
    "-D mapreduce.partition.keypartitioner.options=-k1,2 \\\n",
    "-input /user/miki/week02/hw2_3_output_predict1/part* \\\n",
    "-output /user/miki/week02/hw2_3_output_predict2 \\\n",
    "-mapper /home/cloudera/Documents/W261-Fall2016/Week02/mapper.py \\\n",
    "-reducer /home/cloudera/Documents/W261-Fall2016/Week02/reducer.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0001.1999-12-10.farmer\t0\t1\t0.297532503765\t-0.571137978868\r\n",
      "0001.1999-12-10.kaminski\t0\t0\t0.113204382744\t1.26385376709\r\n",
      "0001.2000-01-17.beck\t0\t1\t169.543130008\t90.3123970245\r\n",
      "0001.2000-06-06.lokay\t0\t1\t78.6474462189\t63.7759129326\r\n",
      "0001.2001-02-07.kitchen\t0\t0\t-4.91174394799\t19.3581092075\r\n",
      "0001.2001-04-02.williams\t0\t1\t92.726947742\t25.1973485387\r\n",
      "0002.1999-12-13.farmer\t0\t1\t165.42887908\t79.7887273931\r\n",
      "0002.2001-02-07.kitchen\t0\t1\t34.6821001409\t9.45843412656\r\n",
      "0002.2001-05-25.SA_and_HP\t1\t0\t16.5257848821\t44.2580237651\r\n",
      "0002.2003-12-18.GP\t1\t0\t37.2185783664\t113.654013859\r\n",
      "0002.2004-08-01.BG\t1\t0\t10.3592327894\t40.7088088645\r\n",
      "0003.1999-12-10.kaminski\t0\t0\t9.59451403488\t15.0385098979\r\n",
      "0003.1999-12-14.farmer\t0\t0\t-2.07230198359\t2.34613600894\r\n",
      "0003.2000-01-17.beck\t0\t0\t5.75351190859\t45.4936466099\r\n",
      "0003.2001-02-08.kitchen\t0\t0\t47.0267714899\t51.0442098234\r\n",
      "0003.2003-12-18.GP\t1\t0\t12.2227879211\t101.829090429\r\n",
      "0003.2004-08-01.BG\t1\t0\t21.9298587399\t83.8623563784\r\n",
      "0004.1999-12-10.kaminski\t0\t1\t98.7061887109\t46.2287321398\r\n",
      "0004.1999-12-14.farmer\t0\t1\t99.2673762463\t39.2164434206\r\n",
      "0004.2001-04-02.williams\t0\t1\t36.65069642\t9.1700734168\r\n",
      "0004.2001-06-12.SA_and_HP\t1\t1\t10.7143395783\t6.61237069189\r\n",
      "0004.2004-08-01.BG\t1\t0\t19.6981437006\t117.314735017\r\n",
      "0005.1999-12-12.kaminski\t0\t1\t97.8844924209\t40.0404516599\r\n",
      "0005.1999-12-14.farmer\t0\t1\t106.527053548\t37.8525143758\r\n",
      "0005.2000-06-06.lokay\t0\t1\t65.4359799298\t15.8765185879\r\n",
      "0005.2001-02-08.kitchen\t0\t1\t75.4056901527\t37.7551075561\r\n",
      "0005.2001-06-23.SA_and_HP\t1\t0\t5.6753860237\t17.1698143565\r\n",
      "0005.2003-12-18.GP\t1\t0\t137.811243519\t585.083911502\r\n",
      "0006.1999-12-13.kaminski\t0\t1\t50.7969164759\t12.3965191484\r\n",
      "0006.2001-02-08.kitchen\t0\t1\t307.973749747\t208.350287573\r\n",
      "0006.2001-04-03.williams\t0\t1\t11.7418063795\t5.57659693223\r\n",
      "0006.2001-06-25.SA_and_HP\t1\t0\t5.20677238256\t40.7937492924\r\n",
      "0006.2003-12-18.GP\t1\t0\t29.6832752731\t182.588412962\r\n",
      "0006.2004-08-01.BG\t1\t0\t29.4840365512\t157.852748864\r\n",
      "0007.1999-12-13.kaminski\t0\t1\t42.3755000153\t33.7484693414\r\n",
      "0007.1999-12-14.farmer\t0\t1\t50.9493678142\t23.4689365187\r\n",
      "0007.2000-01-17.beck\t0\t0\t47.5855932808\t111.094114624\r\n",
      "0007.2001-02-09.kitchen\t0\t0\t54.995572522\t59.4856341625\r\n",
      "0007.2003-12-18.GP\t1\t0\t41.3509348976\t186.887074418\r\n",
      "0007.2004-08-01.BG\t1\t0\t35.8950959698\t43.467413026\r\n",
      "0008.2001-02-09.kitchen\t0\t1\t430.726316948\t178.002114771\r\n",
      "0008.2001-06-12.SA_and_HP\t1\t1\t10.7143395783\t6.61237069189\r\n",
      "0008.2001-06-25.SA_and_HP\t1\t0\t98.922134771\t257.145056232\r\n",
      "0008.2003-12-18.GP\t1\t0\t23.8256885389\t95.580787103\r\n",
      "0008.2004-08-01.BG\t1\t0\t131.756705533\t623.022493083\r\n",
      "0009.1999-12-13.kaminski\t0\t1\t183.403687485\t94.0853817714\r\n",
      "0009.1999-12-14.farmer\t0\t0\t21.073378802\t21.4805197694\r\n",
      "0009.2000-06-07.lokay\t0\t1\t251.47919903\t99.6050244508\r\n",
      "0009.2001-02-09.kitchen\t0\t1\t227.173434171\t179.30953168\r\n",
      "0009.2001-06-26.SA_and_HP\t1\t0\t27.6897226804\t62.5655944063\r\n",
      "0009.2003-12-18.GP\t1\t0\t30.0655672894\t140.522387476\r\n",
      "0010.1999-12-14.farmer\t0\t1\t161.833021639\t67.7139701548\r\n",
      "0010.1999-12-14.kaminski\t0\t1\t31.0324401212\t8.70325452445\r\n",
      "0010.2001-02-09.kitchen\t0\t0\t87.6663748619\t125.937308163\r\n",
      "0010.2001-06-28.SA_and_HP\t1\t0\t83.7510988471\t151.941822639\r\n",
      "0010.2003-12-18.GP\t1\t0\t-0.221599978731\t4.55392953763\r\n",
      "0010.2004-08-01.BG\t1\t0\t59.7584814285\t239.214923046\r\n",
      "0011.1999-12-14.farmer\t0\t1\t92.4765549144\t39.585889989\r\n",
      "0011.2001-06-28.SA_and_HP\t1\t0\t83.7510988471\t153.200283629\r\n",
      "0011.2001-06-29.SA_and_HP\t1\t0\t220.064909029\t665.257365372\r\n",
      "0011.2003-12-18.GP\t1\t0\t13.4121267629\t47.8279173387\r\n",
      "0011.2004-08-01.BG\t1\t0\t19.5098627549\t84.053123125\r\n",
      "0012.1999-12-14.farmer\t0\t1\t107.203682004\t93.0891529747\r\n",
      "0012.1999-12-14.kaminski\t0\t1\t46.178880566\t30.5004059974\r\n",
      "0012.2000-01-17.beck\t0\t0\t46.2421400274\t108.777823764\r\n",
      "0012.2000-06-08.lokay\t0\t1\t75.7390979568\t10.2661818667\r\n",
      "0012.2001-02-09.kitchen\t0\t1\t59.0918553883\t17.1457916736\r\n",
      "0012.2003-12-19.GP\t1\t0\t4.87333548434\t5.13822370551\r\n",
      "0013.1999-12-14.farmer\t0\t0\t28.4002562305\t52.5157394552\r\n",
      "0013.1999-12-14.kaminski\t0\t0\t-1.03073769879\t27.4159126062\r\n",
      "0013.2001-04-03.williams\t0\t1\t39.639722119\t17.610258053\r\n",
      "0013.2001-06-30.SA_and_HP\t1\t0\t322.877334058\t1044.3520427\r\n",
      "0013.2004-08-01.BG\t1\t0\t32.3048485856\t191.594256457\r\n",
      "0014.1999-12-14.kaminski\t0\t1\t36.9279275257\t31.3543863222\r\n",
      "0014.1999-12-15.farmer\t0\t1\t78.3484054723\t43.3916795249\r\n",
      "0014.2001-02-12.kitchen\t0\t1\t129.50701609\t57.273420867\r\n",
      "0014.2001-07-04.SA_and_HP\t1\t0\t57.7653477792\t221.253191977\r\n",
      "0014.2003-12-19.GP\t1\t0\t1.81604296129\t17.8072416793\r\n",
      "0014.2004-08-01.BG\t1\t0\t22.0092608235\t128.520067529\r\n",
      "0015.1999-12-14.kaminski\t0\t1\t91.1204787477\t26.4427598743\r\n",
      "0015.1999-12-15.farmer\t0\t1\t53.2390816336\t32.4748017609\r\n",
      "0015.2000-06-09.lokay\t0\t1\t10.5070993788\t3.93904848208\r\n",
      "0015.2001-02-12.kitchen\t0\t1\t132.372830787\t113.198894713\r\n",
      "0015.2001-07-05.SA_and_HP\t1\t0\t15.9430498783\t37.2048613213\r\n",
      "0015.2003-12-19.GP\t1\t0\t29.5753973753\t79.4416866245\r\n",
      "0016.1999-12-15.farmer\t0\t0\t10.0119706608\t16.0099517356\r\n",
      "0016.2001-02-12.kitchen\t0\t1\t69.4278523761\t47.2516610756\r\n",
      "0016.2001-07-05.SA_and_HP\t1\t0\t15.9430498783\t37.2048613213\r\n",
      "0016.2001-07-06.SA_and_HP\t1\t0\t229.412500429\t479.196989984\r\n",
      "0016.2003-12-19.GP\t1\t0\t29.8880719352\t180.17019242\r\n",
      "0016.2004-08-01.BG\t1\t0\t11.5576599416\t19.8304146801\r\n",
      "0017.1999-12-14.kaminski\t0\t1\t32.0050389938\t4.46252349026\r\n",
      "0017.2000-01-17.beck\t0\t0\t59.9254726255\t109.646427869\r\n",
      "0017.2001-04-03.williams\t0\t1\t18.9905947629\t8.86356505632\r\n",
      "0017.2003-12-18.GP\t1\t0\t4.69023903519\t20.7354713243\r\n",
      "0017.2004-08-01.BG\t1\t0\t45.8804043982\t320.673290761\r\n",
      "0017.2004-08-02.BG\t1\t0\t36.2615688518\t179.747118823\r\n",
      "0018.1999-12-14.kaminski\t0\t1\t50.4509964836\t12.1283207572\r\n",
      "0018.2001-07-13.SA_and_HP\t1\t0\t52.2970813731\t169.461804351\r\n",
      "0018.2003-12-18.GP\t1\t0\t51.7296513995\t266.331989139\r\n",
      "\t\r\n",
      "\t\r\n",
      "Number of documents: 100.0\t\r\n",
      "Number correct predictions: 17.0\t\r\n",
      "Accuracy: 0.17\t\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/miki/week02/hw2_3_output_predict2/part-00000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
