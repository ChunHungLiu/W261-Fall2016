{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0001.1999-12-10.farmer\t0\t0\t-inf\t-45.2019719279\n",
      "\n",
      "0001.1999-12-10.kaminski\t0\t0\t-inf\t-29.2849813805\n",
      "\n",
      "0001.2000-01-17.beck\t0\t0\t-inf\t-3185.18745489\n",
      "\n",
      "0001.2000-06-06.lokay\t0\t0\t-inf\t-3441.47340135\n",
      "\n",
      "0001.2001-02-07.kitchen\t0\t0\t-inf\t-296.797357693\n",
      "\n",
      "0001.2001-04-02.williams\t0\t0\t-inf\t-1334.94836616\n",
      "\n",
      "0002.1999-12-13.farmer\t0\t0\t-inf\t-1750.40861103\n",
      "\n",
      "0002.2001-02-07.kitchen\t0\t0\t-inf\t-441.700059468\n",
      "\n",
      "0002.2001-05-25.SA_and_HP\t1\t1\t-522.550997486\t-inf\n",
      "\n",
      "0002.2003-12-18.GP\t1\t1\t-1167.96499392\t-inf\n",
      "\n",
      "0002.2004-08-01.BG\t1\t1\t-834.682696876\t-inf\n",
      "\n",
      "0003.1999-12-10.kaminski\t0\t0\t-inf\t-334.7570576\n",
      "\n",
      "0003.1999-12-14.farmer\t0\t0\t-inf\t-71.434062621\n",
      "\n",
      "0003.2000-01-17.beck\t0\t0\t-inf\t-1192.06226701\n",
      "\n",
      "0003.2001-02-08.kitchen\t0\t0\t-inf\t-1187.79836064\n",
      "\n",
      "0003.2003-12-18.GP\t1\t1\t-850.351820212\t-inf\n",
      "\n",
      "0003.2004-08-01.BG\t1\t1\t-753.616687991\t-inf\n",
      "\n",
      "0004.1999-12-10.kaminski\t0\t0\t-inf\t-931.048480333\n",
      "\n",
      "0004.1999-12-14.farmer\t0\t0\t-inf\t-819.377722954\n",
      "\n",
      "0004.2001-04-02.williams\t0\t0\t-inf\t-609.666885187\n",
      "\n",
      "0004.2001-06-12.SA_and_HP\t1\t1\t-831.29509553\t-inf\n",
      "\n",
      "0004.2004-08-01.BG\t1\t1\t-679.195992381\t-inf\n",
      "\n",
      "0005.1999-12-12.kaminski\t0\t0\t-inf\t-756.536216694\n",
      "\n",
      "0005.1999-12-14.farmer\t0\t0\t-inf\t-849.971034835\n",
      "\n",
      "0005.2000-06-06.lokay\t0\t0\t-inf\t-413.005685986\n",
      "\n",
      "0005.2001-02-08.kitchen\t0\t0\t-inf\t-759.046352118\n",
      "\n",
      "0005.2001-06-23.SA_and_HP\t1\t1\t-164.775456041\t-inf\n",
      "\n",
      "0005.2003-12-18.GP\t1\t1\t-6874.60898405\t-inf\n",
      "\n",
      "0006.1999-12-13.kaminski\t0\t0\t-inf\t-452.593716842\n",
      "\n",
      "0006.2001-02-08.kitchen\t0\t0\t-inf\t-9071.43250786\n",
      "\n",
      "0006.2001-04-03.williams\t0\t0\t-inf\t-299.745378771\n",
      "\n",
      "0006.2001-06-25.SA_and_HP\t1\t1\t-309.872318556\t-inf\n",
      "\n",
      "0006.2003-12-18.GP\t1\t1\t-1111.95982955\t-inf\n",
      "\n",
      "0006.2004-08-01.BG\t1\t1\t-1026.18855558\t-inf\n",
      "\n",
      "0007.1999-12-13.kaminski\t0\t0\t-inf\t-1365.36734905\n",
      "\n",
      "0007.1999-12-14.farmer\t0\t0\t-inf\t-619.570402378\n",
      "\n",
      "0007.2000-01-17.beck\t0\t0\t-inf\t-2572.28316796\n",
      "\n",
      "0007.2001-02-09.kitchen\t0\t0\t-inf\t-1602.14234885\n",
      "\n",
      "0007.2003-12-18.GP\t1\t1\t-1227.84273701\t-inf\n",
      "\n",
      "0007.2004-08-01.BG\t1\t1\t-1163.5032243\t-inf\n",
      "\n",
      "0008.2001-02-09.kitchen\t0\t0\t-inf\t-3762.6825163\n",
      "\n",
      "0008.2001-06-12.SA_and_HP\t1\t1\t-831.29509553\t-inf\n",
      "\n",
      "0008.2001-06-25.SA_and_HP\t1\t1\t-4028.18477973\t-inf\n",
      "\n",
      "0008.2003-12-18.GP\t1\t1\t-1009.088985\t-inf\n",
      "\n",
      "0008.2004-08-01.BG\t1\t1\t-5841.4793088\t-inf\n",
      "\n",
      "0009.1999-12-13.kaminski\t0\t0\t-inf\t-5125.61768154\n",
      "\n",
      "0009.1999-12-14.farmer\t0\t0\t-inf\t-371.038660103\n",
      "\n",
      "0009.2000-06-07.lokay\t0\t0\t-inf\t-2656.29063253\n",
      "\n",
      "0009.2001-02-09.kitchen\t0\t0\t-inf\t-5330.15967071\n",
      "\n",
      "0009.2001-06-26.SA_and_HP\t1\t1\t-933.286435349\t-inf\n",
      "\n",
      "0009.2003-12-18.GP\t1\t1\t-763.745091736\t-inf\n",
      "\n",
      "0010.1999-12-14.farmer\t0\t0\t-inf\t-998.513747461\n",
      "\n",
      "0010.1999-12-14.kaminski\t0\t0\t-inf\t-200.984121648\n",
      "\n",
      "0010.2001-02-09.kitchen\t0\t0\t-inf\t-2809.09386716\n",
      "\n",
      "0010.2001-06-28.SA_and_HP\t1\t1\t-3317.62238469\t-inf\n",
      "\n",
      "0010.2003-12-18.GP\t1\t1\t-55.8442713705\t-inf\n",
      "\n",
      "0010.2004-08-01.BG\t1\t1\t-1877.12000256\t-inf\n",
      "\n",
      "0011.1999-12-14.farmer\t0\t0\t-inf\t-1771.3213928\n",
      "\n",
      "0011.2001-06-28.SA_and_HP\t1\t1\t-3309.88607759\t-inf\n",
      "\n",
      "0011.2001-06-29.SA_and_HP\t1\t1\t-15251.9932327\t-inf\n",
      "\n",
      "0011.2003-12-18.GP\t1\t1\t-499.519274752\t-inf\n",
      "\n",
      "0011.2004-08-01.BG\t1\t1\t-676.375627758\t-inf\n",
      "\n",
      "0012.1999-12-14.farmer\t0\t0\t-inf\t-2984.66985343\n",
      "\n",
      "0012.1999-12-14.kaminski\t0\t0\t-inf\t-738.277072616\n",
      "\n",
      "0012.2000-01-17.beck\t0\t0\t-inf\t-2563.28061447\n",
      "\n",
      "0012.2000-06-08.lokay\t0\t0\t-inf\t-902.72251937\n",
      "\n",
      "0012.2001-02-09.kitchen\t0\t0\t-inf\t-440.09395379\n",
      "\n",
      "0012.2003-12-19.GP\t1\t1\t-129.714352162\t-inf\n",
      "\n",
      "0013.1999-12-14.farmer\t0\t0\t-inf\t-1733.00027038\n",
      "\n",
      "0013.1999-12-14.kaminski\t0\t0\t-inf\t-998.901610691\n",
      "\n",
      "0013.2001-04-03.williams\t0\t0\t-inf\t-642.781670849\n",
      "\n",
      "0013.2001-06-30.SA_and_HP\t1\t1\t-27383.6229056\t-inf\n",
      "\n",
      "0013.2004-08-01.BG\t1\t1\t-1472.41224013\t-inf\n",
      "\n",
      "0014.1999-12-14.kaminski\t0\t0\t-inf\t-1505.00347858\n",
      "\n",
      "0014.1999-12-15.farmer\t0\t0\t-inf\t-872.772164383\n",
      "\n",
      "0014.2001-02-12.kitchen\t0\t0\t-inf\t-1143.32609159\n",
      "\n",
      "0014.2001-07-04.SA_and_HP\t1\t1\t-3138.81368607\t-inf\n",
      "\n",
      "0014.2003-12-19.GP\t1\t1\t-130.727406886\t-inf\n",
      "\n",
      "0014.2004-08-01.BG\t1\t1\t-786.540750288\t-inf\n",
      "\n",
      "0015.1999-12-14.kaminski\t0\t0\t-inf\t-485.192582863\n",
      "\n",
      "0015.1999-12-15.farmer\t0\t0\t-inf\t-521.266322421\n",
      "\n",
      "0015.2000-06-09.lokay\t0\t0\t-inf\t-120.613897384\n",
      "\n",
      "0015.2001-02-12.kitchen\t0\t0\t-inf\t-4827.12947125\n",
      "\n",
      "0015.2001-07-05.SA_and_HP\t1\t1\t-834.366441096\t-inf\n",
      "\n",
      "0015.2003-12-19.GP\t1\t1\t-1146.65030976\t-inf\n",
      "\n",
      "0016.1999-12-15.farmer\t0\t0\t-inf\t-638.72260642\n",
      "\n",
      "0016.2001-02-12.kitchen\t0\t0\t-inf\t-1032.39173733\n",
      "\n",
      "0016.2001-07-05.SA_and_HP\t1\t1\t-834.366441096\t-inf\n",
      "\n",
      "0016.2001-07-06.SA_and_HP\t1\t1\t-15945.7157236\t-inf\n",
      "\n",
      "0016.2003-12-19.GP\t1\t1\t-785.179398318\t-inf\n",
      "\n",
      "0016.2004-08-01.BG\t1\t1\t-640.289568638\t-inf\n",
      "\n",
      "0017.1999-12-14.kaminski\t0\t0\t-inf\t-370.573258464\n",
      "\n",
      "0017.2000-01-17.beck\t0\t0\t-inf\t-2574.38112574\n",
      "\n",
      "0017.2001-04-03.williams\t0\t0\t-inf\t-433.805569602\n",
      "\n",
      "0017.2003-12-18.GP\t1\t1\t-201.512972271\t-inf\n",
      "\n",
      "0017.2004-08-01.BG\t1\t1\t-904.393827439\t-inf\n",
      "\n",
      "0017.2004-08-02.BG\t1\t1\t-2348.29610174\t-inf\n",
      "\n",
      "0018.1999-12-14.kaminski\t0\t0\t-inf\t-904.60021141\n",
      "\n",
      "0018.2001-07-13.SA_and_HP\t1\t1\t-3244.29283897\t-inf\n",
      "\n",
      "0018.2003-12-18.GP\t1\t1\t-3344.80041028\t-inf\n",
      "\n",
      "Number of documents\t100\n",
      "Number correct predictions\t100\n",
      "Error rate\t0.0%\n",
      "Number of zero probability spam\t2927\n",
      "Number of zero probability ham\t3991\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: reducer code for HW2.3\n",
    "\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "import math\n",
    "import re\n",
    "import decimal\n",
    "\n",
    "# Initialize some variables\n",
    "doc = None\n",
    "spam = None\n",
    "count = 1\n",
    "class_count = {'1':0, '0':0}\n",
    "word = None\n",
    "\n",
    "prev_doc = None\n",
    "prev_spam = None\n",
    "prev_count = 1\n",
    "prev_class_count = {'1':0, '0':0}\n",
    "prev_word = None\n",
    "\n",
    "docs_total = 0\n",
    "docs = {'1':0, '0':0}\n",
    "words_total = 0\n",
    "words = {'1':0, '0':0}\n",
    "log_prior = {'1':0, '0':0}\n",
    "log_posterior = {'1':0, '0':0}\n",
    "log_likelihood = {'1':0, '0':0}\n",
    "\n",
    "classes = {'1':'spam', '0':'ham'}\n",
    "num_errors = {'1':0, '0':0}\n",
    "num_total = 0.0\n",
    "num_correct = 0.0\n",
    "\n",
    "print_debug = False\n",
    "\n",
    "# Create a function to update the posterior\n",
    "# since we need to do it in multiple locations.\n",
    "# We don't want to duplicate code\n",
    "def update_posterior():\n",
    "    # Calculate evidence\n",
    "    if print_debug:\n",
    "        print \"times word occured:\", prev_count\n",
    "\n",
    "    for item in classes:\n",
    "        if prev_class_count[item] > 0 and log_likelihood[item] != float('-inf'):\n",
    "            log_likelihood[item] = math.log(prev_class_count[item] / words[item])\n",
    "            log_posterior[item] += prev_count * log_likelihood[item]\n",
    "            if print_debug:\n",
    "                print \"updated log posterior:\", log_posterior[item]\n",
    "\n",
    "        else:\n",
    "            if print_debug:\n",
    "                print \"zero probability, set log_post to -inf for\", classes[item]\n",
    "                print '\\n'\n",
    "            log_posterior[item] = float('-inf')\n",
    "            num_errors[item] += 1\n",
    "        \n",
    "\n",
    "# Create a function to avoid code duplication\n",
    "def make_prediction(): \n",
    "    global num_total, num_correct\n",
    "    \n",
    "    # We can compare non-normalized posterior probabilities\n",
    "    num_total += 1\n",
    "    if log_posterior['1'] > log_posterior['0']: prediction = '1'\n",
    "    else: prediction = '0'\n",
    "\n",
    "    # Count correct guesses\n",
    "    if prev_spam == prediction:\n",
    "        num_correct += 1\n",
    "        \n",
    "    # Output the log posteriors. We can normalize later.\n",
    "    print '%s\\t%s\\t%s\\t%s\\t%s\\n' % (prev_doc, prev_spam, prediction, \n",
    "                                    log_posterior['1'],\n",
    "                                    log_posterior['0'])\n",
    "\n",
    "myfile = open('predict1_output.txt', 'r')\n",
    "for line in myfile:\n",
    "    # Remove end of line\n",
    "    line = line.replace('\\n', '')\n",
    "    \n",
    "    # Split words when running locally\n",
    "    doc, spam, word, class_count['1'], class_count['0'] = line.split('\\t')\n",
    "    \n",
    "    # Split words when running in Hadoop\n",
    "#     key, value = line.strip().split('\\t')\n",
    "#     doc, spam, word = key.split('^')\n",
    "#     class_count['1'], class_count['0'] = value.split('^')    \n",
    "\n",
    "    # Keep this in a try/except statement so we don't fail\n",
    "    try:\n",
    "        for item in classes:\n",
    "            class_count[item] = float(class_count[item])\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "    # Let's calculate some probabilities\n",
    "    if prev_doc == doc:\n",
    "        # We haven't changed documents\n",
    "        if prev_word == word:\n",
    "            # We haven't changed words, so just increment\n",
    "            prev_count += 1\n",
    "\n",
    "        else:\n",
    "            # We are at a new word\n",
    "            # We need to check if we are at a keyword\n",
    "            if print_debug: print '\\n', prev_word, '\\n'\n",
    "            if prev_word == '*alldocs': \n",
    "                # We are at a record where we need to output total docs\n",
    "                docs_total = prev_class_count['1']\n",
    "                if print_debug: print \"total docs:\", docs_total\n",
    "\n",
    "            elif prev_word == '*docs': \n",
    "                # We are at a record where we need to output unique docs per class\n",
    "                for item in classes:\n",
    "                    docs[item] = prev_class_count[item]\n",
    "                    if print_debug: print \"prior\", item, docs[item], '/', docs_total\n",
    "                    log_prior[item] = math.log(docs[item] / docs_total)\n",
    "\n",
    "                    # We will update the posterior after each word\n",
    "                    # Initialize it to the prior\n",
    "                    log_posterior[item] = log_prior[item]\n",
    "                if print_debug: \n",
    "                    print \"log prior:\", log_prior\n",
    "                    print 'log posterior initial', log_posterior\n",
    "\n",
    "            elif prev_word == '*words':\n",
    "                # We are at a record where we need to output words per class\n",
    "                for item in classes:\n",
    "                    words[item] = prev_class_count[item]\n",
    "                words_total = sum(prev_class_count.values())\n",
    "                if print_debug: print \"word class_count:\", words\n",
    "\n",
    "            elif prev_word:\n",
    "                # We are at a new normal word, and need to calculate stuff\n",
    "                update_posterior()\n",
    "\n",
    "            prev_word = word\n",
    "            prev_count = 1\n",
    "            for item in classes:\n",
    "                prev_class_count[item] = class_count[item]\n",
    "\n",
    "    else:\n",
    "        # We are done with one document. We need to: \n",
    "        # - process the last word\n",
    "        # - output our predictions\n",
    "        if prev_doc:\n",
    "            if print_debug: print '\\n', prev_word, '\\n'\n",
    "            # We are at a new normal word, and need to calculate stuff\n",
    "            update_posterior()\n",
    "\n",
    "            # Now we can calculate the prediction\n",
    "            make_prediction()\n",
    "            if print_debug: print num_correct, \"out of\", num_total\n",
    "\n",
    "        prev_doc = doc\n",
    "        prev_spam = spam\n",
    "        prev_word = word\n",
    "        for item in classes:\n",
    "            prev_class_count[item] = class_count[item]\n",
    "        log_likelihood = {'1':0, '0':0}\n",
    "        if print_debug: print \"reset log likelihood\"\n",
    "\n",
    "# Output our final prediction\n",
    "if print_debug: print '\\n', prev_word, '\\n'\n",
    "update_posterior()\n",
    "make_prediction()\n",
    "\n",
    "print \"Number of documents\\t%d\" % (num_total)\n",
    "print \"Number correct predictions\\t%d\" % (num_correct)\n",
    "print \"Error rate\\t%s\" % (100 - 100 * num_correct / num_total) + \"%\"\n",
    "print \"Number of zero probability spam\\t%d\" % (num_errors['1'])\n",
    "print \"Number of zero probability ham\\t%d\" % (num_errors['0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.exp(float('-inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decimal('1.799494017011657230805087954E-382')"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from decimal import *\n",
    "Decimal(-879).exp()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
