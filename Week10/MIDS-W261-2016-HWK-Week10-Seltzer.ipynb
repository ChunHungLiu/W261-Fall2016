{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASCI W261: Machine Learning at Scale\n",
    "## Assignment Week 10\n",
    "Miki Seltzer (miki.seltzer@berkeley.edu)<br>\n",
    "W261-2, Spring 2016<br>\n",
    "Submission: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 1.5.0-cdh5.5.0\n",
      "      /_/\n",
      "\n",
      "Using Python version 2.7.11 (default, Dec  6 2015 18:08:32)\n",
      "SparkContext available as sc, HiveContext available as sqlContext.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME enviroment variable is not set')\n",
    "sys.path.insert(0,os.path.join(spark_home,'python'))\n",
    "sys.path.insert(0,os.path.join(spark_home,'python/lib/py4j-0.8.2.1-src.zip'))\n",
    "execfile(os.path.join(spark_home,'python/pyspark/shell.py'))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# HW 10.0\n",
    "## What is Apache Spark and how is it different to Apache Hadoop? \n",
    "\n",
    "Spark is a much faster processing engine than we have currently been using. It can run on top of Hadoop clusters, or in standalone mode. It is built to use data that is not only stored in HDFS, but also in Hadoop-based systems like HBase, Hive, Cassandra, etc. It can also ingest local text files, CSV files, etc.\n",
    "\n",
    "Spark's main difference from Hadoop is that processing in Spark is done in memory via RDD (Resilient Distributed Datasets), which allows iterative jobs to be completed much more efficiently.\n",
    "\n",
    "## Fill in the blanks:\n",
    "Spark API consists of interfaces to develop applications based on it in Java, __Python, Scala__ languages (list languages). \n",
    "\n",
    "Using Spark, resource management can be done either in a single server instance or using a framework such as Mesos or __YARN__ in a distributed manner.\n",
    "\n",
    "## What is an RDD and show a fun example of creating one and bringing the first element back to the driver program.\n",
    "\n",
    "An RDD is a resilient distributed dataset with the following properties:\n",
    "- Distributed storage\n",
    "- Lazy evaluation\n",
    "- Can have transformations (similar to mappers) and actions (similar to reducers)\n",
    "- RDD is not materialized until an action is called (lazy evaluation)\n",
    "- Results of an action can be cached/persisted so that they are not repetitively processed\n",
    "- Can be a base RDD (only values) or a pair RDD (key-value pairs)\n",
    "\n",
    "#### Example:\n",
    "```\n",
    "sc = SparkContext(\"local\", \"Example\")\n",
    "data = sc.textFile('some_text_file.txt').cache()\n",
    "firstWithData = data.filter(lambda x: 'data' in x).first()\n",
    "```\n",
    "\n",
    "## What is lazy evaluation and give an intuitive example of lazy evaluation and comment on the massive computational savings to be had from lazy evaluation.\n",
    "\n",
    "Lazy evaluation means that an RDD is not materialized until an action is called. Thus, when the RDD is loaded and transformed, the only thing that is really being done is to store the instructions on how the data should be constructed when an action is called. There are computational savings to be had by doing lazy evaluations because the data is not actually materialized until a result needs to be sent back to the driver.\n",
    "\n",
    "#### Example:\n",
    ">When working with the 2GB Wikipedia data, if we load the data into an RDD, this should be instantaneous, because we haven't actually performed any actions on it. We can write many transformations and this will all be nearly instantaneous as well, because nothing has been materialized. However, when we want to see the results of our transformation, we can call an action like take(10), and instead of running the transformations on the entire data set, it will only process enough data to return those 10 results that we requested in the action.\n",
    "\n",
    ">For comparison, if we wanted to do this in Hadoop, we would have had to process the entire 2GB file, which is computationally expensive if we only needed to see 10 resulting records from the data set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW10.1\n",
    "In Spark write the code to count how often each word appears in a text document (or set of documents). Please use this homework document as a the example document to run an experiment.  Report the following: provide a sorted list of tokens in decreasing order of frequency of occurence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -copyFromLocal MIDS-MLS-HW-10.txt /user/miki/week10/hw10_1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count  word      \n",
      "--------------------\n",
      "   46  the       \n",
      "   24  and       \n",
      "   20  in        \n",
      "   17  of        \n",
      "   13  hw        \n",
      "   12  a         \n",
      "   11  for       \n",
      "   11  data      \n",
      "   11  using     \n",
      "   10  code      \n",
      "    9  to        \n",
      "    8  this      \n",
      "    8  is        \n",
      "    8  set       \n",
      "    8  your      \n",
      "    8  kmeans    \n",
      "    7  with      \n",
      "    7  on        \n",
      "    6  clusters  \n",
      "    6  x         \n",
      "    6  regression\n",
      "    5  comment   \n",
      "    5  iterations\n",
      "    5  from      \n",
      "    5  as        \n",
      "    5  report    \n",
      "    5  linear    \n",
      "    4  squared   \n",
      "    4  words     \n",
      "    4  hw103     \n",
      "    4  what      \n",
      "    4  sum       \n",
      "    4  please    \n",
      "    4  100       \n",
      "    4  spark     \n",
      "    4  following \n",
      "    4  model     \n",
      "    4  each      \n",
      "    4  provided  \n",
      "    4  plot      \n",
      "    4  one       \n",
      "    4  example   \n",
      "    3  count     \n",
      "    3  list      \n",
      "    3  findings  \n",
      "    3  evaluation\n",
      "    3  available \n",
      "    3  lazy      \n",
      "    3  run       \n",
      "    3  here      \n",
      "    3  training  \n",
      "    3  note      \n",
      "    3  or        \n",
      "    3  results   \n",
      "    3  import    \n",
      "    3  team      \n",
      "    3  notebook  \n",
      "    3  within    \n",
      "    3  plots     \n",
      "    3  it        \n",
      "    3  an        \n",
      "    3  document  \n",
      "    3  after     \n",
      "    3  errors    \n",
      "    2  homeworks \n",
      "    2  snippet   \n",
      "    2  httpswwwdropboxcomsq85t0ytb9apggnhkmeansdatatxtdl0\n",
      "    2  explain   \n",
      "    2  languages \n",
      "    2  homework  \n",
      "    2  lass0     \n",
      "    2  1061      \n",
      "    2  per       \n",
      "    2  modify    \n",
      "    2  apache    \n",
      "    2  optional  \n",
      "    2  load      \n",
      "    2  ridge     \n",
      "    2  word      \n",
      "    2  kmeansdatatxt\n",
      "    2  provide   \n",
      "    2  y         \n",
      "    2  via       \n",
      "    2  how       \n",
      "    2  evaluate  \n",
      "    2  follows   \n",
      "    2  assignments\n",
      "    2  frequency \n",
      "    2  between   \n",
      "    2  be        \n",
      "    2  mymodelpath\n",
      "    2  found     \n",
      "    2  point     \n",
      "    2  2         \n",
      "    2  testing   \n",
      "    2  that      \n",
      "    2  differences\n",
      "    2  10        \n",
      "    2  up        \n",
      "    2  at        \n",
      "    2  generate  \n",
      "    2  vector    \n",
      "    2  cluster   \n",
      "    2  any       \n",
      "    2  fill      \n",
      "    2  mllibs    \n",
      "    2  blanks    \n",
      "    2  assignment\n",
      "    2  repeat    \n",
      "    2  center    \n",
      "    2  points    \n",
      "    2  decreasing\n",
      "    2  order     \n",
      "    1  all       \n",
      "    1  maxiterations10\n",
      "    1  mllibcentric\n",
      "    1  intuitoive\n",
      "    1  questions \n",
      "    1  datsci    \n",
      "    1  03152016  \n",
      "    1  strwssse  \n",
      "    1  group     \n",
      "    1  httpswwwdropboxcoms3nsthvp8g2rrrdhemkmeansipynbdl0\n",
      "    1  distributed\n",
      "    1  weeks     \n",
      "    1  submissions\n",
      "    1  httpsdocsgooglecomformsd1zor9rniea06aczdb6k1mjn4vrlesms2pd6xm3eoiisviewformuspsendform\n",
      "    1  return    \n",
      "    1  not       \n",
      "    1  compute   \n",
      "    1  bringing  \n",
      "    1  runs      \n",
      "    1  resource  \n",
      "    1  exercise  \n",
      "    1  where     \n",
      "    1  datamaplambda\n",
      "    1  generation\n",
      "    1  consists  \n",
      "    1  see       \n",
      "    1  download  \n",
      "    1  hw105     \n",
      "    1  special   \n",
      "    1  gradient  \n",
      "    1  cell      \n",
      "    1  3         \n",
      "    1  machine   \n",
      "    1  above     \n",
      "    1  initializationmoderandom\n",
      "    1  math      \n",
      "    1  interfaces\n",
      "    1  20        \n",
      "    1  fun       \n",
      "    1  based     \n",
      "    1  104       \n",
      "    1  samemodel \n",
      "    1  parameters\n",
      "    1  column    \n",
      "    1  clusterscentersclusterspredictpoint\n",
      "    1  completing\n",
      "    1  sqrtxx    \n",
      "    1  implementation\n",
      "    1  length    \n",
      "    1  resulting \n",
      "    1  uc        \n",
      "    1  letters   \n",
      "    1  mesos     \n",
      "    1  had       \n",
      "    1  102       \n",
      "    1  done      \n",
      "    1  learning  \n",
      "    1  array     \n",
      "    1  clustering\n",
      "    1  use       \n",
      "    1  weight    \n",
      "    1  106       \n",
      "    1  submit    \n",
      "    1  call      \n",
      "    1  forward   \n",
      "    1  numpy     \n",
      "    1  sort      \n",
      "    1  errorpoint\n",
      "    1  form      \n",
      "    1  tokens    \n",
      "    1  pysparkmllibclustering\n",
      "    1  x2        \n",
      "    1  line      \n",
      "    1  clusterssavesc\n",
      "    1  case      \n",
      "    1  iteration \n",
      "    1  single    \n",
      "    1  labeled   \n",
      "    1  norm      \n",
      "    1  carefully \n",
      "    1  def       \n",
      "    1  sqrtsumx2 \n",
      "    1  links     \n",
      "    1  parse     \n",
      "    1  tab       \n",
      "    1  1         \n",
      "    1  different \n",
      "    1  develop   \n",
      "    1  sqrtx12   \n",
      "    1  sqrt      \n",
      "    1  answer    \n",
      "    1  wssse     \n",
      "    1  begin     \n",
      "    1  driver    \n",
      "    1  regularization\n",
      "    1  end       \n",
      "    1  tune      \n",
      "    1  hw10      \n",
      "    1  lower     \n",
      "    1  occurence \n",
      "    1  java      \n",
      "    1  w261      \n",
      "    1  savings   \n",
      "    1  write     \n",
      "    1  algorithms\n",
      "    1  arrayfloatx\n",
      "    1  hyper     \n",
      "    1  httpsdocsgooglecomspreadsheetsd1ncfql5tovn16sld8myjpnzmtpsfigellzw8vsmjgedituspsharing\n",
      "    1  httpswwwdropboxcomsatzqkc0p1eajuz6linearregressionnotebookchallengeipynbdl0\n",
      "    1  linearregressionwithsgd\n",
      "    1  show      \n",
      "    1  v13       \n",
      "    1  kmeansmodelloadsc\n",
      "    1  follow    \n",
      "    1  find      \n",
      "    1  inverse   \n",
      "    1  number    \n",
      "    1  isvc      \n",
      "    1  program   \n",
      "    1  text      \n",
      "    1  save      \n",
      "    1  parseddata\n",
      "    1  good      \n",
      "    1  get       \n",
      "    1  framework \n",
      "    1  made      \n",
      "    1  progress  \n",
      "    1  sorted    \n",
      "    1  dataset   \n",
      "    1  instructions\n",
      "    1  going     \n",
      "    1  homegrown \n",
      "    1  1062      \n",
      "    1  teams     \n",
      "    1  experiments\n",
      "    1  either    \n",
      "    1  output    \n",
      "    1  runs10    \n",
      "    1  often     \n",
      "    1  kmean     \n",
      "    1  do        \n",
      "    1  printwithin\n",
      "    1  back      \n",
      "    1  located   \n",
      "    1  are       \n",
      "    1  measure   \n",
      "    1  experiements\n",
      "    1  server    \n",
      "    1  hw104     \n",
      "    1  scale     \n",
      "    1  experiment\n",
      "    1  weighted  \n",
      "    1  then      \n",
      "    1  1011      \n",
      "    1  x22       \n",
      "    1  creating  \n",
      "    1  berkeley  \n",
      "    1  sctextfilekmeansdatatxt\n",
      "    1  101       \n",
      "    1  by        \n",
      "    1  105       \n",
      "    1  la        \n",
      "    1  rdd       \n",
      "    1  massive   \n",
      "    1  mids      \n",
      "    1  first     \n",
      "    1  computational\n",
      "    1  api       \n",
      "    1  kmeansmodel\n",
      "    1  kmeanstrainparseddata\n",
      "    1  parseddatamaplambda\n",
      "    1  management\n",
      "    1  1x        \n",
      "    1  give      \n",
      "    1  mllib     \n",
      "    1  appears   \n",
      "    1  thru      \n",
      "    1  final     \n",
      "    1  more      \n",
      "    1  train     \n",
      "    1  x1        \n",
      "    1  hundred   \n",
      "    1  work      \n",
      "    1  euclidean \n",
      "    1  can       \n",
      "    1  error     \n",
      "    1  103       \n",
      "    1  computing \n",
      "    1  manner    \n",
      "    1  az        \n",
      "    1  again     \n",
      "    1  documents \n",
      "    1  descent   \n",
      "    1  hadoop    \n",
      "    1  instance  \n",
      "    1  other     \n",
      "    1  build     \n",
      "    1  separate  \n",
      "    1  applications\n",
      "    1  linesplit \n",
      "    1  such      \n",
      "    1  short     \n",
      "    1  errorpointreducelambda\n",
      "    1  sets      \n",
      "    1  element   \n",
      "    1  insturctions\n",
      "    1  dropbox   \n",
      "    1  weightx   \n",
      "    1  justify   \n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "\n",
    "def splitString(line):\n",
    "    noPunc = re.sub(u'[^A-Za-z0-9 ]+', '', line)\n",
    "    return noPunc.strip().split()\n",
    "\n",
    "hwText = sc.textFile('/user/miki/week10/hw10_1/MIDS-MLS-HW-10.txt')\n",
    "wordCounts = hwText.map(lambda x: x.lower()).flatMap(splitString).map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\n",
    "sortedCounts = wordCounts.map(lambda x: (x[1], x[0])).sortByKey(ascending=False).collect()\n",
    "\n",
    "print '{:5s}  {:10s}'.format('count', 'word')\n",
    "print '--------------------'\n",
    "for count, word in sortedCounts:\n",
    "    print '{:5d}  {:10s}'.format(count, word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 10.1.1\n",
    "\n",
    "Modify the above word count code to count words that begin with lower case letters (a-z) and report your findings. Again sort the output words in decreasing order of frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count  word      \n",
      "--------------------\n",
      "   46  the       \n",
      "   24  and       \n",
      "   18  in        \n",
      "   17  of        \n",
      "   12  a         \n",
      "   11  for       \n",
      "   10  data      \n",
      "   10  code      \n",
      "    9  to        \n",
      "    8  is        \n",
      "    7  with      \n",
      "    7  this      \n",
      "    7  on        \n",
      "    7  your      \n",
      "    6  clusters  \n",
      "    5  iterations\n",
      "    5  from      \n",
      "    5  as        \n",
      "    5  regression\n",
      "    4  words     \n",
      "    4  following \n",
      "    4  model     \n",
      "    4  using     \n",
      "    4  each      \n",
      "    4  x         \n",
      "    4  set       \n",
      "    4  linear    \n",
      "    4  one       \n",
      "    4  example   \n",
      "    4  provided  \n",
      "    3  report    \n",
      "    3  list      \n",
      "    3  findings  \n",
      "    3  evaluation\n",
      "    3  available \n",
      "    3  lazy      \n",
      "    3  training  \n",
      "    3  count     \n",
      "    3  results   \n",
      "    3  import    \n",
      "    3  plots     \n",
      "    3  or        \n",
      "    3  plot      \n",
      "    3  it        \n",
      "    3  an        \n",
      "    3  document  \n",
      "    3  after     \n",
      "    3  notebook  \n",
      "    2  httpswwwdropboxcomsq85t0ytb9apggnhkmeansdatatxtdl0\n",
      "    2  languages \n",
      "    2  homework  \n",
      "    2  per       \n",
      "    2  run       \n",
      "    2  here      \n",
      "    2  word      \n",
      "    2  kmeansdatatxt\n",
      "    2  provide   \n",
      "    2  snippet   \n",
      "    2  y         \n",
      "    2  myModelPath\n",
      "    2  follows   \n",
      "    2  vector    \n",
      "    2  testing   \n",
      "    2  frequency \n",
      "    2  between   \n",
      "    2  be        \n",
      "    2  found     \n",
      "    2  team      \n",
      "    2  how       \n",
      "    2  via       \n",
      "    2  point     \n",
      "    2  that      \n",
      "    2  differences\n",
      "    2  up        \n",
      "    2  at        \n",
      "    2  cluster   \n",
      "    2  any       \n",
      "    2  blanks    \n",
      "    2  repeat    \n",
      "    2  center    \n",
      "    2  points    \n",
      "    2  decreasing\n",
      "    2  order     \n",
      "    1  sameModel \n",
      "    1  load      \n",
      "    1  all       \n",
      "    1  intuitoive\n",
      "    1  questions \n",
      "    1  carefully \n",
      "    1  write     \n",
      "    1  group     \n",
      "    1  implementation\n",
      "    1  had       \n",
      "    1  weeks     \n",
      "    1  submissions\n",
      "    1  case      \n",
      "    1  return    \n",
      "    1  homeworks \n",
      "    1  compute   \n",
      "    1  bringing  \n",
      "    1  runs      \n",
      "    1  resource  \n",
      "    1  exercise  \n",
      "    1  where     \n",
      "    1  datamaplambda\n",
      "    1  generation\n",
      "    1  strWSSSE  \n",
      "    1  consists  \n",
      "    1  please    \n",
      "    1  cell      \n",
      "    1  above     \n",
      "    1  math      \n",
      "    1  interfaces\n",
      "    1  modify    \n",
      "    1  not       \n",
      "    1  based     \n",
      "    1  parameters\n",
      "    1  column    \n",
      "    1  clusterscentersclusterspredictpoint\n",
      "    1  completing\n",
      "    1  length    \n",
      "    1  resulting \n",
      "    1  comment   \n",
      "    1  letters   \n",
      "    1  distributed\n",
      "    1  done      \n",
      "    1  initializationModerandom\n",
      "    1  array     \n",
      "    1  clustering\n",
      "    1  use       \n",
      "    1  submit    \n",
      "    1  forward   \n",
      "    1  numpy     \n",
      "    1  sort      \n",
      "    1  errorpoint\n",
      "    1  form      \n",
      "    1  lower     \n",
      "    1  tokens    \n",
      "    1  pysparkmllibclustering\n",
      "    1  line      \n",
      "    1  clusterssavesc\n",
      "    1  iteration \n",
      "    1  labeled   \n",
      "    1  norm      \n",
      "    1  fun       \n",
      "    1  def       \n",
      "    1  sqrtsumx2 \n",
      "    1  links     \n",
      "    1  httpswwwdropboxcoms3nsthvp8g2rrrdhEMKmeansipynbdl0\n",
      "    1  parse     \n",
      "    1  single    \n",
      "    1  tab       \n",
      "    1  different \n",
      "    1  develop   \n",
      "    1  sqrt      \n",
      "    1  answer    \n",
      "    1  begin     \n",
      "    1  driver    \n",
      "    1  regularization\n",
      "    1  tune      \n",
      "    1  httpsdocsgooglecomformsd1ZOr9RnIeA06AcZDB6K1mJN4vrLeSmS2PD6Xm3eOiisviewformuspsendform\n",
      "    1  occurence \n",
      "    1  maxIterations10\n",
      "    1  savings   \n",
      "    1  algorithms\n",
      "    1  arrayfloatx\n",
      "    1  hyper     \n",
      "    1  show      \n",
      "    1  text      \n",
      "    1  experiments\n",
      "    1  follow    \n",
      "    1  find      \n",
      "    1  inverse   \n",
      "    1  la        \n",
      "    1  program   \n",
      "    1  do        \n",
      "    1  good      \n",
      "    1  get       \n",
      "    1  evaluate  \n",
      "    1  framework \n",
      "    1  made      \n",
      "    1  progress  \n",
      "    1  sorted    \n",
      "    1  dataset   \n",
      "    1  instructions\n",
      "    1  homegrown \n",
      "    1  server    \n",
      "    1  assignments\n",
      "    1  either    \n",
      "    1  runs10    \n",
      "    1  often     \n",
      "    1  back      \n",
      "    1  located   \n",
      "    1  are       \n",
      "    1  measure   \n",
      "    1  experiements\n",
      "    1  parsedData\n",
      "    1  separate  \n",
      "    1  experiment\n",
      "    1  creating  \n",
      "    1  output    \n",
      "    1  by        \n",
      "    1  massive   \n",
      "    1  first     \n",
      "    1  computational\n",
      "    1  number    \n",
      "    1  weighted  \n",
      "    1  management\n",
      "    1  give      \n",
      "    1  appears   \n",
      "    1  going     \n",
      "    1  thru      \n",
      "    1  more      \n",
      "    1  httpswwwdropboxcomsatzqkc0p1eajuz6LinearRegressionNotebookChallengeipynbdl0\n",
      "    1  train     \n",
      "    1  hundred   \n",
      "    1  parsedDatamaplambda\n",
      "    1  work      \n",
      "    1  can       \n",
      "    1  computing \n",
      "    1  manner    \n",
      "    1  az        \n",
      "    1  httpsdocsgooglecomspreadsheetsd1ncFQl5Tovn16slD8mYjPnzMTPSfiGeLLzW8vsMjgedituspsharing\n",
      "    1  documents \n",
      "    1  descent   \n",
      "    1  instance  \n",
      "    1  other     \n",
      "    1  printWithin\n",
      "    1  sctextFilekmeansdatatxt\n",
      "    1  assignment\n",
      "    1  applications\n",
      "    1  linesplit \n",
      "    1  such      \n",
      "    1  weightX   \n",
      "    1  errorpointreducelambda\n",
      "    1  sets      \n",
      "    1  element   \n"
     ]
    }
   ],
   "source": [
    "def startsWithLower(word):\n",
    "    if word[0].islower(): return True\n",
    "    else: return False\n",
    "    \n",
    "hwText = sc.textFile('/user/miki/week10/hw10_1/MIDS-MLS-HW-10.txt')\n",
    "wordCounts = hwText.flatMap(splitString).filter(startsWithLower).map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\n",
    "sortedCounts = wordCounts.map(lambda x: (x[1], x[0])).sortByKey(ascending=False).collect()\n",
    "\n",
    "print '{:5s}  {:10s}'.format('count', 'word')\n",
    "print '--------------------'\n",
    "for count, word in sortedCounts:\n",
    "    print '{:5d}  {:10s}'.format(count, word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
