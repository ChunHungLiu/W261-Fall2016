{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Student**: Minhchau Dang\n",
    "* **Email Address**: minhchau.dang@berkeley.edu\n",
    "* **Course**: 2016-0111 DATASCI W261: Machine Learning at Scale\n",
    "* **Section**: Spring 2016, Section 2\n",
    "* **Assignment**: Homework 13, Week 13\n",
    "* **Submission Date**: April 21, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Cluster Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using r3.xlarge nodes, there is 23424 MB of available memory for Yarn scheduling and there are 4 cores per server.\n",
    "\n",
    "* http://www.ec2instances.info\n",
    "* http://docs.aws.amazon.com/ElasticMapReduce/latest/ReleaseGuide/emr-hadoop-task-config.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "node_count = 9\n",
    "server_cores = 4\n",
    "yarn_memory = 23424\n",
    "executors_per_node = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Cluster Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster_name = 'Week13_GraphX'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 13.3: Spark GraphX versus your implementation of PageRank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll want to test it against our test data. However, the data format for GraphX requires integer values and our test data consists of letters. So, we'll need to transform it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cat PageRank-test.txt | sed 's/A/1/g' | sed 's/B/2/g' | sed 's/C/3/g' | \\\n",
    "    sed 's/D/4/g' | sed 's/E/5/g' | sed 's/F/6/g' | sed 's/G/7/g' | sed 's/H/8/g' | \\\n",
    "    sed 's/I/9/g' | sed 's/J/10/g' | sed 's/K/11/g' > PageRank-testint.txt\n",
    "\n",
    "!hdfs dfs -copyFromLocal PageRank-testint.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run page rank on the test data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Scala Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing PageRank.scala\n"
     ]
    }
   ],
   "source": [
    "%%writefile PageRank.scala\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.graphx.lib._\n",
    "import org.apache.spark.rdd.RDD\n",
    "\n",
    "/**\n",
    " * Retrieve edges from a single line in the input file\n",
    " */\n",
    "def getEdges(line:String): List[Edge[Int]] = {\n",
    "  val splitLine = line.split(\"\\t\")\n",
    "\n",
    "  val label:String = splitLine(0)\n",
    "  val re = \"'([^']*)'\".r\n",
    "\n",
    "  val matchList = (re findAllIn splitLine(1)).matchData.toList\n",
    "  matchList.map { matchItem => Edge(label.toLong, matchItem.group(1).toLong, 1) }\n",
    "}\n",
    "\n",
    "val edgesRDD = sc.textFile(\"PageRank-testint.txt\").flatMap(getEdges)\n",
    "val dataRDD:Graph[Any,Int] = Graph.fromEdges(edgesRDD, 1)\n",
    "val pageRankRDD = PageRank.run(dataRDD, numIter=40, resetProb=0.15).vertices\n",
    "\n",
    "println pageRankRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit Scala job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we run the shell script, we'll make sure that our log4j.properties suppresses INFO level logging so that we only print out WARN or above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!sudo sed -i 's/INFO/WARN/g' /usr/lib/spark/conf/log4j.properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we submit our script to the Spark shell. Given that we are just using the test set, we don't need to provide the extra configuration options for the shell. We'll use the options mentioned here.\n",
    "\n",
    "http://spark.apache.org/docs/latest/quick-start.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: scala: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!spark-shell < PageRank.scala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
