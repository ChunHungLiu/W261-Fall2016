{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASCI W261: Machine Learning at Scale\n",
    "## Assignment Week 3\n",
    "Miki Seltzer (miki.seltzer@berkeley.edu)<br>\n",
    "W261-2, Spring 2016<br>\n",
    "Submission: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## HW3.0\n",
    "### What is a merge sort? Where is it used in Hadoop?\n",
    "Merge sort is used to combine two pre-sorted lists. It is a very efficient sort, as it only needs to iteratively look for the smallest element in multiple sorted lists. It is utilized in Hadoop during the shuffle, when key-value pairs are shuffled to reducers, then sorted.\n",
    "\n",
    "### How is  a combiner function in the context of Hadoop? \n",
    "A combiner function allows for preaggregation before key-value pairs are sent from the mappers to reducers. It is similar to a reducer, but there are some key differences. One difference is that the combiner may not always be used during the job -- it may be used 0, 1, or many times. Thus, we cannot count on Hadoop actually using a combiner we have included in the job, and we must be careful about matching output types of the mapper and combiner.\n",
    "\n",
    "### Give an example where it can be used and justify why it should be used in the context of this problem.\n",
    "In the classic word count example, a document is scanned, and each word is paired with the value of 1. A combiner can be used to combine values of 1 with the same key (word) before they are shuffled to reducers. This reduces the amount of data that is shuffled between the mapper and reducer, and increases efficiency.\n",
    "\n",
    "### What is the Hadoop shuffle?\n",
    "The Hadoop shuffle is the process by which data from mappers is shuffled and sorted while being sent to reducers. The shuffle ensures that keys are grouped together and sorted within the reducer they are sent to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.1: Use Counters to do EDA (exploratory data analysis and to monitor progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: `/user/miki/week03': File exists\r\n"
     ]
    }
   ],
   "source": [
    "# I am running this locally, so make sure that the Hadoop streaming API is in this folder.\n",
    "!wget http://central.maven.org/maven2/org/apache/hadoop/hadoop-streaming/2.7.1/hadoop-streaming-2.7.1.jar\n",
    "    \n",
    "# Create a folder on HDFS for this week's assignment, strip the header line from Consumer_Complaints.csv\n",
    "!echo \"$(tail -n +2 Consumer_Complaints.csv)\" > Consumer_Complaints.csv\n",
    "!hdfs dfs -mkdir /user/miki/week03\n",
    "!hdfs dfs -put Consumer_Complaints.csv /user/miki/week03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper_31.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_31.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: mapper code for HW3.1\n",
    "\n",
    "import sys\n",
    "from csv import reader\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in reader(sys.stdin):\n",
    "    product = line[1]\n",
    "    if product == \"Debt collection\": sys.stderr.write(\"reporter:counter:Product,Debt,1\\n\")\n",
    "    elif product == \"Mortgage\": sys.stderr.write(\"reporter:counter:Product,Mortgage,1\\n\")\n",
    "    else: sys.stderr.write(\"reporter:counter:Product,Other,1\\n\")\n",
    "    print line\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer_31.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_31.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: reducer code for HW3.1\n",
    "\n",
    "import sys\n",
    "from operator import itemgetter\n",
    "from csv import reader\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    print line\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/miki/week03/hw3_1_output\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.5.0.jar] /tmp/streamjob7547497993532515688.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "# Change permissions on mapper and reducer\n",
    "!chmod +x mapper_31.py\n",
    "!chmod +x reducer_31.py\n",
    "\n",
    "# If output folder already exists, delete it\n",
    "!hdfs dfs -rm -r /user/miki/week03/hw3_1_output\n",
    "\n",
    "# Run job\n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-mapper /home/cloudera/Documents/W261-Fall2016/Week03/mapper_31.py \\\n",
    "-reducer /home/cloudera/Documents/W261-Fall2016/Week03/reducer_31.py \\\n",
    "-input /user/miki/week03/Consumer_Complaints.csv \\\n",
    "-output /user/miki/week03/hw3_1_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Screenshot\n",
    "\n",
    "![Custom Counters](Counters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 3.2 Analyze the performance of your Mappers, Combiners and Reducers using Counters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HW3_2_input.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile HW3_2_input.txt\n",
    "foo foo quux labs foo bar quux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -put HW3_2_input.txt /user/miki/week03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper_32a.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_32a.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: mapper code for HW3.2\n",
    "\n",
    "import sys\n",
    "from csv import reader\n",
    "\n",
    "# Increment mapper counter\n",
    "sys.stderr.write(\"reporter:counter:Custom_Counter,Mapper,1\\n\")\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    line = line.split()\n",
    "    for word in line:\n",
    "        print '%s\\t%s' % (word, 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer_32a.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_32a.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: reducer code for HW3.2\n",
    "\n",
    "import sys\n",
    "from operator import itemgetter\n",
    "\n",
    "# Increment mapper counter\n",
    "sys.stderr.write(\"reporter:counter:Custom_Counter,Reducer,1\\n\")\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    print line\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/miki/week03/hw3_2a_output\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.5.0.jar] /tmp/streamjob7785257004505429063.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "# Change permissions on mapper and reducer\n",
    "!chmod +x mapper_32a.py\n",
    "!chmod +x reducer_32a.py\n",
    "\n",
    "# If output folder already exists, delete it\n",
    "!hdfs dfs -rm -r /user/miki/week03/hw3_2a_output\n",
    "\n",
    "# Run job\n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.map.tasks=1 \\\n",
    "-D mapred.reduce.tasks=4 \\\n",
    "-mapper /home/cloudera/Documents/W261-Fall2016/Week03/mapper_32a.py \\\n",
    "-reducer /home/cloudera/Documents/W261-Fall2016/Week03/reducer_32a.py \\\n",
    "-input /user/miki/week03/HW3_2_input.txt \\\n",
    "-output /user/miki/week03/hw3_2a_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the value of your user defined Mapper Counter, and Reducer Counter after completing this word count job? The answer  should be 1 and 4 respectively. Please explain.\n",
    "\n",
    "I had to specify the number of map tasks and reduce tasks to get 1 and 4, since the defaults produced counters of 2 and 1 respectively.\n",
    "\n",
    "The counters were incremented each time the mapper and reducer scripts were executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.2b: Perform a word count analysis of the Issue column of the Consumer Complaints Dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper_32b.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_32b.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: mapper code for HW3.2b\n",
    "\n",
    "import sys\n",
    "from csv import reader\n",
    "import string\n",
    "\n",
    "# Increment mapper counter\n",
    "sys.stderr.write(\"reporter:counter:Custom_Counter,Mapper,1\\n\")\n",
    "\n",
    "# Initialize variables\n",
    "total = 0\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in reader(sys.stdin):\n",
    "    # Format our line\n",
    "    issue = line[3].lower()\n",
    "    issue = issue.replace(',',' ').replace('/',' ')\n",
    "    \n",
    "    for word in issue.split():\n",
    "        if len(word) > 0:\n",
    "            print '%s\\t%s' % (word, 1)\n",
    "            total += 1\n",
    "\n",
    "# Print total words\n",
    "print '%s\\t%s' % ('*total', total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer_32b.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_32b.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: reducer code for HW3.2b\n",
    "\n",
    "import sys\n",
    "from operator import itemgetter\n",
    "\n",
    "# Increment mapper counter\n",
    "sys.stderr.write(\"reporter:counter:Custom_Counter,Reducer,1\\n\")\n",
    "\n",
    "# Initialize variables\n",
    "prev_word = None\n",
    "prev_count = 0\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "\n",
    "    # Split line\n",
    "    word, count = line.split('\\t')\n",
    "\n",
    "    # Convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # Count wasn't an int, so move on\n",
    "        continue\n",
    "\n",
    "    if prev_word == word:\n",
    "        # We haven't moved to a new word\n",
    "        prev_count += count\n",
    "    \n",
    "    else:\n",
    "        if prev_word:\n",
    "            print '%s\\t%s' % (prev_word, prev_count)\n",
    "\n",
    "        prev_count = count\n",
    "        prev_word = word\n",
    "\n",
    "# Output the last line\n",
    "if prev_word == word:\n",
    "    print '%s\\t%s' % (prev_word, prev_count)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/miki/week03/hw3_2b_output\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.5.0.jar] /tmp/streamjob1465294680810041979.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "# Change permissions on mapper and reducer\n",
    "!chmod +x mapper_32b.py\n",
    "!chmod +x reducer_32b.py\n",
    "\n",
    "# If output folder already exists, delete it\n",
    "!hdfs dfs -rm -r /user/miki/week03/hw3_2b_output\n",
    "\n",
    "# Run job\n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.reduce.tasks=4 \\\n",
    "-mapper /home/cloudera/Documents/W261-Fall2016/Week03/mapper_32b.py \\\n",
    "-reducer /home/cloudera/Documents/W261-Fall2016/Week03/reducer_32b.py \\\n",
    "-input /user/miki/week03/Consumer_Complaints.csv \\\n",
    "-output /user/miki/week03/hw3_2b_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job?\n",
    "\n",
    "After completing this job, the counters show the following values:\n",
    "- Mapper: 2\n",
    "- Reducer: 4 (this is explicitly set when running the job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.2c: Perform a word count analysis of the Issue column of the Consumer Complaints Dataset (ADD: standalone combiner)\n",
    "\n",
    "We can reuse the reducer in this case, and rename it combiner. We update the line to increment the combiner counter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing combiner_32c.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile combiner_32c.py\n",
    "#!/usr/bin/python\n",
    "## combiner.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: combiner code for HW3.2c\n",
    "\n",
    "import sys\n",
    "from operator import itemgetter\n",
    "\n",
    "# Increment mapper counter\n",
    "sys.stderr.write(\"reporter:counter:Custom_Counter,Combiner,1\\n\")\n",
    "\n",
    "prev_word = None\n",
    "prev_count = 0\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    word, count = line.split('\\t')\n",
    "\n",
    "    # Convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # Count wasn't an int, so move on\n",
    "        continue\n",
    "\n",
    "    # Check if we've moved to a new word\n",
    "    if prev_word == word:\n",
    "        prev_count += count\n",
    "    else:\n",
    "        if prev_word:\n",
    "            # We are at a new word, need to print previous word sum\n",
    "            print '%s\\t%s' % (prev_word, prev_count)\n",
    "        prev_count = count\n",
    "        prev_word = word\n",
    "\n",
    "# Output the last line\n",
    "if prev_word == word:\n",
    "    print '%s\\t%s' % (prev_word, prev_count)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/miki/week03/hw3_2c_output\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.5.0.jar] /tmp/streamjob2612824031763828089.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "# Change permissions on mapper and reducer\n",
    "!chmod +x mapper_32b.py\n",
    "!chmod +x combiner_32c.py\n",
    "!chmod +x reducer_32b.py\n",
    "\n",
    "# If output folder already exists, delete it\n",
    "!hdfs dfs -rm -r /user/miki/week03/hw3_2c_output\n",
    "\n",
    "# Run job\n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.reduce.tasks=4 \\\n",
    "-mapper /home/cloudera/Documents/W261-Fall2016/Week03/mapper_32b.py \\\n",
    "-combiner /home/cloudera/Documents/W261-Fall2016/Week03/combiner_32c.py \\\n",
    "-reducer /home/cloudera/Documents/W261-Fall2016/Week03/reducer_32b.py \\\n",
    "-input /user/miki/week03/Consumer_Complaints.csv \\\n",
    "-output /user/miki/week03/hw3_2c_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the value of your user defined Mapper Counter, Combiner Counter and Reducer Counter after completing your word count job?\n",
    "\n",
    "After completing this job, the counters show the following values:\n",
    "- Mapper: 2\n",
    "- Combiner: 8\n",
    "- Reducer: 4 (this is explicitly set when running the job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.2d: Using a single reducer, present frequency and relative frequency of top 50 and bottom 10 terms\n",
    "\n",
    "For this section, we only need an identity mapper and an identity reducer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_32d.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_32d.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: mapper code for HW3.2d\n",
    "\n",
    "import sys\n",
    "from csv import reader\n",
    "import string\n",
    "\n",
    "# Increment mapper counter\n",
    "sys.stderr.write(\"reporter:counter:Custom_Counter,Mapper,1\\n\")\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    word, count = line.replace('\\n','').split('\\t')\n",
    "    print '%s\\t%s' % (count, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_32d.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_32d.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: reducer code for HW3.2d\n",
    "\n",
    "import sys\n",
    "from operator import itemgetter\n",
    "\n",
    "# Increment mapper counter\n",
    "sys.stderr.write(\"reporter:counter:Custom_Counter,Reducer,1\\n\")\n",
    "\n",
    "# Initialize variables\n",
    "total = 0\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    fields = line.replace('\\n','').split('\\t')\n",
    "    count = fields[0]\n",
    "    word = fields[1]\n",
    "    \n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        continue\n",
    "        \n",
    "    # The first word should be *total, save this as total\n",
    "    if word == '*total': total = float(count)\n",
    "    else: print '%s\\t%s\\t%s' % (word, count, count/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/miki/week03/hw3_2d_output\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.5.0.jar] /tmp/streamjob6548003399415369612.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "# Change permissions on mapper and reducer\n",
    "!chmod +x mapper_32d.py\n",
    "!chmod +x reducer_32d.py\n",
    "\n",
    "# If output folder already exists, delete it\n",
    "!hdfs dfs -rm -r /user/miki/week03/hw3_2d_output\n",
    "\n",
    "# Run job\n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.text.key.partitioner.options=-k1,1 \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D mapred.text.key.comparator.options='-k1,1nr -k2,2n' \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-mapper /home/cloudera/Documents/W261-Fall2016/Week03/mapper_32d.py \\\n",
    "-reducer /home/cloudera/Documents/W261-Fall2016/Week03/reducer_32d.py \\\n",
    "-input /user/miki/week03/hw3_2b_output/part* \\\n",
    "-output /user/miki/week03/hw3_2d_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!rm hw3_2d_output.txt\n",
    "!hdfs dfs -copyToLocal /user/miki/week03/hw3_2d_output/part-00000 hw3_2d_output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      word               count  relative freq\n",
      "--------------------------------------------\n",
      "[  1] loan             119,630          8.87%\n",
      "[  2] collection        72,394          5.37%\n",
      "[  3] foreclosure       70,487          5.23%\n",
      "[  4] modification      70,487          5.23%\n",
      "[  5] account           57,448          4.26%\n",
      "[  6] credit            55,251          4.10%\n",
      "[  7] or                40,508          3.00%\n",
      "[  8] payments          39,993          2.97%\n",
      "[  9] escrow            36,767          2.73%\n",
      "[ 10] servicing         36,767          2.73%\n",
      "[ 11] report            34,903          2.59%\n",
      "[ 12] incorrect         29,133          2.16%\n",
      "[ 13] information       29,069          2.16%\n",
      "[ 14] on                29,069          2.16%\n",
      "[ 15] debt              27,874          2.07%\n",
      "[ 16] closing           19,000          1.41%\n",
      "[ 17] not               18,477          1.37%\n",
      "[ 18] attempts          17,972          1.33%\n",
      "[ 19] cont'd            17,972          1.33%\n",
      "[ 20] collect           17,972          1.33%\n",
      "[ 21] owed              17,972          1.33%\n",
      "[ 22] and               16,448          1.22%\n",
      "[ 23] management        16,205          1.20%\n",
      "[ 24] opening           16,205          1.20%\n",
      "[ 25] of                13,983          1.04%\n",
      "[ 26] my                10,731          0.80%\n",
      "[ 27] deposits          10,555          0.78%\n",
      "[ 28] withdrawals       10,555          0.78%\n",
      "[ 29] problems           9,484          0.70%\n",
      "[ 30] application        8,868          0.66%\n",
      "[ 31] communication      8,671          0.64%\n",
      "[ 32] tactics            8,671          0.64%\n",
      "[ 33] broker             8,625          0.64%\n",
      "[ 34] mortgage           8,625          0.64%\n",
      "[ 35] originator         8,625          0.64%\n",
      "[ 36] to                 8,401          0.62%\n",
      "[ 37] unable             8,178          0.61%\n",
      "[ 38] billing            8,158          0.61%\n",
      "[ 39] other              7,886          0.58%\n",
      "[ 40] disclosure         7,655          0.57%\n",
      "[ 41] verification       7,655          0.57%\n",
      "[ 42] disputes           6,938          0.51%\n",
      "[ 43] reporting          6,559          0.49%\n",
      "[ 44] lease              6,337          0.47%\n",
      "[ 45] the                6,248          0.46%\n",
      "[ 46] by                 5,663          0.42%\n",
      "[ 47] being              5,663          0.42%\n",
      "[ 48] caused             5,663          0.42%\n",
      "[ 49] funds              5,663          0.42%\n",
      "[ 50] low                5,663          0.42%\n",
      "...\n",
      "[165] apply                118          0.01%\n",
      "[166] amount                98          0.01%\n",
      "[167] credited              92          0.01%\n",
      "[168] payment               92          0.01%\n",
      "[169] checks                75          0.01%\n",
      "[170] convenience           75          0.01%\n",
      "[171] amt                   71          0.01%\n",
      "[172] day                   71          0.01%\n",
      "[173] disclosures           64          0.00%\n",
      "[174] missing               64          0.00%\n",
      "\n",
      "-------------------------------------------\n",
      "      Unique words         174\n"
     ]
    }
   ],
   "source": [
    "# Function to pretty print:\n",
    "# - the top x and bottom y items\n",
    "# - unique items\n",
    "def print_results(file, x=50, y=10):\n",
    "    words = []\n",
    "    special_words = []\n",
    "    \n",
    "    with open(file,'r') as myfile:\n",
    "        for line in myfile:\n",
    "            fields = line.replace('\\n','').split('\\t')\n",
    "            if fields[0][0] != '*': words.append(fields)\n",
    "            else: special_words.append(fields)\n",
    "\n",
    "    print '      {:16s}{:>8s}{:>15s}'.format('word', 'count', 'relative freq')\n",
    "    print '--------------------------------------------'\n",
    "    for i in range(x):\n",
    "        print '[{:3d}] {:16s}{:8,d}{:15.2%}'.format(i+1, \n",
    "                                                    words[i][0], \n",
    "                                                    int(words[i][1]), \n",
    "                                                    float(words[i][2]))\n",
    "    print '...'\n",
    "    for i in range(y):\n",
    "        j = len(words) - 10 + i\n",
    "        print '[{:3d}] {:16s}{:8,d}{:15.2%}'.format(j+1, \n",
    "                                                    words[j][0], \n",
    "                                                    int(words[j][1]), \n",
    "                                                    float(words[j][2]))\n",
    "    \n",
    "    print '\\n-------------------------------------------'\n",
    "    print '      {:16s}{:>8,d}'.format('Unique words', len(words))\n",
    "    for item in special_words:\n",
    "        name = item[0][1:].replace('_',' ')\n",
    "        print '      {:16s}{:>8,d}'.format(name, int(item[1]))\n",
    "\n",
    "print_results('hw3_2d_output.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.3. Shopping Cart Analysis Exploratory Data Analysis\n",
    "\n",
    "We can reuse the reducer from HW3.2b, but there are small changes that need to be made to the mapper:\n",
    "- We do not have to format the products to lower case, assume there is no punctuation stripping needed\n",
    "- Keep track of the largest basket size as we loop through baskets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -put ProductPurchaseData.txt /user/miki/week03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_33a.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_33a.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: mapper code for HW3.3a\n",
    "\n",
    "import sys\n",
    "\n",
    "# Increment mapper counter\n",
    "sys.stderr.write(\"reporter:counter:Custom_Counter,Mapper,1\\n\")\n",
    "\n",
    "# Initialize variables\n",
    "total = 0\n",
    "basket_size = 0\n",
    "largest_basket_size = 0\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # Split our line into products\n",
    "    for product in line.replace('\\n','').split():\n",
    "        print '%s\\t%s' % (product, 1)\n",
    "        basket_size += 1\n",
    "        total += 1\n",
    "    if basket_size > largest_basket_size:\n",
    "        largest_basket_size = basket_size\n",
    "    \n",
    "    basket_size = 0\n",
    "\n",
    "# Print total words\n",
    "print '%s\\t%s' % ('*total', total)\n",
    "print '%s\\t%s' % ('*largest_basket', largest_basket_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/miki/week03/hw3_3a_output\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.5.0.jar] /tmp/streamjob4662501162287957069.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "# Change permissions on mapper and reducer\n",
    "!chmod +x mapper_33a.py\n",
    "\n",
    "# If output folder already exists, delete it\n",
    "!hdfs dfs -rm -r /user/miki/week03/hw3_3a_output\n",
    "\n",
    "# Run job\n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-mapper /home/cloudera/Documents/W261-Fall2016/Week03/mapper_33a.py \\\n",
    "-reducer /home/cloudera/Documents/W261-Fall2016/Week03/reducer_32b.py \\\n",
    "-input /user/miki/week03/ProductPurchaseData.txt \\\n",
    "-output /user/miki/week03/hw3_3a_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a single reducer: Report your findings such as number of unique products; largest basket; report the top 50 most frequently purchased items,  their frequency,  and their relative frequency (break ties by sorting the products alphabetical order) etc. using Hadoop Map-Reduce. \n",
    "\n",
    "We can use the mapper and reducer from HW3.2d to get the sorted frequencies and relative frequencies of the products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/miki/week03/hw3_3b_output\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.5.0.jar] /tmp/streamjob7461671848105384853.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "# If output folder already exists, delete it\n",
    "!hdfs dfs -rm -r /user/miki/week03/hw3_3b_output\n",
    "\n",
    "# Run job\n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.text.key.partitioner.options=-k1,1 \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D mapred.text.key.comparator.options='-k1,1nr -k2,2n' \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-mapper /home/cloudera/Documents/W261-Fall2016/Week03/mapper_32d.py \\\n",
    "-reducer /home/cloudera/Documents/W261-Fall2016/Week03/reducer_32d.py \\\n",
    "-input /user/miki/week03/hw3_3a_output/part* \\\n",
    "-output /user/miki/week03/hw3_3b_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!rm hw3_3b_output.txt\n",
    "!hdfs dfs -copyToLocal /user/miki/week03/hw3_3b_output/part-00000 hw3_3b_output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      word               count  relative freq\n",
      "--------------------------------------------\n",
      "[  1] DAI62779           6,667          1.75%\n",
      "[  2] FRO40251           3,881          1.02%\n",
      "[  3] ELE17451           3,875          1.02%\n",
      "[  4] GRO73461           3,602          0.95%\n",
      "[  5] SNA80324           3,044          0.80%\n",
      "[  6] ELE32164           2,851          0.75%\n",
      "[  7] DAI75645           2,736          0.72%\n",
      "[  8] SNA45677           2,455          0.64%\n",
      "[  9] FRO31317           2,330          0.61%\n",
      "[ 10] DAI85309           2,293          0.60%\n",
      "[ 11] ELE26917           2,292          0.60%\n",
      "[ 12] FRO80039           2,233          0.59%\n",
      "[ 13] GRO21487           2,115          0.56%\n",
      "[ 14] SNA99873           2,083          0.55%\n",
      "[ 15] GRO59710           2,004          0.53%\n",
      "[ 16] GRO71621           1,920          0.50%\n",
      "[ 17] FRO85978           1,918          0.50%\n",
      "[ 18] GRO30386           1,840          0.48%\n",
      "[ 19] ELE74009           1,816          0.48%\n",
      "[ 20] GRO56726           1,784          0.47%\n",
      "[ 21] DAI63921           1,773          0.47%\n",
      "[ 22] GRO46854           1,756          0.46%\n",
      "[ 23] ELE66600           1,713          0.45%\n",
      "[ 24] DAI83733           1,712          0.45%\n",
      "[ 25] FRO32293           1,702          0.45%\n",
      "[ 26] ELE66810           1,697          0.45%\n",
      "[ 27] SNA55762           1,646          0.43%\n",
      "[ 28] DAI22177           1,627          0.43%\n",
      "[ 29] FRO78087           1,531          0.40%\n",
      "[ 30] ELE99737           1,516          0.40%\n",
      "[ 31] ELE34057           1,489          0.39%\n",
      "[ 32] GRO94758           1,489          0.39%\n",
      "[ 33] FRO35904           1,436          0.38%\n",
      "[ 34] FRO53271           1,420          0.37%\n",
      "[ 35] SNA93860           1,407          0.37%\n",
      "[ 36] SNA90094           1,390          0.36%\n",
      "[ 37] GRO38814           1,352          0.36%\n",
      "[ 38] ELE56788           1,345          0.35%\n",
      "[ 39] GRO61133           1,321          0.35%\n",
      "[ 40] DAI88807           1,316          0.35%\n",
      "[ 41] ELE74482           1,316          0.35%\n",
      "[ 42] ELE59935           1,311          0.34%\n",
      "[ 43] SNA96271           1,295          0.34%\n",
      "[ 44] DAI43223           1,290          0.34%\n",
      "[ 45] ELE91337           1,289          0.34%\n",
      "[ 46] GRO15017           1,275          0.33%\n",
      "[ 47] DAI31081           1,261          0.33%\n",
      "[ 48] GRO81087           1,220          0.32%\n",
      "[ 49] DAI22896           1,219          0.32%\n",
      "[ 50] GRO85051           1,214          0.32%\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "      Unique words      12,592\n",
      "      largest basket        74\n"
     ]
    }
   ],
   "source": [
    "print_results('hw3_3b_output.txt', 50, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.4: Pairs\n",
    "\n",
    "Suppose we want to recommend new products to the customer based on the products they have already browsed on the online website. Write a map-reduce program to find products which are frequently browsed together. Fix the support count (cooccurence count) to s = 100 (i.e. product pairs need to occur together at least 100 times to be considered frequent) and find pairs of items (sometimes referred to itemsets of size 2 in association rule mining) that have a support count of 100 or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_34a.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_34a.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: mapper code for HW3.4a\n",
    "\n",
    "import sys\n",
    "import itertools\n",
    "\n",
    "# Increment mapper counter\n",
    "sys.stderr.write(\"reporter:counter:Custom_Counter,Mapper,1\\n\")\n",
    "\n",
    "# Initialize variables\n",
    "total = 0\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # Split our line into products\n",
    "    products = line.replace('\\n','').split()\n",
    "    \n",
    "    # Get all combinations of products:\n",
    "    #  - Use a set to remove duplicate products\n",
    "    #  - Combinations finds tuples of length 2 with no repeats\n",
    "    pairs = list(itertools.combinations(set(products), 2))\n",
    "    \n",
    "    # For each pair, sort the pair alphabetically, then emit\n",
    "    for pair in pairs:\n",
    "        sorted_pair = sorted(pair)\n",
    "        print '%s\\t%s\\t%s' % (sorted_pair[0], sorted_pair[1], 1)\n",
    "    \n",
    "    # Increment total number of baskets\n",
    "    total += 1\n",
    "        \n",
    "# Print total words\n",
    "print '%s\\t%s\\t%s' % ('*total', '', total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_34a.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_34a.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: reducer code for HW3.4a\n",
    "\n",
    "import sys\n",
    "from operator import itemgetter\n",
    "\n",
    "# Increment mapper counter\n",
    "sys.stderr.write(\"reporter:counter:Custom_Counter,Reducer,1\\n\")\n",
    "\n",
    "# Initialize variables\n",
    "prev_pair = []\n",
    "prev_count = 0\n",
    "total = 0\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # Define our key and value\n",
    "    fields = line.replace('\\n','').split('\\t')\n",
    "    pair = [fields[0], fields[1]]\n",
    "    count = fields[2]\n",
    "\n",
    "    # Convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # Count wasn't an int, so move on\n",
    "        continue\n",
    "\n",
    "    # Check if we've moved to a new word\n",
    "    if prev_pair == pair:\n",
    "        prev_count += count\n",
    "    else:\n",
    "        if len(prev_pair) > 0:\n",
    "            # We are at a new pair, need to print previous pair sum\n",
    "            print '%s\\t%s\\t%s' % (prev_pair[0], prev_pair[1], prev_count)\n",
    "        prev_count = count\n",
    "        prev_pair = pair\n",
    "\n",
    "# Output the last line\n",
    "if prev_pair == pair:\n",
    "    print '%s\\t%s\\t%s' % (prev_pair[0], prev_pair[1], prev_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/miki/week03/hw3_4a_output\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.5.0.jar] /tmp/streamjob4258626330607127192.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "# Change permissions on mapper and reducer\n",
    "!chmod +x mapper_34a.py\n",
    "!chmod +x reducer_34a.py\n",
    "\n",
    "# If output folder already exists, delete it\n",
    "!hdfs dfs -rm -r /user/miki/week03/hw3_4a_output\n",
    "\n",
    "# Run job\n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.text.key.partitioner.options=-k1,1 \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D mapred.text.key.comparator.options='-k1,1 -k2,2' \\\n",
    "-mapper /home/cloudera/Documents/W261-Fall2016/Week03/mapper_34a.py \\\n",
    "-reducer /home/cloudera/Documents/W261-Fall2016/Week03/reducer_34a.py \\\n",
    "-input /user/miki/week03/ProductPurchaseData.txt \\\n",
    "-output /user/miki/week03/hw3_4a_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_34b.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_34b.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: mapper code for HW3.4b\n",
    "\n",
    "import sys\n",
    "\n",
    "# Increment mapper counter\n",
    "sys.stderr.write(\"reporter:counter:Custom_Counter,Mapper,1\\n\")\n",
    "\n",
    "# Initialize variables\n",
    "total = 0\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    fields = line.replace('\\n','').split('\\t')\n",
    "    if int(fields[2]) >= 100:\n",
    "        print '%s\\t%s\\t%s' % (fields[2], fields[0], fields[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer_34b.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_34b.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: reducer code for HW3.4b\n",
    "\n",
    "import sys\n",
    "from operator import itemgetter\n",
    "\n",
    "# Increment mapper counter\n",
    "sys.stderr.write(\"reporter:counter:Custom_Counter,Reducer,1\\n\")\n",
    "\n",
    "# Initialize variables\n",
    "total = 0\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    fields = line.replace('\\n','').split('\\t')\n",
    "    count = fields[0]\n",
    "    item1 = fields[1]\n",
    "    item2 = fields[2]\n",
    "    \n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        continue\n",
    "        \n",
    "    # The first word should be *total, save this as total\n",
    "    if item1 == '*total': total = float(count)\n",
    "    else: print '%s\\t%s\\t%s\\t%s' % (item1, item2, count, count/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/miki/week03/hw3_4b_output\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.5.0.jar] /tmp/streamjob2299545763059091234.jar tmpDir=null\n",
      "\n",
      "real\t0m26.758s\n",
      "user\t0m4.982s\n",
      "sys\t0m0.345s\n"
     ]
    }
   ],
   "source": [
    "# Change permissions on mapper and reducer\n",
    "!chmod +x mapper_34b.py\n",
    "!chmod +x reducer_34b.py\n",
    "\n",
    "# If output folder already exists, delete it\n",
    "!hdfs dfs -rm -r /user/miki/week03/hw3_4b_output\n",
    "\n",
    "# Run job\n",
    "!time hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.text.key.partitioner.options=-k1,1 \\\n",
    "-D stream.num.map.output.key.fields=3 \\\n",
    "-D mapred.text.key.comparator.options='-k1,1nr -k2,2 -k3,3' \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-mapper /home/cloudera/Documents/W261-Fall2016/Week03/mapper_34b.py \\\n",
    "-reducer /home/cloudera/Documents/W261-Fall2016/Week03/reducer_34b.py \\\n",
    "-input /user/miki/week03/hw3_4a_output/part* \\\n",
    "-output /user/miki/week03/hw3_4b_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -copyToLocal /user/miki/week03/hw3_4b_output/part-00000 hw3_4b_output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to pretty print:\n",
    "# - the top x and bottom y items\n",
    "# - unique items\n",
    "def print_results(file, x=50, y=10):\n",
    "    words = []\n",
    "    special_words = []\n",
    "    \n",
    "    with open(file,'r') as myfile:\n",
    "        for line in myfile:\n",
    "            fields = line.replace('\\n','').split('\\t')\n",
    "            if fields[0][0] != '*': words.append(fields)\n",
    "            else: special_words.append(fields)\n",
    "\n",
    "    print '      {:10s}{:10s}{:>8s}{:>15s}'.format('item1', 'item2', 'count', 'relative freq')\n",
    "    print '------------------------------------------------------'\n",
    "    for i in range(x):\n",
    "        print '[{:3d}] {:10s}{:10s}{:8,d}{:15.2%}'.format(i+1, \n",
    "                                                          words[i][0], \n",
    "                                                          words[i][1],\n",
    "                                                          int(words[i][2]), \n",
    "                                                          float(words[i][3]))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List the top 50 product pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      item1     item2        count  relative freq\n",
      "------------------------------------------------------\n",
      "[  1] DAI62779  ELE17451     1,592          5.12%\n",
      "[  2] FRO40251  SNA80324     1,412          4.54%\n",
      "[  3] DAI75645  FRO40251     1,254          4.03%\n",
      "[  4] FRO40251  GRO85051     1,213          3.90%\n",
      "[  5] DAI62779  GRO73461     1,139          3.66%\n",
      "[  6] DAI75645  SNA80324     1,130          3.63%\n",
      "[  7] DAI62779  FRO40251     1,070          3.44%\n",
      "[  8] DAI62779  SNA80324       923          2.97%\n",
      "[  9] DAI62779  DAI85309       918          2.95%\n",
      "[ 10] ELE32164  GRO59710       911          2.93%\n",
      "[ 11] DAI62779  DAI75645       882          2.84%\n",
      "[ 12] FRO40251  GRO73461       882          2.84%\n",
      "[ 13] DAI62779  ELE92920       877          2.82%\n",
      "[ 14] FRO40251  FRO92469       835          2.68%\n",
      "[ 15] DAI62779  ELE32164       832          2.68%\n",
      "[ 16] DAI75645  GRO73461       712          2.29%\n",
      "[ 17] DAI43223  ELE32164       711          2.29%\n",
      "[ 18] DAI62779  GRO30386       709          2.28%\n",
      "[ 19] ELE17451  FRO40251       697          2.24%\n",
      "[ 20] DAI85309  ELE99737       659          2.12%\n",
      "[ 21] DAI62779  ELE26917       650          2.09%\n",
      "[ 22] GRO21487  GRO73461       631          2.03%\n",
      "[ 23] DAI62779  SNA45677       604          1.94%\n",
      "[ 24] ELE17451  SNA80324       597          1.92%\n",
      "[ 25] DAI62779  GRO71621       595          1.91%\n",
      "[ 26] DAI62779  SNA55762       593          1.91%\n",
      "[ 27] DAI62779  DAI83733       586          1.88%\n",
      "[ 28] ELE17451  GRO73461       580          1.86%\n",
      "[ 29] GRO73461  SNA80324       562          1.81%\n",
      "[ 30] DAI62779  GRO59710       561          1.80%\n",
      "[ 31] DAI62779  FRO80039       550          1.77%\n",
      "[ 32] DAI75645  ELE17451       547          1.76%\n",
      "[ 33] DAI62779  SNA93860       537          1.73%\n",
      "[ 34] DAI55148  DAI62779       526          1.69%\n",
      "[ 35] DAI43223  GRO59710       512          1.65%\n",
      "[ 36] ELE17451  ELE32164       511          1.64%\n",
      "[ 37] DAI62779  SNA18336       506          1.63%\n",
      "[ 38] ELE32164  GRO73461       486          1.56%\n",
      "[ 39] DAI62779  FRO78087       482          1.55%\n",
      "[ 40] DAI85309  ELE17451       482          1.55%\n",
      "[ 41] DAI62779  GRO94758       479          1.54%\n",
      "[ 42] DAI62779  GRO21487       471          1.51%\n",
      "[ 43] GRO85051  SNA80324       471          1.51%\n",
      "[ 44] ELE17451  GRO30386       468          1.50%\n",
      "[ 45] FRO85978  SNA95666       463          1.49%\n",
      "[ 46] DAI62779  FRO19221       462          1.49%\n",
      "[ 47] DAI62779  GRO46854       461          1.48%\n",
      "[ 48] DAI43223  DAI62779       459          1.48%\n",
      "[ 49] ELE92920  SNA18336       455          1.46%\n",
      "[ 50] DAI88079  FRO40251       446          1.43%\n"
     ]
    }
   ],
   "source": [
    "print_results('hw3_4b_output.txt', 50, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report  the compute time for the Pairs job. \n",
    "The job reports the following compute times:\n",
    "```\n",
    "real\t0m26.758s\n",
    "user\t0m4.982s\n",
    "sys \t0m0.345s\n",
    "```\n",
    "### Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)\n",
    "Cloudera QuickStart VM: single computer, 2 processors, 2 mappers (default), 1 reducer\n",
    "\n",
    "### How many times is each mapper and reducer called?\n",
    "- Mapper: 2\n",
    "- Reducer: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.5: Stripes\n",
    "Repeat 3.4 using the stripes design pattern for finding cooccuring pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_35a.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_35a.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: mapper code for HW3.5a\n",
    "\n",
    "import sys\n",
    "import itertools\n",
    "\n",
    "# Increment mapper counter\n",
    "sys.stderr.write(\"reporter:counter:Custom_Counter,Mapper,1\\n\")\n",
    "\n",
    "# Initialize variables\n",
    "total = 0\n",
    "stripes = {}\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # Split our line into products\n",
    "    products = line.replace('\\n','').split()\n",
    "    \n",
    "    # Get all combinations of products:\n",
    "    #  - Use a set to remove duplicate products\n",
    "    #  - Combinations finds tuples of length 2 with no repeats\n",
    "    items = sorted(list(set(products)))\n",
    "\n",
    "    for i in range(len(items)-1):\n",
    "        for j in range(i+1, len(items)):\n",
    "            stripes[items[j]] = 1\n",
    "        print '%s\\t%s' % (items[i], stripes)\n",
    "        stripes = {}\n",
    "        \n",
    "    # Increment total number of baskets\n",
    "    total += 1\n",
    "        \n",
    "# Print total words\n",
    "print '%s\\t%s' % ('*total', total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/miki/week03/hw3_5a_output\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.5.0.jar] /tmp/streamjob1564090710484002419.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "# Change permissions on mapper and reducer\n",
    "!chmod +x mapper_35a.py\n",
    "#!chmod +x reducer_35a.py\n",
    "\n",
    "# If output folder already exists, delete it\n",
    "!hdfs dfs -rm -r /user/miki/week03/hw3_5a_output\n",
    "\n",
    "# Run job\n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-mapper /home/cloudera/Documents/W261-Fall2016/Week03/mapper_35a.py \\\n",
    "-reducer /bin/cat \\\n",
    "-input /user/miki/week03/ProductPurchaseData.txt \\\n",
    "-output /user/miki/week03/hw3_5a_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
