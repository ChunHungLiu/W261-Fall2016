{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASCI W261: Machine Learning at Scale\n",
    "## Assignment Week 3\n",
    "Minhchau Dang (minhchau.dang@berkeley.edu)<br>\n",
    "Miki Seltzer (miki.seltzer@berkeley.edu)<br>\n",
    "W261-2, Spring 2016<br>\n",
    "Submission: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## HW3.0\n",
    "### What is a merge sort? Where is it used in Hadoop?\n",
    "Merge sort is used to combine two pre-sorted lists. It is a very efficient sort, as it only needs to iteratively look for the smallest element in multiple sorted lists. It is utilized in Hadoop during the shuffle, when key-value pairs are shuffled to reducers, then sorted.\n",
    "\n",
    "### How is  a combiner function in the context of Hadoop? \n",
    "A combiner function allows for preaggregation before key-value pairs are sent from the mappers to reducers. It is similar to a reducer, but there are some key differences. One difference is that the combiner may not always be used during the job -- it may be used 0, 1, or many times. Thus, we cannot count on Hadoop actually using a combiner we have included in the job, and we must be careful about matching output types of the mapper and combiner.\n",
    "\n",
    "### Give an example where it can be used and justify why it should be used in the context of this problem.\n",
    "In the classic word count example, a document is scanned, and each word is paired with the value of 1. A combiner can be used to combine values of 1 with the same key (word) before they are shuffled to reducers. This reduces the amount of data that is shuffled between the mapper and reducer, and increases efficiency.\n",
    "\n",
    "### What is the Hadoop shuffle?\n",
    "The Hadoop shuffle is the process by which data from mappers is shuffled and sorted while being sent to reducers. The shuffle ensures that keys are grouped together and sorted within the reducer they are sent to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.1: Use Counters to do EDA (exploratory data analysis and to monitor progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: `/user/miki/week03': File exists\r\n"
     ]
    }
   ],
   "source": [
    "# I am running this locally, so make sure that the Hadoop streaming API is in this folder.\n",
    "!wget http://central.maven.org/maven2/org/apache/hadoop/hadoop-streaming/2.7.1/hadoop-streaming-2.7.1.jar\n",
    "    \n",
    "# Create a folder on HDFS for this week's assignment, strip the header line from Consumer_Complaints.csv\n",
    "!echo \"$(tail -n +2 Consumer_Complaints.csv)\" > Consumer_Complaints.csv\n",
    "!hdfs dfs -mkdir /user/miki/week03\n",
    "!hdfs dfs -put Consumer_Complaints.csv /user/miki/week03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper_31.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_31.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: mapper code for HW3.1\n",
    "\n",
    "import sys\n",
    "from csv import reader\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in reader(sys.stdin):\n",
    "    product = line[1]\n",
    "    if product == \"Debt collection\": sys.stderr.write(\"reporter:counter:Product,Debt,1\\n\")\n",
    "    elif product == \"Mortgage\": sys.stderr.write(\"reporter:counter:Product,Mortgage,1\\n\")\n",
    "    else: sys.stderr.write(\"reporter:counter:Product,Other,1\\n\")\n",
    "    print line\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer_31.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_31.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: reducer code for HW3.1\n",
    "\n",
    "import sys\n",
    "from operator import itemgetter\n",
    "from csv import reader\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    print line\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/miki/week03/hw3_1_output\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.5.0.jar] /tmp/streamjob7547497993532515688.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "# Change permissions on mapper and reducer\n",
    "!chmod +x mapper_31.py\n",
    "!chmod +x reducer_31.py\n",
    "\n",
    "# If output folder already exists, delete it\n",
    "!hdfs dfs -rm -r /user/miki/week03/hw3_1_output\n",
    "\n",
    "# Run job\n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-mapper /home/cloudera/Documents/W261-Fall2016/Week03/mapper_31.py \\\n",
    "-reducer /home/cloudera/Documents/W261-Fall2016/Week03/reducer_31.py \\\n",
    "-input /user/miki/week03/Consumer_Complaints.csv \\\n",
    "-output /user/miki/week03/hw3_1_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Screenshot\n",
    "\n",
    "![Custom Counters](Counters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 3.2 Analyze the performance of your Mappers, Combiners and Reducers using Counters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HW3_2_input.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile HW3_2_input.txt\n",
    "foo foo quux labs foo bar quux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -put HW3_2_input.txt /user/miki/week03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper_32a.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_32a.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: mapper code for HW3.2\n",
    "\n",
    "import sys\n",
    "from csv import reader\n",
    "\n",
    "# Increment mapper counter\n",
    "sys.stderr.write(\"reporter:counter:Custom_Counter,Mapper,1\\n\")\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    line = line.split()\n",
    "    for word in line:\n",
    "        print '%s\\t%s' % (word, 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer_32a.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_32a.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: reducer code for HW3.2\n",
    "\n",
    "import sys\n",
    "from operator import itemgetter\n",
    "\n",
    "# Increment mapper counter\n",
    "sys.stderr.write(\"reporter:counter:Custom_Counter,Reducer,1\\n\")\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    print line\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/miki/week03/hw3_2a_output\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.5.0.jar] /tmp/streamjob7785257004505429063.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "# Change permissions on mapper and reducer\n",
    "!chmod +x mapper_32a.py\n",
    "!chmod +x reducer_32a.py\n",
    "\n",
    "# If output folder already exists, delete it\n",
    "!hdfs dfs -rm -r /user/miki/week03/hw3_2a_output\n",
    "\n",
    "# Run job\n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.map.tasks=1 \\\n",
    "-D mapred.reduce.tasks=4 \\\n",
    "-mapper /home/cloudera/Documents/W261-Fall2016/Week03/mapper_32a.py \\\n",
    "-reducer /home/cloudera/Documents/W261-Fall2016/Week03/reducer_32a.py \\\n",
    "-input /user/miki/week03/HW3_2_input.txt \\\n",
    "-output /user/miki/week03/hw3_2a_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the value of your user defined Mapper Counter, and Reducer Counter after completing this word count job? The answer  should be 1 and 4 respectively. Please explain.\n",
    "\n",
    "I had to specify the number of map tasks and reduce tasks to get 1 and 4, since the defaults produced counters of 2 and 1 respectively.\n",
    "\n",
    "The counters were incremented each time the mapper and reducer scripts were executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.2b: Perform a word count analysis of the Issue column of the Consumer Complaints Dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper_32b.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_32b.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: mapper code for HW3.2b\n",
    "\n",
    "import sys\n",
    "from csv import reader\n",
    "import string\n",
    "\n",
    "# Increment mapper counter\n",
    "sys.stderr.write(\"reporter:counter:Custom_Counter,Mapper,1\\n\")\n",
    "\n",
    "# Initialize variables\n",
    "total = 0\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in reader(sys.stdin):\n",
    "    # Format our line\n",
    "    issue = line[3].lower()\n",
    "    issue = issue.replace(',',' ').replace('/',' ')\n",
    "    \n",
    "    for word in issue.split():\n",
    "        if len(word) > 0:\n",
    "            print '%s\\t%s' % (word, 1)\n",
    "            total += 1\n",
    "\n",
    "# Print total words\n",
    "print '%s\\t%s' % ('*total', total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer_32b.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_32b.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: reducer code for HW3.2b\n",
    "\n",
    "import sys\n",
    "from operator import itemgetter\n",
    "\n",
    "# Increment mapper counter\n",
    "sys.stderr.write(\"reporter:counter:Custom_Counter,Reducer,1\\n\")\n",
    "\n",
    "# Initialize variables\n",
    "prev_word = None\n",
    "prev_count = 0\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "\n",
    "    # Split line\n",
    "    word, count = line.split('\\t')\n",
    "\n",
    "    # Convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # Count wasn't an int, so move on\n",
    "        continue\n",
    "\n",
    "    if prev_word == word:\n",
    "        # We haven't moved to a new word\n",
    "        prev_count += count\n",
    "    \n",
    "    else:\n",
    "        if prev_word:\n",
    "            print '%s\\t%s' % (prev_word, prev_count)\n",
    "\n",
    "        prev_count = count\n",
    "        prev_word = word\n",
    "\n",
    "# Output the last line\n",
    "if prev_word == word:\n",
    "    print '%s\\t%s' % (prev_word, prev_count)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/miki/week03/hw3_2b_output\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.5.0.jar] /tmp/streamjob1465294680810041979.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "# Change permissions on mapper and reducer\n",
    "!chmod +x mapper_32b.py\n",
    "!chmod +x reducer_32b.py\n",
    "\n",
    "# If output folder already exists, delete it\n",
    "!hdfs dfs -rm -r /user/miki/week03/hw3_2b_output\n",
    "\n",
    "# Run job\n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.reduce.tasks=4 \\\n",
    "-mapper /home/cloudera/Documents/W261-Fall2016/Week03/mapper_32b.py \\\n",
    "-reducer /home/cloudera/Documents/W261-Fall2016/Week03/reducer_32b.py \\\n",
    "-input /user/miki/week03/Consumer_Complaints.csv \\\n",
    "-output /user/miki/week03/hw3_2b_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job?\n",
    "\n",
    "After completing this job, the counters show the following values:\n",
    "- Mapper: 2\n",
    "- Reducer: 4 (this is explicitly set when running the job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.2c: Perform a word count analysis of the Issue column of the Consumer Complaints Dataset (ADD: standalone combiner)\n",
    "\n",
    "We can reuse the reducer in this case, and rename it combiner. We update the line to increment the combiner counter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing combiner_32c.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile combiner_32c.py\n",
    "#!/usr/bin/python\n",
    "## combiner.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: combiner code for HW3.2c\n",
    "\n",
    "import sys\n",
    "from operator import itemgetter\n",
    "\n",
    "# Increment mapper counter\n",
    "sys.stderr.write(\"reporter:counter:Custom_Counter,Combiner,1\\n\")\n",
    "\n",
    "prev_word = None\n",
    "prev_count = 0\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    word, count = line.split('\\t')\n",
    "\n",
    "    # Convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # Count wasn't an int, so move on\n",
    "        continue\n",
    "\n",
    "    # Check if we've moved to a new word\n",
    "    if prev_word == word:\n",
    "        prev_count += count\n",
    "    else:\n",
    "        if prev_word:\n",
    "            # We are at a new word, need to print previous word sum\n",
    "            print '%s\\t%s' % (prev_word, prev_count)\n",
    "        prev_count = count\n",
    "        prev_word = word\n",
    "\n",
    "# Output the last line\n",
    "if prev_word == word:\n",
    "    print '%s\\t%s' % (prev_word, prev_count)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/miki/week03/hw3_2c_output\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.5.0.jar] /tmp/streamjob2612824031763828089.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "# Change permissions on mapper and reducer\n",
    "!chmod +x mapper_32b.py\n",
    "!chmod +x combiner_32c.py\n",
    "!chmod +x reducer_32b.py\n",
    "\n",
    "# If output folder already exists, delete it\n",
    "!hdfs dfs -rm -r /user/miki/week03/hw3_2c_output\n",
    "\n",
    "# Run job\n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.reduce.tasks=4 \\\n",
    "-mapper /home/cloudera/Documents/W261-Fall2016/Week03/mapper_32b.py \\\n",
    "-combiner /home/cloudera/Documents/W261-Fall2016/Week03/combiner_32c.py \\\n",
    "-reducer /home/cloudera/Documents/W261-Fall2016/Week03/reducer_32b.py \\\n",
    "-input /user/miki/week03/Consumer_Complaints.csv \\\n",
    "-output /user/miki/week03/hw3_2c_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the value of your user defined Mapper Counter, Combiner Counter and Reducer Counter after completing your word count job?\n",
    "\n",
    "After completing this job, the counters show the following values:\n",
    "- Mapper: 2\n",
    "- Combiner: 8\n",
    "- Reducer: 4 (this is explicitly set when running the job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.2d: Using a single reducer, present frequency and relative frequency of top 50 and bottom 10 terms\n",
    "\n",
    "For this section, we only need an identity mapper and an identity reducer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_32d.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_32d.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: mapper code for HW3.2d\n",
    "\n",
    "import sys\n",
    "from csv import reader\n",
    "import string\n",
    "\n",
    "# Increment mapper counter\n",
    "sys.stderr.write(\"reporter:counter:Custom_Counter,Mapper,1\\n\")\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    word, count = line.replace('\\n','').split('\\t')\n",
    "    print '%s\\t%s' % (count, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_32d.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_32d.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: reducer code for HW3.2d\n",
    "\n",
    "import sys\n",
    "from operator import itemgetter\n",
    "\n",
    "# Increment mapper counter\n",
    "sys.stderr.write(\"reporter:counter:Custom_Counter,Reducer,1\\n\")\n",
    "\n",
    "# Initialize variables\n",
    "total = 0\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    fields = line.replace('\\n','').split('\\t')\n",
    "    count = fields[0]\n",
    "    word = fields[1]\n",
    "    \n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        continue\n",
    "        \n",
    "    # The first word should be *total, save this as total\n",
    "    if word == '*total': total = float(count)\n",
    "    else: print '%s\\t%s\\t%s' % (word, count, count/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/miki/week03/hw3_2d_output\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.5.0.jar] /tmp/streamjob6548003399415369612.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "# Change permissions on mapper and reducer\n",
    "!chmod +x mapper_32d.py\n",
    "!chmod +x reducer_32d.py\n",
    "\n",
    "# If output folder already exists, delete it\n",
    "!hdfs dfs -rm -r /user/miki/week03/hw3_2d_output\n",
    "\n",
    "# Run job\n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.text.key.partitioner.options=-k1,1 \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D mapred.text.key.comparator.options='-k1,1nr -k2,2n' \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-mapper /home/cloudera/Documents/W261-Fall2016/Week03/mapper_32d.py \\\n",
    "-reducer /home/cloudera/Documents/W261-Fall2016/Week03/reducer_32d.py \\\n",
    "-input /user/miki/week03/hw3_2b_output/part* \\\n",
    "-output /user/miki/week03/hw3_2d_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!rm hw3_2d_output.txt\n",
    "!hdfs dfs -copyToLocal /user/miki/week03/hw3_2d_output/part-00000 hw3_2d_output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      word               count  relative freq\n",
      "--------------------------------------------\n",
      "[  1] loan             119,630          8.87%\n",
      "[  2] collection        72,394          5.37%\n",
      "[  3] foreclosure       70,487          5.23%\n",
      "[  4] modification      70,487          5.23%\n",
      "[  5] account           57,448          4.26%\n",
      "[  6] credit            55,251          4.10%\n",
      "[  7] or                40,508          3.00%\n",
      "[  8] payments          39,993          2.97%\n",
      "[  9] escrow            36,767          2.73%\n",
      "[ 10] servicing         36,767          2.73%\n",
      "[ 11] report            34,903          2.59%\n",
      "[ 12] incorrect         29,133          2.16%\n",
      "[ 13] information       29,069          2.16%\n",
      "[ 14] on                29,069          2.16%\n",
      "[ 15] debt              27,874          2.07%\n",
      "[ 16] closing           19,000          1.41%\n",
      "[ 17] not               18,477          1.37%\n",
      "[ 18] attempts          17,972          1.33%\n",
      "[ 19] cont'd            17,972          1.33%\n",
      "[ 20] collect           17,972          1.33%\n",
      "[ 21] owed              17,972          1.33%\n",
      "[ 22] and               16,448          1.22%\n",
      "[ 23] management        16,205          1.20%\n",
      "[ 24] opening           16,205          1.20%\n",
      "[ 25] of                13,983          1.04%\n",
      "[ 26] my                10,731          0.80%\n",
      "[ 27] deposits          10,555          0.78%\n",
      "[ 28] withdrawals       10,555          0.78%\n",
      "[ 29] problems           9,484          0.70%\n",
      "[ 30] application        8,868          0.66%\n",
      "[ 31] communication      8,671          0.64%\n",
      "[ 32] tactics            8,671          0.64%\n",
      "[ 33] broker             8,625          0.64%\n",
      "[ 34] mortgage           8,625          0.64%\n",
      "[ 35] originator         8,625          0.64%\n",
      "[ 36] to                 8,401          0.62%\n",
      "[ 37] unable             8,178          0.61%\n",
      "[ 38] billing            8,158          0.61%\n",
      "[ 39] other              7,886          0.58%\n",
      "[ 40] disclosure         7,655          0.57%\n",
      "[ 41] verification       7,655          0.57%\n",
      "[ 42] disputes           6,938          0.51%\n",
      "[ 43] reporting          6,559          0.49%\n",
      "[ 44] lease              6,337          0.47%\n",
      "[ 45] the                6,248          0.46%\n",
      "[ 46] by                 5,663          0.42%\n",
      "[ 47] being              5,663          0.42%\n",
      "[ 48] caused             5,663          0.42%\n",
      "[ 49] funds              5,663          0.42%\n",
      "[ 50] low                5,663          0.42%\n",
      "...\n",
      "[165] apply                118          0.01%\n",
      "[166] amount                98          0.01%\n",
      "[167] credited              92          0.01%\n",
      "[168] payment               92          0.01%\n",
      "[169] checks                75          0.01%\n",
      "[170] convenience           75          0.01%\n",
      "[171] amt                   71          0.01%\n",
      "[172] day                   71          0.01%\n",
      "[173] disclosures           64          0.00%\n",
      "[174] missing               64          0.00%\n",
      "\n",
      "-------------------------------------------\n",
      "      Unique words         174\n"
     ]
    }
   ],
   "source": [
    "# Function to pretty print:\n",
    "# - the top x and bottom y items\n",
    "# - unique items\n",
    "def print_results(file, x=50, y=10):\n",
    "    words = []\n",
    "    special_words = []\n",
    "    \n",
    "    with open(file,'r') as myfile:\n",
    "        for line in myfile:\n",
    "            fields = line.replace('\\n','').split('\\t')\n",
    "            if fields[0][0] != '*': words.append(fields)\n",
    "            else: special_words.append(fields)\n",
    "\n",
    "    print '      {:16s}{:>8s}{:>15s}'.format('word', 'count', 'relative freq')\n",
    "    print '--------------------------------------------'\n",
    "    for i in range(x):\n",
    "        print '[{:3d}] {:16s}{:8,d}{:15.2%}'.format(i+1, \n",
    "                                                    words[i][0], \n",
    "                                                    int(words[i][1]), \n",
    "                                                    float(words[i][2]))\n",
    "    print '...'\n",
    "    for i in range(y):\n",
    "        j = len(words) - 10 + i\n",
    "        print '[{:3d}] {:16s}{:8,d}{:15.2%}'.format(j+1, \n",
    "                                                    words[j][0], \n",
    "                                                    int(words[j][1]), \n",
    "                                                    float(words[j][2]))\n",
    "    \n",
    "    print '\\n-------------------------------------------'\n",
    "    print '      {:16s}{:>8,d}'.format('Unique words', len(words))\n",
    "    for item in special_words:\n",
    "        name = item[0][1:].replace('_',' ')\n",
    "        print '      {:16s}{:>8,d}'.format(name, int(item[1]))\n",
    "\n",
    "print_results('hw3_2d_output.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.3. Shopping Cart Analysis Exploratory Data Analysis\n",
    "\n",
    "We can reuse the reducer from HW3.2b, but there are small changes that need to be made to the mapper:\n",
    "- We do not have to format the products to lower case, assume there is no punctuation stripping needed\n",
    "- Keep track of the largest basket size as we loop through baskets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -put ProductPurchaseData.txt /user/miki/week03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_33a.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_33a.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: mapper code for HW3.3a\n",
    "\n",
    "import sys\n",
    "\n",
    "# Increment mapper counter\n",
    "sys.stderr.write(\"reporter:counter:Custom_Counter,Mapper,1\\n\")\n",
    "\n",
    "# Initialize variables\n",
    "total = 0\n",
    "basket_size = 0\n",
    "largest_basket_size = 0\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # Split our line into products\n",
    "    for product in line.replace('\\n','').split():\n",
    "        print '%s\\t%s' % (product, 1)\n",
    "        basket_size += 1\n",
    "        total += 1\n",
    "    if basket_size > largest_basket_size:\n",
    "        largest_basket_size = basket_size\n",
    "    \n",
    "    basket_size = 0\n",
    "\n",
    "# Print total words\n",
    "print '%s\\t%s' % ('*total', total)\n",
    "print '%s\\t%s' % ('*largest_basket', largest_basket_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/miki/week03/hw3_3a_output\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.5.0.jar] /tmp/streamjob4662501162287957069.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "# Change permissions on mapper and reducer\n",
    "!chmod +x mapper_33a.py\n",
    "\n",
    "# If output folder already exists, delete it\n",
    "!hdfs dfs -rm -r /user/miki/week03/hw3_3a_output\n",
    "\n",
    "# Run job\n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-mapper /home/cloudera/Documents/W261-Fall2016/Week03/mapper_33a.py \\\n",
    "-reducer /home/cloudera/Documents/W261-Fall2016/Week03/reducer_32b.py \\\n",
    "-input /user/miki/week03/ProductPurchaseData.txt \\\n",
    "-output /user/miki/week03/hw3_3a_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a single reducer: Report your findings such as number of unique products; largest basket; report the top 50 most frequently purchased items,  their frequency,  and their relative frequency (break ties by sorting the products alphabetical order) etc. using Hadoop Map-Reduce. \n",
    "\n",
    "We can use the mapper and reducer from HW3.2d to get the sorted frequencies and relative frequencies of the products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/miki/week03/hw3_3b_output\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.5.0.jar] /tmp/streamjob7461671848105384853.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "# If output folder already exists, delete it\n",
    "!hdfs dfs -rm -r /user/miki/week03/hw3_3b_output\n",
    "\n",
    "# Run job\n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.text.key.partitioner.options=-k1,1 \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D mapred.text.key.comparator.options='-k1,1nr -k2,2n' \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-mapper /home/cloudera/Documents/W261-Fall2016/Week03/mapper_32d.py \\\n",
    "-reducer /home/cloudera/Documents/W261-Fall2016/Week03/reducer_32d.py \\\n",
    "-input /user/miki/week03/hw3_3a_output/part* \\\n",
    "-output /user/miki/week03/hw3_3b_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!rm hw3_3b_output.txt\n",
    "!hdfs dfs -copyToLocal /user/miki/week03/hw3_3b_output/part-00000 hw3_3b_output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      word               count  relative freq\n",
      "--------------------------------------------\n",
      "[  1] DAI62779           6,667          1.75%\n",
      "[  2] FRO40251           3,881          1.02%\n",
      "[  3] ELE17451           3,875          1.02%\n",
      "[  4] GRO73461           3,602          0.95%\n",
      "[  5] SNA80324           3,044          0.80%\n",
      "[  6] ELE32164           2,851          0.75%\n",
      "[  7] DAI75645           2,736          0.72%\n",
      "[  8] SNA45677           2,455          0.64%\n",
      "[  9] FRO31317           2,330          0.61%\n",
      "[ 10] DAI85309           2,293          0.60%\n",
      "[ 11] ELE26917           2,292          0.60%\n",
      "[ 12] FRO80039           2,233          0.59%\n",
      "[ 13] GRO21487           2,115          0.56%\n",
      "[ 14] SNA99873           2,083          0.55%\n",
      "[ 15] GRO59710           2,004          0.53%\n",
      "[ 16] GRO71621           1,920          0.50%\n",
      "[ 17] FRO85978           1,918          0.50%\n",
      "[ 18] GRO30386           1,840          0.48%\n",
      "[ 19] ELE74009           1,816          0.48%\n",
      "[ 20] GRO56726           1,784          0.47%\n",
      "[ 21] DAI63921           1,773          0.47%\n",
      "[ 22] GRO46854           1,756          0.46%\n",
      "[ 23] ELE66600           1,713          0.45%\n",
      "[ 24] DAI83733           1,712          0.45%\n",
      "[ 25] FRO32293           1,702          0.45%\n",
      "[ 26] ELE66810           1,697          0.45%\n",
      "[ 27] SNA55762           1,646          0.43%\n",
      "[ 28] DAI22177           1,627          0.43%\n",
      "[ 29] FRO78087           1,531          0.40%\n",
      "[ 30] ELE99737           1,516          0.40%\n",
      "[ 31] ELE34057           1,489          0.39%\n",
      "[ 32] GRO94758           1,489          0.39%\n",
      "[ 33] FRO35904           1,436          0.38%\n",
      "[ 34] FRO53271           1,420          0.37%\n",
      "[ 35] SNA93860           1,407          0.37%\n",
      "[ 36] SNA90094           1,390          0.36%\n",
      "[ 37] GRO38814           1,352          0.36%\n",
      "[ 38] ELE56788           1,345          0.35%\n",
      "[ 39] GRO61133           1,321          0.35%\n",
      "[ 40] DAI88807           1,316          0.35%\n",
      "[ 41] ELE74482           1,316          0.35%\n",
      "[ 42] ELE59935           1,311          0.34%\n",
      "[ 43] SNA96271           1,295          0.34%\n",
      "[ 44] DAI43223           1,290          0.34%\n",
      "[ 45] ELE91337           1,289          0.34%\n",
      "[ 46] GRO15017           1,275          0.33%\n",
      "[ 47] DAI31081           1,261          0.33%\n",
      "[ 48] GRO81087           1,220          0.32%\n",
      "[ 49] DAI22896           1,219          0.32%\n",
      "[ 50] GRO85051           1,214          0.32%\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "      Unique words      12,592\n",
      "      largest basket        74\n"
     ]
    }
   ],
   "source": [
    "print_results('hw3_3b_output.txt', 50, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.4: Pairs\n",
    "\n",
    "Suppose we want to recommend new products to the customer based on the products they have already browsed on the online website. Write a map-reduce program to find products which are frequently browsed together. Fix the support count (cooccurence count) to s = 100 (i.e. product pairs need to occur together at least 100 times to be considered frequent) and find pairs of items (sometimes referred to itemsets of size 2 in association rule mining) that have a support count of 100 or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_34a.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_34a.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: mapper code for HW3.4a\n",
    "\n",
    "import sys\n",
    "import itertools\n",
    "\n",
    "# Increment mapper counter\n",
    "sys.stderr.write(\"reporter:counter:Custom_Counter,Mapper,1\\n\")\n",
    "\n",
    "# Initialize variables\n",
    "total = 0\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # Split our line into products\n",
    "    products = line.replace('\\n','').split()\n",
    "    \n",
    "    # Get all combinations of products:\n",
    "    #  - Use a set to remove duplicate products\n",
    "    #  - Combinations finds tuples of length 2 with no repeats\n",
    "    pairs = list(itertools.combinations(set(products), 2))\n",
    "    \n",
    "    # For each pair, sort the pair alphabetically, then emit\n",
    "    for pair in pairs:\n",
    "        sorted_pair = sorted(pair)\n",
    "        print '%s\\t%s\\t%s' % (sorted_pair[0], sorted_pair[1], 1)\n",
    "    \n",
    "    # Increment total number of baskets\n",
    "    total += 1\n",
    "        \n",
    "# Print total words\n",
    "print '%s\\t%s\\t%s' % ('*total', '', total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_34a.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_34a.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: reducer code for HW3.4a\n",
    "\n",
    "import sys\n",
    "from operator import itemgetter\n",
    "\n",
    "# Increment mapper counter\n",
    "sys.stderr.write(\"reporter:counter:Custom_Counter,Reducer,1\\n\")\n",
    "\n",
    "# Initialize variables\n",
    "prev_pair = []\n",
    "prev_count = 0\n",
    "total = 0\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # Define our key and value\n",
    "    fields = line.replace('\\n','').split('\\t')\n",
    "    pair = [fields[0], fields[1]]\n",
    "    count = fields[2]\n",
    "\n",
    "    # Convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # Count wasn't an int, so move on\n",
    "        continue\n",
    "\n",
    "    # Check if we've moved to a new word\n",
    "    if prev_pair == pair:\n",
    "        prev_count += count\n",
    "    else:\n",
    "        if len(prev_pair) > 0:\n",
    "            # We are at a new pair, need to print previous pair sum\n",
    "            print '%s\\t%s\\t%s' % (prev_pair[0], prev_pair[1], prev_count)\n",
    "        prev_count = count\n",
    "        prev_pair = pair\n",
    "\n",
    "# Output the last line\n",
    "if prev_pair == pair:\n",
    "    print '%s\\t%s\\t%s' % (prev_pair[0], prev_pair[1], prev_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/miki/week03/hw3_4a_output\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.5.0.jar] /tmp/streamjob5023424855138306517.jar tmpDir=null\n",
      "\n",
      "real\t0m50.284s\n",
      "user\t0m5.474s\n",
      "sys\t0m0.365s\n"
     ]
    }
   ],
   "source": [
    "# Change permissions on mapper and reducer\n",
    "!chmod +x mapper_34a.py\n",
    "!chmod +x reducer_34a.py\n",
    "\n",
    "# If output folder already exists, delete it\n",
    "!hdfs dfs -rm -r /user/miki/week03/hw3_4a_output\n",
    "\n",
    "# Run job\n",
    "!time hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.text.key.partitioner.options=-k1,1 \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D mapred.text.key.comparator.options='-k1,1 -k2,2' \\\n",
    "-mapper /home/cloudera/Documents/W261-Fall2016/Week03/mapper_34a.py \\\n",
    "-reducer /home/cloudera/Documents/W261-Fall2016/Week03/reducer_34a.py \\\n",
    "-input /user/miki/week03/ProductPurchaseData.txt \\\n",
    "-output /user/miki/week03/hw3_4a_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have each pair and the number of times that the pair co-occurs. We need to run another job to calculate the relative frequency and sort the resulting pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_34b.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_34b.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: mapper code for HW3.4b\n",
    "\n",
    "import sys\n",
    "\n",
    "# Increment mapper counter\n",
    "sys.stderr.write(\"reporter:counter:Custom_Counter,Mapper,1\\n\")\n",
    "\n",
    "# Initialize variables\n",
    "total = 0\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    fields = line.replace('\\n','').split('\\t')\n",
    "    if int(fields[2]) >= 100:\n",
    "        print '%s\\t%s\\t%s' % (fields[2], fields[0], fields[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer_34b.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_34b.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: reducer code for HW3.4b\n",
    "\n",
    "import sys\n",
    "from operator import itemgetter\n",
    "\n",
    "# Increment mapper counter\n",
    "sys.stderr.write(\"reporter:counter:Custom_Counter,Reducer,1\\n\")\n",
    "\n",
    "# Initialize variables\n",
    "total = 0\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    fields = line.replace('\\n','').split('\\t')\n",
    "    count = fields[0]\n",
    "    item1 = fields[1]\n",
    "    item2 = fields[2]\n",
    "    \n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        continue\n",
    "        \n",
    "    # The first word should be *total, save this as total\n",
    "    if item1 == '*total': total = float(count)\n",
    "    else: print '%s\\t%s\\t%s\\t%s' % (item1, item2, count, count/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/miki/week03/hw3_4b_output\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.5.0.jar] /tmp/streamjob2299545763059091234.jar tmpDir=null\n",
      "\n",
      "real\t0m26.758s\n",
      "user\t0m4.982s\n",
      "sys\t0m0.345s\n"
     ]
    }
   ],
   "source": [
    "# Change permissions on mapper and reducer\n",
    "!chmod +x mapper_34b.py\n",
    "!chmod +x reducer_34b.py\n",
    "\n",
    "# If output folder already exists, delete it\n",
    "!hdfs dfs -rm -r /user/miki/week03/hw3_4b_output\n",
    "\n",
    "# Run job\n",
    "!time hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.text.key.partitioner.options=-k1,1 \\\n",
    "-D stream.num.map.output.key.fields=3 \\\n",
    "-D mapred.text.key.comparator.options='-k1,1nr -k2,2 -k3,3' \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-mapper /home/cloudera/Documents/W261-Fall2016/Week03/mapper_34b.py \\\n",
    "-reducer /home/cloudera/Documents/W261-Fall2016/Week03/reducer_34b.py \\\n",
    "-input /user/miki/week03/hw3_4a_output/part* \\\n",
    "-output /user/miki/week03/hw3_4b_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -copyToLocal /user/miki/week03/hw3_4b_output/part-00000 hw3_4b_output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to pretty print:\n",
    "# - the top x and bottom y items\n",
    "# - unique items\n",
    "def print_results(file, x=50, y=10):\n",
    "    words = []\n",
    "    special_words = []\n",
    "    \n",
    "    with open(file,'r') as myfile:\n",
    "        for line in myfile:\n",
    "            fields = line.replace('\\n','').split('\\t')\n",
    "            if fields[0][0] != '*': words.append(fields)\n",
    "            else: special_words.append(fields)\n",
    "\n",
    "    print '      {:10s}{:10s}{:>8s}{:>15s}'.format('item1', 'item2', 'count', 'relative freq')\n",
    "    print '------------------------------------------------------'\n",
    "    for i in range(x):\n",
    "        print '[{:3d}] {:10s}{:10s}{:8,d}{:15.2%}'.format(i+1, \n",
    "                                                          words[i][0], \n",
    "                                                          words[i][1],\n",
    "                                                          int(words[i][2]), \n",
    "                                                          float(words[i][3]))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List the top 50 product pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      item1     item2        count  relative freq\n",
      "------------------------------------------------------\n",
      "[  1] DAI62779  ELE17451     1,592          5.12%\n",
      "[  2] FRO40251  SNA80324     1,412          4.54%\n",
      "[  3] DAI75645  FRO40251     1,254          4.03%\n",
      "[  4] FRO40251  GRO85051     1,213          3.90%\n",
      "[  5] DAI62779  GRO73461     1,139          3.66%\n",
      "[  6] DAI75645  SNA80324     1,130          3.63%\n",
      "[  7] DAI62779  FRO40251     1,070          3.44%\n",
      "[  8] DAI62779  SNA80324       923          2.97%\n",
      "[  9] DAI62779  DAI85309       918          2.95%\n",
      "[ 10] ELE32164  GRO59710       911          2.93%\n",
      "[ 11] DAI62779  DAI75645       882          2.84%\n",
      "[ 12] FRO40251  GRO73461       882          2.84%\n",
      "[ 13] DAI62779  ELE92920       877          2.82%\n",
      "[ 14] FRO40251  FRO92469       835          2.68%\n",
      "[ 15] DAI62779  ELE32164       832          2.68%\n",
      "[ 16] DAI75645  GRO73461       712          2.29%\n",
      "[ 17] DAI43223  ELE32164       711          2.29%\n",
      "[ 18] DAI62779  GRO30386       709          2.28%\n",
      "[ 19] ELE17451  FRO40251       697          2.24%\n",
      "[ 20] DAI85309  ELE99737       659          2.12%\n",
      "[ 21] DAI62779  ELE26917       650          2.09%\n",
      "[ 22] GRO21487  GRO73461       631          2.03%\n",
      "[ 23] DAI62779  SNA45677       604          1.94%\n",
      "[ 24] ELE17451  SNA80324       597          1.92%\n",
      "[ 25] DAI62779  GRO71621       595          1.91%\n",
      "[ 26] DAI62779  SNA55762       593          1.91%\n",
      "[ 27] DAI62779  DAI83733       586          1.88%\n",
      "[ 28] ELE17451  GRO73461       580          1.86%\n",
      "[ 29] GRO73461  SNA80324       562          1.81%\n",
      "[ 30] DAI62779  GRO59710       561          1.80%\n",
      "[ 31] DAI62779  FRO80039       550          1.77%\n",
      "[ 32] DAI75645  ELE17451       547          1.76%\n",
      "[ 33] DAI62779  SNA93860       537          1.73%\n",
      "[ 34] DAI55148  DAI62779       526          1.69%\n",
      "[ 35] DAI43223  GRO59710       512          1.65%\n",
      "[ 36] ELE17451  ELE32164       511          1.64%\n",
      "[ 37] DAI62779  SNA18336       506          1.63%\n",
      "[ 38] ELE32164  GRO73461       486          1.56%\n",
      "[ 39] DAI62779  FRO78087       482          1.55%\n",
      "[ 40] DAI85309  ELE17451       482          1.55%\n",
      "[ 41] DAI62779  GRO94758       479          1.54%\n",
      "[ 42] DAI62779  GRO21487       471          1.51%\n",
      "[ 43] GRO85051  SNA80324       471          1.51%\n",
      "[ 44] ELE17451  GRO30386       468          1.50%\n",
      "[ 45] FRO85978  SNA95666       463          1.49%\n",
      "[ 46] DAI62779  FRO19221       462          1.49%\n",
      "[ 47] DAI62779  GRO46854       461          1.48%\n",
      "[ 48] DAI43223  DAI62779       459          1.48%\n",
      "[ 49] ELE92920  SNA18336       455          1.46%\n",
      "[ 50] DAI88079  FRO40251       446          1.43%\n"
     ]
    }
   ],
   "source": [
    "print_results('hw3_4b_output.txt', 50, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report  the compute time for the Pairs job. \n",
    "The 1st job (counts) reports the following compute times:\n",
    "```\n",
    "real\t0m50.284s\n",
    "user\t0m5.474s\n",
    "sys \t0m0.365s\n",
    "```\n",
    "The 2nd job (sorts) reports the following compute times:\n",
    "```\n",
    "real\t0m26.758s\n",
    "user\t0m4.982s\n",
    "sys \t0m0.345s\n",
    "```\n",
    "### Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)\n",
    "Cloudera QuickStart VM: single computer, 2 processors, 2 mappers (default), 1 reducer\n",
    "\n",
    "### How many times is each mapper and reducer called?\n",
    "- Mapper: 2\n",
    "- Reducer: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.5: Stripes\n",
    "Repeat 3.4 using the stripes design pattern for finding cooccuring pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_35a.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_35a.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: mapper code for HW3.5a\n",
    "\n",
    "import sys\n",
    "import itertools\n",
    "\n",
    "# Increment mapper counter\n",
    "sys.stderr.write(\"reporter:counter:Custom_Counter,Mapper,1\\n\")\n",
    "\n",
    "# Initialize variables\n",
    "total = 0\n",
    "stripes = {}\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # Split our line into products\n",
    "    products = line.replace('\\n','').split()\n",
    "    \n",
    "    # Get all combinations of products:\n",
    "    #  - Use a set to remove duplicate products\n",
    "    #  - Combinations finds tuples of length 2 with no repeats\n",
    "    items = sorted(list(set(products)))\n",
    "\n",
    "    for i in range(len(items)-1):\n",
    "        for j in range(i+1, len(items)):\n",
    "            stripes[items[j]] = 1\n",
    "        print '%s\\t%s' % (items[i], stripes)\n",
    "        stripes = {}\n",
    "        \n",
    "    # Increment total number of baskets\n",
    "    total += 1\n",
    "        \n",
    "# Print total words\n",
    "print '%s\\t%s' % ('*total', {'*total':total})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_35a.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_35a.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: reducer code for HW3.5a\n",
    "\n",
    "import sys\n",
    "from operator import itemgetter\n",
    "\n",
    "# Increment mapper counter\n",
    "sys.stderr.write(\"reporter:counter:Custom_Counter,Reducer,1\\n\")\n",
    "\n",
    "# Initialize variables\n",
    "prev_word = None\n",
    "prev_stripe = {}\n",
    "total = 0\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # Define our key and value\n",
    "    fields = line.replace('\\n','').split('\\t')\n",
    "    word = fields[0]\n",
    "    stripe = eval(fields[1])\n",
    "\n",
    "    # Check if we've moved to a new word\n",
    "    if prev_word == word:\n",
    "        # We need to move through the dictionary and update counts\n",
    "        for item in stripe:\n",
    "            if item in prev_stripe:\n",
    "                prev_stripe[item] += stripe[item]\n",
    "            else:\n",
    "                prev_stripe[item] = stripe[item]\n",
    "        \n",
    "    else:\n",
    "        if len(prev_stripe) > 0:\n",
    "            # We are at a new pair, need to print previous pair sum\n",
    "            print '%s\\t%s' % (prev_word, prev_stripe)\n",
    "        prev_stripe = stripe\n",
    "        prev_word = word\n",
    "\n",
    "# Output the last line\n",
    "if prev_stripe == stripe:\n",
    "    print '%s\\t%s' % (prev_word, prev_stripe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/miki/week03/hw3_5a_output\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.5.0.jar] /tmp/streamjob8692232309490935031.jar tmpDir=null\n",
      "\n",
      "real\t0m44.977s\n",
      "user\t0m5.111s\n",
      "sys\t0m0.337s\n"
     ]
    }
   ],
   "source": [
    "# Change permissions on mapper and reducer\n",
    "!chmod +x mapper_35a.py\n",
    "!chmod +x reducer_35a.py\n",
    "\n",
    "# If output folder already exists, delete it\n",
    "!hdfs dfs -rm -r /user/miki/week03/hw3_5a_output\n",
    "\n",
    "# Run job\n",
    "!time hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-mapper /home/cloudera/Documents/W261-Fall2016/Week03/mapper_35a.py \\\n",
    "-reducer /home/cloudera/Documents/W261-Fall2016/Week03/reducer_35a.py \\\n",
    "-input /user/miki/week03/ProductPurchaseData.txt \\\n",
    "-output /user/miki/week03/hw3_5a_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a stripe for each product (item1). The stripe contains the second item in the pair (item2), along with the count of co-occurrences with item1. Now, we need to \"unpack\" the stripe, and sort the pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_35b.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_35b.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Miki Seltzer\n",
    "## Description: mapper code for HW3.5b\n",
    "\n",
    "import sys\n",
    "import itertools\n",
    "\n",
    "# Increment mapper counter\n",
    "sys.stderr.write(\"reporter:counter:Custom_Counter,Mapper,1\\n\")\n",
    "\n",
    "# Initialize variables\n",
    "total = 0\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # Define our key and value\n",
    "    fields = line.replace('\\n','').split('\\t')\n",
    "    word = fields[0]\n",
    "    stripe = eval(fields[1])\n",
    "    # Now we need to \"unpack\" the stripe and emit each pair for sorting\n",
    "    for item in stripe:\n",
    "        if stripe[item] >= 100: \n",
    "            print '%s\\t%s\\t%s' % (stripe[item], word, item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this mapper unpacks the pairs and emits them in the same format as the Pairs method in HW3.4, we can use the same reducer from the previous part to find the relative frequencies and top 50 pairs of items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/miki/week03/hw3_5b_output\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.5.0.jar] /tmp/streamjob2185279117818801498.jar tmpDir=null\n",
      "\n",
      "real\t0m34.390s\n",
      "user\t0m5.735s\n",
      "sys\t0m0.335s\n"
     ]
    }
   ],
   "source": [
    "# Change permissions on mapper and reducer\n",
    "!chmod +x mapper_35b.py\n",
    "\n",
    "# If output folder already exists, delete it\n",
    "!hdfs dfs -rm -r /user/miki/week03/hw3_5b_output\n",
    "\n",
    "# Run job\n",
    "!time hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.text.key.partitioner.options=-k1,1 \\\n",
    "-D stream.num.map.output.key.fields=3 \\\n",
    "-D mapred.text.key.comparator.options='-k1,1nr -k2,2 -k3,3' \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-mapper /home/cloudera/Documents/W261-Fall2016/Week03/mapper_35b.py \\\n",
    "-reducer /home/cloudera/Documents/W261-Fall2016/Week03/reducer_34b.py \\\n",
    "-input /user/miki/week03/hw3_5a_output/part* \\\n",
    "-output /user/miki/week03/hw3_5b_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -copyToLocal /user/miki/week03/hw3_5b_output/part-00000 hw3_5b_output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List the top 50 product pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      item1     item2        count  relative freq\n",
      "------------------------------------------------------\n",
      "[  1] DAI62779  ELE17451     1,592          5.12%\n",
      "[  2] FRO40251  SNA80324     1,412          4.54%\n",
      "[  3] DAI75645  FRO40251     1,254          4.03%\n",
      "[  4] FRO40251  GRO85051     1,213          3.90%\n",
      "[  5] DAI62779  GRO73461     1,139          3.66%\n",
      "[  6] DAI75645  SNA80324     1,130          3.63%\n",
      "[  7] DAI62779  FRO40251     1,070          3.44%\n",
      "[  8] DAI62779  SNA80324       923          2.97%\n",
      "[  9] DAI62779  DAI85309       918          2.95%\n",
      "[ 10] ELE32164  GRO59710       911          2.93%\n",
      "[ 11] DAI62779  DAI75645       882          2.84%\n",
      "[ 12] FRO40251  GRO73461       882          2.84%\n",
      "[ 13] DAI62779  ELE92920       877          2.82%\n",
      "[ 14] FRO40251  FRO92469       835          2.68%\n",
      "[ 15] DAI62779  ELE32164       832          2.68%\n",
      "[ 16] DAI75645  GRO73461       712          2.29%\n",
      "[ 17] DAI43223  ELE32164       711          2.29%\n",
      "[ 18] DAI62779  GRO30386       709          2.28%\n",
      "[ 19] ELE17451  FRO40251       697          2.24%\n",
      "[ 20] DAI85309  ELE99737       659          2.12%\n",
      "[ 21] DAI62779  ELE26917       650          2.09%\n",
      "[ 22] GRO21487  GRO73461       631          2.03%\n",
      "[ 23] DAI62779  SNA45677       604          1.94%\n",
      "[ 24] ELE17451  SNA80324       597          1.92%\n",
      "[ 25] DAI62779  GRO71621       595          1.91%\n",
      "[ 26] DAI62779  SNA55762       593          1.91%\n",
      "[ 27] DAI62779  DAI83733       586          1.88%\n",
      "[ 28] ELE17451  GRO73461       580          1.86%\n",
      "[ 29] GRO73461  SNA80324       562          1.81%\n",
      "[ 30] DAI62779  GRO59710       561          1.80%\n",
      "[ 31] DAI62779  FRO80039       550          1.77%\n",
      "[ 32] DAI75645  ELE17451       547          1.76%\n",
      "[ 33] DAI62779  SNA93860       537          1.73%\n",
      "[ 34] DAI55148  DAI62779       526          1.69%\n",
      "[ 35] DAI43223  GRO59710       512          1.65%\n",
      "[ 36] ELE17451  ELE32164       511          1.64%\n",
      "[ 37] DAI62779  SNA18336       506          1.63%\n",
      "[ 38] ELE32164  GRO73461       486          1.56%\n",
      "[ 39] DAI62779  FRO78087       482          1.55%\n",
      "[ 40] DAI85309  ELE17451       482          1.55%\n",
      "[ 41] DAI62779  GRO94758       479          1.54%\n",
      "[ 42] DAI62779  GRO21487       471          1.51%\n",
      "[ 43] GRO85051  SNA80324       471          1.51%\n",
      "[ 44] ELE17451  GRO30386       468          1.50%\n",
      "[ 45] FRO85978  SNA95666       463          1.49%\n",
      "[ 46] DAI62779  FRO19221       462          1.49%\n",
      "[ 47] DAI62779  GRO46854       461          1.48%\n",
      "[ 48] DAI43223  DAI62779       459          1.48%\n",
      "[ 49] ELE92920  SNA18336       455          1.46%\n",
      "[ 50] DAI88079  FRO40251       446          1.43%\n"
     ]
    }
   ],
   "source": [
    "print_results('hw3_5b_output.txt', 50, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report  the compute time for the Stripes job. \n",
    "The 1st job (counts) reports the following compute times:\n",
    "```\n",
    "real\t0m44.977s\n",
    "user\t0m5.111s\n",
    "sys \t0m0.337s\n",
    "```\n",
    "The 2nd job (sorts) reports the following compute times:\n",
    "```\n",
    "real\t0m34.390s\n",
    "user\t0m5.735s\n",
    "sys \t0m0.335s\n",
    "```\n",
    "### Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)\n",
    "Cloudera QuickStart VM: single computer, 2 processors, 2 mappers (default), 1 reducer\n",
    "\n",
    "### How many times is each mapper and reducer called?\n",
    "- Mapper: 2\n",
    "- Reducer: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss the differences in these counts between the Pairs and Stripes jobs\n",
    "\n",
    "Below is a table showing the timings for the pairs and stripes jobs (1st job counts the co-occurrences, 2nd job sorts pairs):\n",
    "\n",
    "item | Pairs count | Stripes count | Pairs sort | Stripes sort\n",
    "-----|-------------|---------------|------------|-------------\n",
    "real | 0m50.284s   |  0m44.977s    | 0m26.758s  | 0m34.390s\n",
    "user | 0m5.474s    |  0m5.111s     | 0m4.982s   | 0m5.735s\n",
    "sys  | 0m0.365s    |  0m0.337s     | 0m0.345s   | 0m0.335s\n",
    "\n",
    "Indeed, the stripes job took less time to complete than the pairs job in the counting phase.\n",
    "\n",
    "However, the pairs job took less time to complete when attempting to sort the pairs. This is likely due to the fact that in order to sort the output of the stripes job, we need to \"unpack\" the stripes to recover each individual pair, whereas the output of the pairs job does not need any additional unpacking.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.7. Shopping Cart Analysis\n",
    "\n",
    "Product Recommendations: The action or practice of selling additional products or services to existing customers is called cross-selling. Giving product recommendation is one of the examples of cross-selling that are frequently used by online retailers. One simple method to give product recommendations is to recommend products that are frequently browsed together by the customers.\n",
    "\n",
    "Suppose we want to recommend new products to the customer based on the products they have already browsed on the online website. Write a program using the A-priori algorithm to find products which are frequently browsed together. Fix the support to s = 100 (i.e. product sets need to occur together at least 100 times to be considered frequent) and find itemsets of size 2 and 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper_37.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_37.py\n",
    "#!/usr/bin/env python\n",
    "import itertools\n",
    "import sys\n",
    "\n",
    "item_count = int(sys.argv[1])\n",
    "valid_items = set()\n",
    "\n",
    "# If our item count is greater than 1, then load the corresponding model file\n",
    "# indicating the items we should care about.\n",
    "\n",
    "if item_count > 1:\n",
    "    model_id = str(item_count - 1)\n",
    "\n",
    "    # The first k items in each model row will correspond to the products. We\n",
    "    # can build up the set of valid items simply by iterating over the model\n",
    "    # and adding each of the elements in the first k columns.\n",
    "\n",
    "    with open('apriori_model_' + model_id + '.txt') as model_file:\n",
    "        for line in model_file:\n",
    "            model_row = line.strip().split('\\t')\n",
    "            old_itemset = model_row[0:item_count - 1]\n",
    "            valid_items.update(old_itemset)\n",
    "\n",
    "\"\"\"\n",
    "Emit all the itemsets for this basket.\n",
    "\"\"\"\n",
    "def emit_itemsets(basket):\n",
    "    # First, we need to find out which items in the basket match the items\n",
    "    # which we can accept in our k-itemsets. Note that we will accept every\n",
    "    # item when the item count is 1.\n",
    "    \n",
    "    matching_items = []\n",
    "    \n",
    "    for item in basket:\n",
    "        if item_count == 1 or item in valid_items:\n",
    "            matching_items.append(item)\n",
    "    \n",
    "    # If we don't have enough items, we have no itemsets to emit.\n",
    "    \n",
    "    if len(matching_items) < item_count:\n",
    "        return\n",
    "\n",
    "    # Otherwise, emit all possible subsets. We'll use the pairs approach to\n",
    "    # make things easier to read.\n",
    "    \n",
    "    for itemset in itertools.permutations(matching_items, item_count):\n",
    "        print '\\t'.join(itemset) + '\\t1'\n",
    "        \n",
    "    # Also emit a counter for subcombinations so that we can create a\n",
    "    # tally to use for computing confidence.\n",
    "\n",
    "    if item_count > 1:\n",
    "        for sub_itemset in itertools.permutations(matching_items, item_count - 1):\n",
    "            print '\\t'.join(sub_itemset) + '\\t*\\t1'\n",
    "        \n",
    "    # Finally, counter so that we can track the number of matching baskets.\n",
    "\n",
    "    print ('*\\t' * item_count) + str(1)\n",
    "    \n",
    "# Iterate over the baskets and emit the itemsets for each basket.\n",
    "\n",
    "for line in sys.stdin:\n",
    "    basket = line.strip().split(' ')\n",
    "    emit_itemsets(basket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer_37.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_37.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "\n",
    "support_threshold = int(sys.argv[1])\n",
    "\n",
    "basket_count = 0\n",
    "confidence_count = 0\n",
    "\n",
    "current_itemset = None\n",
    "current_count = 0\n",
    "\n",
    "\"\"\"\n",
    "Emit the current itemset and its count if they exceed support_threshold.\n",
    "\"\"\"\n",
    "def emit_count():\n",
    "\n",
    "    # Declare that we want to use the global basket_count and confidence_count\n",
    "    # variables rather than something local to the function.\n",
    "    \n",
    "    global basket_count, confidence_count\n",
    "    \n",
    "    # Check if we haven't started counting anything yet.\n",
    "    \n",
    "    if current_itemset is None:\n",
    "        return\n",
    "\n",
    "    # Check if we are computing the basket count from the sort operation.\n",
    "    \n",
    "    if current_itemset[0] == '*':\n",
    "        basket_count = current_count\n",
    "        return\n",
    "\n",
    "    if current_itemset[-1] == '*':\n",
    "        confidence_count = current_count\n",
    "        return\n",
    "\n",
    "    # Check if we have exceeded the necessary threshold.\n",
    "\n",
    "    if current_count >= support_threshold:\n",
    "        frequency = 1.0 * current_count / basket_count\n",
    "        \n",
    "        itemset_stats = str(current_count) + '\\t' + str(frequency)\n",
    "        \n",
    "        if len(current_itemset) > 1:\n",
    "            confidence = 1.0 * current_count / confidence_count\n",
    "            itemset_stats += '\\t' + str(confidence)\n",
    "        \n",
    "        print '\\t'.join(current_itemset) + '\\t' + itemset_stats\n",
    "\n",
    "for line in sys.stdin:\n",
    "\n",
    "    # Each line corresponds to the itemset stats. The last item will be a count\n",
    "    # value, while the first items will be the itemset.\n",
    "    \n",
    "    itemset_stats = line.strip().split('\\t')\n",
    "\n",
    "    itemset = itemset_stats[0:-1]\n",
    "    count = int(itemset_stats[-1])\n",
    "\n",
    "    # If we haven't switched itemsets, continue accumulating the counter.\n",
    "    \n",
    "    if current_itemset == itemset:\n",
    "        current_count += count\n",
    "        continue\n",
    "\n",
    "    # If we have switched itemsets, emit the count for the old itemset and then\n",
    "    # switch to the new itemset.\n",
    "        \n",
    "    emit_count()\n",
    "    current_itemset = itemset\n",
    "    current_count = count\n",
    "\n",
    "# We are guaranteed to not have printed the very last itemset, so emit it now.\n",
    "    \n",
    "emit_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with the sample data from the slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!echo Beer Diaper BabyPowder Bread Umbrella > SampleSlidesData.txt\n",
    "!echo Diaper BabyPowder >> SampleSlidesData.txt\n",
    "!echo Beer Diaper Milk >> SampleSlidesData.txt\n",
    "!echo Diaper Beer Detergent >> SampleSlidesData.txt\n",
    "!echo Beer Milk CocaCola >> SampleSlidesData.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beer Diaper BabyPowder Bread Umbrella\r\n",
      "Diaper BabyPowder\r\n",
      "Beer Diaper Milk\r\n",
      "Diaper Beer Detergent\r\n",
      "Beer Milk CocaCola\r\n"
     ]
    }
   ],
   "source": [
    "!cat SampleSlidesData.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BabyPowder\t2\t0.4\r\n",
      "Beer\t4\t0.8\r\n",
      "Diaper\t4\t0.8\r\n",
      "Milk\t2\t0.4\r\n"
     ]
    }
   ],
   "source": [
    "!cat SampleSlidesData.txt | python mapper_37.py 1 | sort -k1 | python reducer_37.py 2 | tee apriori_model_1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BabyPowder\tDiaper\t2\t0.4\t1.0\r\n",
      "Beer\tDiaper\t3\t0.6\t0.75\r\n",
      "Beer\tMilk\t2\t0.4\t0.5\r\n",
      "Diaper\tBabyPowder\t2\t0.4\t0.5\r\n",
      "Diaper\tBeer\t3\t0.6\t0.75\r\n",
      "Milk\tBeer\t2\t0.4\t1.0\r\n"
     ]
    }
   ],
   "source": [
    "!cat SampleSlidesData.txt | python mapper_37.py 2 | sort -k1 -k2 | python reducer_37.py 2 | tee apriori_model_2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with the actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cat ProductPurchaseData.txt | python mapper_37.py 1 | sort -k1 | python reducer_37.py 100 > apriori_model_1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cat ProductPurchaseData.txt | python mapper_37.py 2 | sort -k1 -k2 | python reducer_37.py 100 > apriori_model_2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cat ProductPurchaseData.txt | python mapper_37.py 3 | sort -k1 -k2 -k3 | python reducer_37.py 100 > apriori_model_3.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test using Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/miki/week03/hw3_7_output\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.5.0.jar] /tmp/streamjob2467148671885451279.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "# Change permissions on mapper and reducer\n",
    "!chmod +x mapper_37.py\n",
    "!chmod +x reducer_37.py\n",
    "\n",
    "# If output folder already exists, delete it\n",
    "!hdfs dfs -rm -r /user/miki/week03/aprior_model_1\n",
    "\n",
    "# Run job\n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.text.key.partitioner.options=-k1 \\\n",
    "-D stream.num.map.output.key.fields=3 \\\n",
    "-D mapred.text.key.comparator.options='-k1 -k2' \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-mapper '/home/cloudera/Documents/W261-Fall2016/Week03/mapper_37.py 1' \\\n",
    "-reducer '/home/cloudera/Documents/W261-Fall2016/Week03/reducer_37.py 100' \\\n",
    "-input /user/miki/week03/ProductPurchaseData.txt \\\n",
    "-output /user/miki/week03/apriori_model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/miki/week03/apriori_model_2': No such file or directory\n",
      "packageJobJar: [/home/cloudera/Documents/W261-Fall2016/Week03/apriori_model_1.txt] [/usr/jars/hadoop-streaming-2.6.0-cdh5.5.0.jar] /tmp/streamjob890159217878330682.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "# If output folder already exists, delete it\n",
    "!hdfs dfs -rm -r /user/miki/week03/apriori_model_2\n",
    "\n",
    "# Run job\n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.text.key.partitioner.options=-k1 \\\n",
    "-D stream.num.map.output.key.fields=3 \\\n",
    "-D mapred.text.key.comparator.options='-k1 -k2' \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-mapper '/home/cloudera/Documents/W261-Fall2016/Week03/mapper_37.py 2' \\\n",
    "-reducer '/home/cloudera/Documents/W261-Fall2016/Week03/reducer_37.py 100' \\\n",
    "-file /home/cloudera/Documents/W261-Fall2016/Week03/apriori_model_1.txt \\\n",
    "-input /user/miki/week03/ProductPurchaseData.txt \\\n",
    "-output /user/miki/week03/apriori_model_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/miki/week03/apriori_model_3': No such file or directory\n",
      "packageJobJar: [/home/cloudera/Documents/W261-Fall2016/Week03/apriori_model_2.txt] [/usr/jars/hadoop-streaming-2.6.0-cdh5.5.0.jar] /tmp/streamjob3095742417588462696.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "# If output folder already exists, delete it\n",
    "!hdfs dfs -rm -r /user/miki/week03/apriori_model_3\n",
    "\n",
    "# Run job\n",
    "!hadoop jar hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.text.key.partitioner.options=-k1 \\\n",
    "-D stream.num.map.output.key.fields=3 \\\n",
    "-D mapred.text.key.comparator.options='-k1 -k2 -k3' \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-mapper '/home/cloudera/Documents/W261-Fall2016/Week03/mapper_37.py 3' \\\n",
    "-reducer '/home/cloudera/Documents/W261-Fall2016/Week03/reducer_37.py 100' \\\n",
    "-file /home/cloudera/Documents/W261-Fall2016/Week03/apriori_model_2.txt \\\n",
    "-input /user/miki/week03/ProductPurchaseData.txt \\\n",
    "-output /user/miki/week03/apriori_model_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAI22896\tDAI62779\tGRO73461\t101\t0.00379984951091\t0.340067340067\r\n",
      "DAI22896\tGRO73461\tDAI62779\t101\t0.00379984951091\t0.332236842105\r\n",
      "DAI23334\tDAI62779\tELE92920\t143\t0.00537998495109\t0.52380952381\r\n",
      "DAI23334\tELE92920\tDAI62779\t143\t0.00537998495109\t1.0\r\n",
      "DAI31081\tDAI62779\tELE17451\t103\t0.00387509405568\t0.282967032967\r\n",
      "DAI31081\tDAI75645\tFRO40251\t122\t0.004589917231\t0.592233009709\r\n",
      "DAI31081\tELE17451\tDAI62779\t103\t0.00387509405568\t0.449781659389\r\n",
      "DAI31081\tELE32164\tGRO59710\t112\t0.00421369450715\t0.358974358974\r\n",
      "DAI31081\tFRO40251\tDAI75645\t122\t0.004589917231\t0.435714285714\r\n",
      "DAI31081\tFRO40251\tGRO85051\t102\t0.0038374717833\t0.364285714286\r\n",
      "DAI31081\tFRO40251\tSNA80324\t103\t0.00387509405568\t0.367857142857\r\n",
      "DAI31081\tGRO59710\tELE32164\t112\t0.00421369450715\t0.598930481283\r\n",
      "DAI31081\tGRO85051\tFRO40251\t102\t0.0038374717833\t1.0\r\n",
      "DAI31081\tSNA80324\tFRO40251\t103\t0.00387509405568\t0.544973544974\r\n",
      "DAI42083\tDAI62779\tDAI92600\t105\t0.00395033860045\t0.905172413793\r\n",
      "DAI42083\tDAI92600\tDAI62779\t105\t0.00395033860045\t0.415019762846\r\n",
      "DAI42083\tDAI92600\tELE17451\t117\t0.00440180586907\t0.462450592885\r\n",
      "DAI42083\tELE17451\tDAI92600\t117\t0.00440180586907\t0.632432432432\r\n",
      "DAI42493\tDAI62779\tELE17451\t112\t0.00421369450715\t0.363636363636\r\n",
      "DAI42493\tDAI62779\tELE92920\t112\t0.00421369450715\t0.363636363636\r\n",
      "cat: Unable to write to output stream.\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/miki/week03/apriori_model_3/part-00000 | head -n 20"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
